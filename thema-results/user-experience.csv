"University","Title","Abstract","Relevance Score","Publication URL","Coordinates"
"""UCL""","""Iris Publication""","""http://discovery.ucl.ac.uk/1403266/\nAbstract\nOBJECTIVES: To examine the individual and combined associations of unhealthy behaviours (low/intermediate physical activity, consuming fruit and vegetables less than once a day, current smoking/short term ex-smoking, never/former/heavy alcohol drinking), assessed at start of follow-up, with hazard of disability among older French adults and to assess the role of potential mediators, assessed repeatedly, of these associations. DESIGN: Population based cohort study. SETTING: Dijon centre of Three-City study. PARTICIPANTS: 3982 (2410 (60.5%) women) French community dwellers aged 65 or over included during 1999-2001; participants were disability-free at baseline when health behaviours were assessed. MAIN OUTCOME MEASURE: Hierarchical indicator of disability (no, light, moderate, severe) combining data from three disability scales (mobility, instrumental activities of daily living, basic activities of daily living) assessed five times between 2001 and 2012. RESULTS: During the 12 year follow-up, 1236 participants (861 (69.7%) women) developed moderate or severe disability. Interval censored survival analyses (adjusted for age, sex, marital status, and education) showed low/intermediate physical activity (hazard ratio 1.72, 95% confidence interval 1.48 to 2.00), consuming fruit and vegetables less than once a day (1.24, 1.10 to 1.41), and current smoking/short term ex-smoking (1.26, 1.05 to 1.50) to be independently associated with an increased hazard of disability, whereas no robust association with alcohol consumption was found. The hazard of disability increased progressively with the number of unhealthy behaviours independently associated with disability (P<0.001); participants with three unhealthy behaviours had a 2.53 (1.86 to 3.43)-fold increased hazard of disability compared with those with none. Reverse causation bias was verified by excluding participants who developed disability in the first four years of follow-up; these analyses on 890 disability events yielded results similar to those in the main analysis. 30.5% of the association between the unhealthy behaviours score and disability was explained by body mass index, cognitive function, depressive symptoms, trauma, chronic conditions, and cardiovascular disease and its risk factors; the main contributors were chronic conditions and, to a lesser extent, depressive symptoms, trauma, and body mass index. CONCLUSIONS: An unhealthy lifestyle is associated with greater hazard of incident disability, and the hazard increases progressively with the number of unhealthy behaviours. Chronic conditions, depressive symptoms, trauma, and body mass index partially explained this association.\nPublication data is maintained in RPS. Visit https://rps.ucl.ac.uk\n› More search options\n""","0.8898537","""http://iris.ucl.ac.uk/iris/publication/889490/2""",
"""Brunel_University_London""","""A system for remote sighted guidance of visually impaired pedestriansBritish Journal of Visual Impairment - V. Garaj, R. Jirawimut, P. Ptasinski, F. Cecelja, W. Balachandran, 2003""","""PDF\nAbstract\nSighted guidance is arguably the most efficient method for aiding visually impaired pedestrians in mobility. A sighted guide's verbal instructions compensate comprehensively for the insufficiency of visual input in navigation. Moreover, the companionship entails sharing of responsibilities and thus increases the blind traveller's sense of security during a journey. The disadvantages of the sighted guidance are that a sighted guide may not always be available or their presence may not be desirable because it restricts personal independence. This paper presents a novel system for navigation of visually impaired pedestrians, whereby advanced technologies were combined to allow a visually impaired user to remotely access the sighted guidance service. The user can choose when and for how long to use the system. The remote sighted guidance system is enabled by the integration of a remote vision facility with the Global Positioning System, the Geographic Information System and the third generation telecommunication network. A user trial is also reported in which the contribution of the system to the mobility of a visually impaired pedestrian was assessed. The results obtained lead to the conclusion that the remote sighted guidance is potentially a highly usable mobility aid.\nEspinosa, M.A. & Ochaita, E. (1998) 'Using tactile maps to improve the practical spatial knowledge of adults who are blind'. Journal of Visual Impairment and Blindness, 92 (5), 338-45. Google Scholar\nFarmer, L.W. & Smith D.L. (1997) 'Adaptive Technology'. In B. B. Blasch, W. R. Wiener & W. R. Welsh (Eds) Foundations of Orientation and Mobility. 2nd Edition (Chapter 7, pp.231-70). New York: American Foundation for the Blind. Google Scholar\nGaraj, V. (2001) 'The Brunel Navigation System for the Blind: Determination of the appropriate position to mount the external GPS antenna on the user's body'. In Proceedings of the 2001-GPS Technical Meeting. Salt Lake City: Institute of Navigation. Google Scholar\nGuth, D.A. & Rieser, J.J. (1997) 'Perception and the control of locomotion by blind and visually impaired pedestrians'. In B. B. Blasch, W. R. Wiener & W. R. Welsh (Eds) Foundations of Orientation and Mobility. 2nd Edition (Chapter 1, pp.9-38). New York: American Foundation for the Blind. Google Scholar\nHeyes, A.D. (1983) 'The Sonic Pathfinder - a new travel aid for the blind'. In W. J. Perkins (Ed.) High Technology Aids for the Disabled (pp.231-270). London: Butterworth & Co. Google Scholar , Crossref\nHeyes, A.D., Dodds, A.G., Carter, D.D.C. & Howarth, C.I. (1983) 'Evaluation of the mobility of Blind Pedestrians' . In W. J. Perkins (Ed.) High Technology Aids for the Disabled (pp.165-171). London: Butterworth & Co. Google Scholar , Crossref\nJansson, G. (1995) 'Theoretical analysis and literature survey'. In G. Jansson (Ed.) Requirements for Effective Orientation and Mobility by Blind People (Chapter 1, pp. 5-16). London: Royal National Institute of the Blind. Google Scholar\nKang-Tsung, C. (2001) Introduction to Geographic Information Systems . USA: McGraw-Hill. Google Scholar\nKorhonen, J. (2001) Introduction to 3G Mobile Communication. USA: Artech House. Google Scholar\nLoomis, J.M., Golledge, R.G. & Klatzky, R.L. (1998) 'Navigations system for the blind: auditory display modes and guidance'. Presence, 7 (2), 193-203. Google Scholar , Crossref\nMay, M. ( 2002) 'Accessible GPS navigation and digital map information' . In Proceedings of the 10th International Mobility Conference (IMC-10), Warwick. Google Scholar\nParkinson, B. W. & Spilker Jr., J. J. (Eds) (1996) 'Overview of GPS Operation and Design'. In Parkinson, B. W., Spilker Jr., J. J. (Eds) Global Positioning System: Theory and Applications (Volume 1, Chapter 2, pp.29-55). Washington DC: American Institute of Aeronautics and Astronautics. Google Scholar\nPeake, P. & Leonard, J.A. (1971) 'The use of heart-rate as an index of stress in blind pedestrians'. Ergonomics, 14 (2), 189-204. Google Scholar , Crossref , Medline\nPetrie, H., Johnson, V., Strothotte, T., Raab, A., Michel, R., Reichert, L. & Schalt, A. (1997) 'MoBIC: An aid to increase the independent mobility of blind travellers'. The British Journal of Visual Impairment , 15 (2), 63-66. Google Scholar , Link\nShingledecker, C. (1983) 'Measuring the mental effort of blind mobility' . Journal of Visual Impairment and Blindness, 77 (7), 334-39. Google Scholar\nWycherley, R.J. & Nicklin, B.H. (1970) 'The heart rate of blind and sighted pedestrians on a town route'. Ergonomics, 13 (2), 181-92. Google Scholar , Crossref , Medline\n""","1.0","""http://journals.sagepub.com/doi/10.1177/026461960302100204""","[-0.472855,51.532848]"
"""University_of_Greenwich""","""Aircraft Accident Statistics and Knowledge Database: Analyzing Passenger Behavior in Aviation Accidents | Journal of Aircraft""","""Aircraft Accident Statistics and Knowledge Database: Analyzing Passenger Behavior in Aviation Accidents\nSubscribe\nAdding the item to your cart\nYour cart has been updated.\nISSN: 0021-8669\nEISSN: 1533-3868\nYour window on major advances in aircraft, the operation of aircraft, and applications of aircraft technology to other fields.\nSite Tools\nAircraft Accident Statistics and Knowledge Database: Analyzing Passenger Behavior in Aviation Accidents\nEdwin R. Galea\nDavid P. Cooney\nCited by\nDu Hong-bing , Yu Xiao-fang , Feng Zhen-yu .  (2017) Simulation on occupant evacuation during aircraft emergency based on cellular automata. 2017 4th International Conference on Transportation Information and Safety (ICTIS), 716-721.\nFang Zhi-Ming , Lv Wei , Liu Xiao-Dong , Song Wei-Guo .  (2014) Study of Boeing 777 Evacuation Using a Finer-grid Civil Aircraft Evacuation Model. Transportation Research Procedia 2, 246-254\nOnline publication date: 1-Jan-2014.\nMartin Hassel , Bjørn Egil Asbjørnslett , Lars Petter Hole .  (2011) Underreporting of maritime accidents to vessel accident databases. Accident Analysis & Prevention 43:6, 2053-2063\nOnline publication date: 1-Nov-2011.\nYu-Hern Chang , Hui-Hua Yang .  (2011) Cabin safety and emergency evacuation: Passenger experience of flight CI-120 accident. Accident Analysis & Prevention 43:3, 1049-1055\nOnline publication date: 1-May-2011.\nMaría José Piñar-Chelso , Jordi Fernández-Castro .  (2011) A New Scale to Evaluate Disruptive Passenger Management by Cabin Crew. Aviation Psychology and Applied Human Factors 1:1, 21-30\nOnline publication date: 1-Jan-2011.\nAnna-Karin Lindberg , Sven Ove Hansson , Carl Rollenhagen .  (2010) Learning from accidents – What more do we need to know?. Safety Science 48:6, 714-721\nOnline publication date: 1-Jul-2010.\nYu-Hern Chang , Hui-Hua Yang .  (2010) Aviation occupant survival factors: An empirical study of the SQ006 accident. Accident Analysis & Prevention 42:2, 695-703\nOnline publication date: 1-Mar-2010.\n""","1.1629282","""https://arc.aiaa.org/doi/10.2514/1.19388""","[-0.003917,51.484493]"
"""Imperial_College_London""","""AIDS Clinical Research & STDs | Full Text | Herald Scholarly Open Access""","""Tables\nABSTRACT\nBackground: Despite high HIV prevalence amongst key populations in strategic travel corridors in Nigeria, there is still very low access to HIV and Sexual Reproductive Health (SRH) services. Targeting men-who-have-sex-with-men, females who sell sex, long distance truck drivers and those who live along the countryâs transport corridors is vital for effective control of the HIV epidemic in Nigeria. Excellence and Friends Management Consult (EFMC) partnered with the Nigerian National Agency for the Control of AIDS (NACA) to improve access to HIV, SRH services educate and test for HIV, and offer referral/treatment to HIV-infected individuals. We report on the need, process and outcome of this project.\nMethods: The Strategic Travelers Outreach Program (STOP) took place between February and August 2016 in Sagamu/Ogere and Obollo-Afor travel corridors. Through community advocacy and outreach, house-to-house, door-to-door and community/office testing, key populations were provided with targeted HIV, SRH and referral services.\nResults: Over 50,000 people were contacted with HIV and SRH education/counselling. 19,275 (M: 10,965, 56.9%; F: 8,310, 43.1%) were tested with all receiving their results same day. General positivity rate was 1.3%: female (1.9%). Brothel-based female sex workers had higher positivity rate (3.2%). The study also revealed prostitutes did not use condoms with their husbands or long-term boyfriends and that knowledge of HIV was poor.\nConclusion: The results highlight an urgent need to mobilize educational and healthcare resources to mitigate HIV transmission along these corridors. A socio-anthropological approach is required to alter long-held beliefs and change risk-seeking behavior.\nKEYWORDS\nAccess; EFMC; HIV and AIDS; Key populations; NACA; Nigeria; Sexual and reproductive health; STOP program; Travel corridors\nIntroduction\nAlthough HIV is most common among key populations (females who sell sex brothel and non-brothel based), Men-who-have-Sex-with-Men (MSM), People who Inject Drugs (PID), military and paramilitary personnel and long distance drivers; their access to reproductive and sexual health is limited [1,2]. Such limitations are also seen among women and men living with HIV as there is little programmatic guidance as to how best ensure access to sexual and reproductive health services [3]. As HIV/AIDS remains a major public health burden around the world, affecting over 36 million people and leading to the death of about 34 million people, there has been an increasing awareness and discussion of the benefits of linkage between Sexual Reproductive Health (SRH) and HIV programs at the policy, systems and service delivery levels [4,5]. Moreover, in 2010, HIV/AIDS was reported as having the 5th largest associated Disability-Adjusted Life Year (DALY) measurement - a measure of overall disease burden, expressed as the number of years lost due to ill-health, disability or early death. HIV had the most prominent DALY association for people of both sexes, aged from 30-44 years in Eastern, Southern and Central Africa, the Caribbean and Thailand [6]. On that background, HIV has remained one of the most important communicable diseases in Nigeria, affecting close to 3.5 million people with about 1,476,741 requiring Anti-Retroviral Drugs (ARV) in 2013 [7].\nDespite a seeming decline in HIV prevalence, the burden of HIV disease is disproportionately high among key populations, such as females who sell sex, men-who-have-sex-with-men, people who inject drugs and long distance transport workers where rates have remained relatively high with spikes in incidence across the world [8-10]. In Nigeria, the Integrated Biological and Behavioral Surveillance Survey (IBBBS) study revealed that Men-who-have-Sex-with-Men (MSM) have the highest prevalence of 22.9%, followed by Brothel-Based Females Who Sell Sex (BBFWSS) with 19.4% prevalence, Non-Brothel-Based Females Who Sell Sex, (NBBFWSS) at 8.6% and long distance truck drivers, who had the lowest prevalence of the at-risk populations at 1.6% [11]. Some of the factors which increase the vulnerability of these groups to HIV infection and maintenance of the epidemic at the population level include risky sexual behavior, current sex-work laws and enforcement-based policies, poor perception of HIV risk, poverty, ineffective STI programming, poor integration of HIV/AIDS and sexual and reproductive health services and gender inequality [12,13].\nFurthermore, key populations have remain highly stigmatized and are unable to access appropriate health care services or exercise their rights to health due to legal restrictions, discrimination and society mediated violence [14]. For instance, in Nigeria, sex trade is an illegal trade and practitioners are criminalized; and when seen, they are arrested, required to either pay fine for their release (bail) and spend some time in prison/detention [15,16]. In the midst of the current global economic recession, it is important to track the magnitude of the HIV/AIDS epidemic, its drivers and consequences, to allocate efficiently the limited resources and to maintain funding for effective HIV/AIDS interventions and treatments [6]. In Nigeria, a particular focus for disease transmission is the âtravel corridorâ, which is a resting zone along the long distance high traffic motorways, where travelers stop for rest, refreshment and ad hoc purchases, before continuing to their final destinations. To track the epidemic, the Nigerian National Agency for the Control of AIDS (NACA) conducted a travel corridor assessment and mapping exercise in 2013, which revealed the need for a specific travel corridor project to assess the microculture and HIV disease prevalence in these communities [17]. The goal of this project was to improve the access of key populations to reproductive and sexual health along two travel corridors in Nigeria.\nMethodology\nPre-project activities\nThe National Agency for the Control of AIDS (NACA) in Nigeria contracted Excellence and Friends Management Consult (EFMC) to execute the Travel Corridor Prevention Initiative (TCPI) in Sagamu (Ogun State, South West Nigeria) and Obollo-Afor (Enugu State, South East Nigeria) Travel Corridors in 2016 (Figure 1). We signed a 6 monthsâ contract with NACA in Abuja, Nigeria in early 2016, developed strategies and operational guidelines and a standard operating procedure using a minimum prevention package initiative [14]. In addition, office and staff accommodation spaces were secured and equipped. Three full time staff and several volunteers were identified, shortlisted, interviewed and engaged. Banners and fliers were designed, developed, pre-tested and mass produced for the project (Figure 2). Community gatekeepers such as community, religion, market women, Brothel-Based Females Who Sell Sex (BBFWSS), Long Distance Drivers (LDD), commercial motorcycle riders, street traders, the police, Federal Road Safety Commission (FRSC), the National Civil Defense Corps and road traffic union leaders were identified, interviewed, their views sought and engaged.\nPermission to execute the work was sought and obtained from appropriate state, institutional and local government authorities. HIV Rapid Diagnostic Kits, information bulletins and fliers and condoms were sourced from NACA and shipped to Sagamu and Obollo-Afor. Startup meetings were held in Abuja, Sagamu and Obollo-Afor. Information, Education and Communication (IEC) materials were produced and distributed.\nProject areas\nSite 1:\nSagamu: Sagamu Local Government Area in Ogun State is bordered to the north by Ikenne LGA, to the west by Obafemi Owode LGA, to the South by Lagos State and to the East by Odogbolu LGA [17]. The LGA covers a total land area of 614 km2 (237 sq). Sagamu LGA has a total population of 253,412 in the 2006 census. The major occupation of people with the local government area includes agriculture, trading and transportation. Agricultural products in Sagamu include cocoa and kola nut, with the largest kola nut collection and distribution center in Nigeria. Sagamu has major mineral deposits of limestone, housing the West African Portland cement industry (Lafarge). Along the transport corridor in Sabo, Isale-Oko, Awolowo, Kara and WAPCO are transport workers, females who sell sex, road side traders, allied workers (mechanics and electricians), etc., [17]. Apart from a teaching hospital, the other healthcare facilities are primary healthcare centers. Stigma, discrimination and criminalization of sex trade and drug use hinders access to healthcare for those most in need.\nSite 2: Obollo-Afor: Obollo-Afor is the head quarter of Udenu LGA and is situated at the regional boundary between the South-East and North-Central geopolitical zones of the country [17]. Obollo-Afor has mostly farmers and traders as inhabitants. The traders sell mostly locally made and harvested farm products such as cassava, yams, cocoyam, cashew and palm kernels. The Afor market of Obollo-Afor runs every four days and is highly patronized by people from the northern and southern regions of the country. This makes Obollo-Afor a very busy travel corridor linking northern Nigeria to the south. However, healthcare services are scarce as the only secondary care center is in another community.\nProject activities\nThis project took place between March-August 2016. A scoping visit was carried out in March 2016 and office space was identified (Figure 3). We staffed and equipped the two project offices. Our key staff from Abuja trained both new officers and volunteers on Reproductive and Sexual Health (RSH), HIV testing and counselling and community programming. The training curriculum covered relevant health issues such as safe sexual habits, defensive driving techniques and use of personal protective equipment for drivers and commercial motor cycle (Okada) riders and healthy marketing behavior and attitudes. We provided HIV Testing and Counseling (HTC), free condoms and referral for those with HIV positive results to treatment sites within the region. Also, we provided free medical examinations including blood pressure checks, blood glucose testing and doctorsâ consultations. The inaugural program was held at both locations and the project was named STOP (Strategic Travelers Outreach Project). Sexual and Reproductive Health (SRH) services were provided at the project office and within the communities. To reach all, house-to-house, door-to-door and shop-to-shop strategies were deployed with staff and volunteers meeting people where they lived or worked. We enlisted everybody seen along the travel corridors on work days (Monday-Saturday) who gave their consent and those that came to the project office every day. Project data were entered into MS Excel and analyzed.\nQualitative studies\nOver 100 gatekeepers including community, religious groups, market women, BBFWSS, LDDs, commercial motorcycle riders, street traders, the police, National Road Safety Commission, National Civil Defense Corps, and road traffic union leaders, representing the target population were identified and interviewed. Females who sell sex were also interviewed on their sexual habits, knowledge of HIV and AIDS and prevention practices.\nResults\nOver 50,000 people drawn from LDDs, commercial motorcycle riders, street traders, market women, military and paramilitary personnel, BBFWSS and all STOP club members were reached through the STOP project and provided with reproductive and sexual education, support and guidance. Out of this, 19,275 persons (M=10965, 56.9%; F=8310, 43.1%) people within the travel corridor were tested for HIV infection and received their results. In addition, 422 people were trained on SRH, safe driving practices, first aid management and other related topics resulting in the establishment of the STOP clubs in both travel corridors. Of 19,275 tested, 243 (1.3%) were positive for HIV. Higher positivity rate was seen among females (113, 1.8%) as compared to males (41, 0.8%). Males and Females from Sagamu had higher positively that those in Obollo-Afor (Table 1). In Sagamu travel corridor, activities took place in two separate locations-Sagamu town and Ogere. Overall positivity rate out of 9606 persons tested in this location was 1.8%. Females Who Sell Sex (FWSS) had the highest positivity rate of 7.2% with Ogere having a higher rate of 7.8% as against Sagamuâs 6.5% (Table 2). Similar reports was seen in Obollo-Afor where FWSS had the highest positivity rate, followed by military and paramilitary persons as shown in table 2. Among 220 key stakeholders tested in Obollo-Afor, 4.5% were positive for HIV. Females who sell sex also had the highest positivity rate (33.3%) as shown in table 3.\nGenerally, people aged 25-49 years-old had the highest positivity rate (2.4%). In Ogere alone, those within the age bracket 20-24 years had a higher prevalence rate of 3.9%, as against 3.8% for those 25-49 years (Table 4). A similar finding was made in Obollo-Afor where persons aged 25-29 years had the highest age specific positivity rate (Table 5).\nInterviews of FWSS revealed that they did not use condoms with their husbands or long-term boyfriends. Also, their knowledge of HIV transmission risk was poor as less than 10% had a good understanding of the real meaning of HIV, transmission channels and prevention strategies. Unstandardized interviews of some FWSS revealed that a number of them knew their status (some for over 3 years), were already on antiretroviral medications or had stopped their medications because of lack of access to antiretroviral therapy (as some moved out from their previous locations to seek acceptance in new locations). Also, a number of them were married with grown-up children, had boyfriends with whom they do not use condoms and were unwilling to share their positive HIV status with their partners, clients and boyfriends. Finally, less than 10% of all those surveyed had adequate knowledge of HIV, its origin, channels of transmission and control strategies.\nDiscussion\nThe project improved access to reproductive and sexual health for over 50,000 people who were reached with sexual health education, free condoms, HIV testing and counselling, and management of sexually transmitted diseases. Of the over 19,000 people tested in the two travel corridors, 1.3% were positive for HIV with a higher prevalence in the Sagamu travel corridor that links the South-West to the Northern part of Nigeria. The 1.3% overall prevalence was similar to the 1.3% for Enugu State reported by the National HIV&AIDS and Reproductive Health Survey, but higher than the 0.6% reported for Ogun State by the same report [18]. The individual travel corridors positivity rates of 1.8% for Sagamu/Ogere and 0.8% for Obollo-Afor showed significant reversal when compared with the state figures from the above study of 0.6% and 1.3% for Ogun and Enugu States respectively. However, as the individuals in these travel corridors represent people of the various geopolitical zones, the findings were similar to the zonal reports which indicates that prevalence in the South-West of Nigeria is higher than that of the South East (2.8 vs 1.8%) [18].\nThe FWSS had the highest prevalence of 7.8% in Ogere, 6.5% in Sagamu and 1.9% in Obollo-Afor while military and paramilitary personnel had the lowest positivity rate in Sagamu/Ogere axis but higher rates in Obollo-Afor. These finding are in line with the recent Nigerian study which showed an overall HIV prevalence among the target groups of 9.5% with the highest prevalence seen among MSM (22.9%), followed by brothel based females who sell sex (19.4%), non-brothel based females who sell sex (8.6%), people who inject drugs (3.4%), police (2.5%), transport workers (1.6%) and armed forces (1.5%) [11]. A field study by EFMC in 2012 showed a similar high prevalence among females who sell sex with some brothel based females who sell sex having positivity rate of over 50% [19].\nThe different positivity rate among the military and paramilitary groups in both regions may reflect their higher level of mobility in the Nigerian South East, compared to the South-West zone. Also, there was no identifiable pattern for the positively rates seen in the rest of the key populations. This calls for a more detailed controlled study to uncover the sociological factors responsible for the current positive rates seen in the communities. During the project, it was found that FWSS were not disclosing their status, not even to their long-term boyfriends. The non-disclosure of status by FWSS shows the dangers men face when they patronize FWSS. This risk is high as studies have shown prevalence rates of up to 50% among BBFWSS [11,19]. What is more disturbing is the fact that some male customers pay extra money to the FWSS for sex without condoms, a practice that increases the possibility of new or multiple infection(s). The high HIV positivity level seen in travelers along the Obollo-Afor corridor is another issue of concern, as recent studies show a rising prevalence in Anambra (from 8.7% in 2010 to 9.7% in 2014) and Imo (3.0% to 7.5%) States, but a slightly reducing prevalence in the other States in the South East of Nigeria, including Enugu State, which decreased from 5.1% in 2010 to 4.9% in 2014 [20,21]. If the majority of those with HIV were traveling to the region (this was not studied in this work), the region may be at the risk of rising prevalence, if nothing is done.\nMore males were found to be reactive early in life (2.7%). Prevalence among the various sex groups gradually decreased until 20-24 years and begins to rise again (Table 5 and Figure 4). Among females, the trend was different. Females aged 0-14 years had the lowest HIV rate, but the rate increases in a linear pattern over the age-groups considered up to 25-49 years and then began to decline (Figure 4). This study, therefore, observed that more males were found to be HIV-infected at younger ages, with HIV prevalence decreasing up to 25 years, then increasing after 25 years of age. With females, the HIV prevalence increased gradually increases up to 49 and by 50 years and above, began to decline. An STI screening and HTC conducted among FWSS at other locations in Enugu State in 2016 showed that HIV prevalence was highest among those aged 25-49 [21]. Beyond sexual exposure, the factors behind this trend are not immediately apparent. The reason for this trend will need further exploration and thus will require future expanded qualitative studies. Refusal to disclose HIV status, even to close sexual partners, can negatively affect the overall outcome of any control measure. Working to improve their HIV knowledge will facilitate control efforts.\nLimitations\nThis study may have suffered from self-selection bias as some who already knew their HIV status may have opted out from the exercise [22,23]. If this is the case, the positivity rate in this report may not be representative of the population studies.\nConclusion\nHIV remains a major communicable health challenge in Nigeria, especially among the key populations such as females who sell sex and long distance travelers. As there are few programs targeting the informal sectors, coupled with limited access to other reproductive and sexual health services, these hotspots will remain a major cause of concern in the control of HIV epidemic until active steps are taken to reduce infections and provide compressive care and support in a sustainable manner. Expanding the STOP project may therefore be a step in the right direction.\nRecommendations\nAccess to sexual and reproductive healthcare services should be improved upon for key populations especially those in high and strategic traffic travel corridors like Obollo-Afor and Sagamu/Ogere axis. Beyond behavioral change and STI/HCT interventions, STOP should be funded to provide anti-retroviral therapy services as this will minimize loss to follow-up. This will accelerate Nigeriaâs progress towards the 90-90-90 target by 2020 and contribute to slowing the rate of new HIV infections. There is a need to institutionalize travel corridor projects in all key travel corridors in Nigeria by ensuring it is reflected in the forthcoming Nigerian National HIV/AIDS Strategic Plan. The Nigerian government and development partners should mobilize resources to stop HIV transmission in these marginalized key communities through community targeted projects. Finally, there is the need to utilize socio-anthropological approaches to change the behavior of people in Nigeria towards HIV prevention in Nigeria.\nReferences\n""","0.34273112","""http://www.heraldopenaccess.us/fulltext/AIDS-Clinical-Research-&-STDs/Improving-Access-to-HIV-and-Sexual-Reproductive-Health-Services-for-Key-Populations-along-Nigerias-Transport-Corridors-The-STOP-Project.php""","[-0.178219,51.500505]"
"""University_of_Aberdeen""","""Discriminant content validity: A quantitative methodology for assessing content of theory-based measures, with illustrative applications - Johnston - 2014 - British Journal of Health Psychology - Wiley Online Library""","""British Journal of Health Psychology\nDiscriminant content validity: A quantitative methodology for assessing content of theory-based measures, with illustrative applications\nAuthors\nCorresponding author\nInstitute of Applied Health Sciences, University of Aberdeen, UK\nCorrespondence should be addressed to Marie Johnston, Aberdeen Health Psychology Group, College of Life Sciences and Medicine, University of Aberdeen, Health Sciences Building, Foresterhill, Aberdeen AB25 2ZD, UK (email: m.johnston@abdn.ac.uk ).\nCited by (CrossRef): 15 articles Check for updates\nCitation tools\nAbstract\nObjectives\nIn studies involving theoretical constructs, it is important that measures have good content validity and that there is not contamination of measures by content from other constructs. While reliability and construct validity are routinely reported, to date, there has not been a satisfactory, transparent, and systematic method of assessing and reporting content validity. In this paper, we describe a methodology of discriminant content validity (DCV) and illustrate its application in three studies.\nMethods\nDiscriminant content validity involves six steps: construct definition, item selection, judge identification, judgement format, single-sample test of content validity, and assessment of discriminant items. In three studies, these steps were applied to a measure of illness perceptions (IPQ-R) and control cognitions.\nResults\nThe IPQ-R performed well with most items being purely related to their target construct, although timeline and consequences had small problems. By contrast, the study of control cognitions identified problems in measuring constructs independently. In the final study, direct estimation response formats for theory of planned behaviour constructs were found to have as good DCV as Likert format.\nConclusions\nThe DCV method allowed quantitative assessment of each item and can therefore inform the content validity of the measures assessed. The methods can be applied to assess content validity before or after collecting data to select the appropriate items to measure theoretical constructs. Further, the data reported for each item in Appendix S1 can be used in item or measure selection.\nStatement of contribution\nWhat is already known on this subject?\nThere are agreed methods of assessing and reporting construct validity of measures of theoretical constructs, but not their content validity. Content validity is rarely reported in a systematic and transparent manner.\nWhat does this study add?\nThe paper proposes discriminant content validity (DCV), a systematic and transparent method of assessing and reporting whether items assess the intended theoretical construct and only that construct.\nIn three studies, DCV was applied to measures of illness perceptions, control cognitions, and theory of planned behaviour response formats.\nAppendix S1 gives content validity indices for each item of each questionnaire investigated.\nDiscriminant content validity is ideally applied while the measure is being developed, before using to measure the construct(s), but can also be applied after using a measure.\nAppendix S1. Tables S1, S2 and S3 for Studies 1, 2 and 3.\nTable S1. Study 1: Illness representation measures in the Illness Perceptions Questionnaire Revised (IPQ-R) items.\nTable S2. Study 2: Perceived behavioural control (PBC) and self-efficacy (SE).\nTable S3. Study 3: Theory of planned behaviour: content validity of TPB items with two response formats.\nPlease note: Wiley-Blackwell is not responsible for the content or functionality of any supporting information supplied by the authors. Any queries (other than missing content) should be directed to the corresponding author for the article.\nRelated content\nArticles related to the one you are viewing\nCiting Literature\nNumber of times cited: 15\n1\nHein De Vries, Thinking is the best way to travel: towards an ecological interactionist approach: a comment on Peters and Crutzen, Health Psychology Review, 2017, 11, 2, 135\nCrossRef\n2\nCheryl Bell, Derek Johnston, Julia Allan, Beth Pollard, Marie Johnston, What do Demand-Control and Effort-Reward work stress questionnaires really measure? A discriminant content validity study of relevance and representativeness of measures, British Journal of Health Psychology, 2017, 22, 2, 295\nWiley Online Library\n3\nIngrid Koller, Michael R. Levenson, Judith Glück, What Do You Think You Are Measuring? A Mixed-Methods Procedure for Assessing the Content Validity of Test Items and Theory-Based Scaling, Frontiers in Psychology, 2017, 8\nCrossRef\n4\nMarie Johnston, A science for all reasons: A comment on Ogden (2016), Health Psychology Review, 2016, 10, 3, 256\nCrossRef\n5\nAnnerieke C. van Groenestijn, Esther T. Kruitwagen-van Reenen, Johanna M. A. Visser-Meily, Leonard H. van den Berg, Carin D. Schröder, Associations between psychological factors and health-related quality of life and global quality of life in patients with ALS: a systematic review, Health and Quality of Life Outcomes, 2016, 14, 1\nCrossRef\n6\nJulie Boiché, Gonzalo Marchant, Virginie Nicaise, Aline Bison, Development of the Generic Multifaceted Automaticity Scale (GMAS) and preliminary validation for physical activity, Psychology of Sport and Exercise, 2016, 25, 60\nCrossRef\n7\nRichard Cooke, Mary Dahdah, Paul Norman, David P. French, How well does the theory of planned behaviour predict alcohol consumption? A systematic review and meta-analysis, Health Psychology Review, 2016, 10, 2, 148\nCrossRef\n8\nVolker Stein, Arnd Wiedemann, Risk governance: conceptualization, tasks, and research agenda, Journal of Business Economics, 2016, 86, 8, 813\nCrossRef\n9\nDavid M. Williams, Ryan E. Rhodes, The confounded self-efficacy construct: conceptual analysis and recommendations for future research, Health Psychology Review, 2016, 10, 2, 113\nCrossRef\n10\nAlex Gillespie, Tom W Reader, The Healthcare Complaints Analysis Tool: development and reliability testing of a method for service monitoring and organisational learning, BMJ Quality & Safety, 2016, 25, 12, 937\nCrossRef\n11\nChristine McMonagle, Susan Rasmussen, Mark A. Elliott, Diane Dixon, Use of the ICF to investigate impairment, activity limitation and participation restriction in people using ankle-foot orthoses to manage mobility disabilities, Disability and Rehabilitation, 2016, 38, 6, 605\nCrossRef\n12\nSusan Michie, Caroline E Wood, Marie Johnston, Charles Abraham, Jill J Francis, Wendy Hardeman, Behaviour change techniques: the development and evaluation of a taxonomic method for reporting and describing behaviour change interventions (a suite of five studies involving consensus methods, randomised controlled trials and analysis of qualitative data), Health Technology Assessment, 2015, 19, 99, 1\n""","0.27002954","""http://onlinelibrary.wiley.com/doi/10.1111/bjhp.12095/abstract""","[-2.099122,57.165019]"
"""University_of_Aberdeen""","""Meeting the on-line needs of disabled tourists: an assessment of UK-based hotel websites - Williams - 2006 - International Journal of Tourism Research - Wiley Online Library""","""International Journal of Tourism Research\nPrevious article in issue: The changing profile of caravanners in Australia\nPrevious article in issue: The changing profile of caravanners in Australia\nMeeting the on-line needs of disabled tourists: an assessment of UK-based hotel websites\nAuthors\nUniversity of Aberdeen Business School, Aberdeen, Scotland, UK\nUniversity of Aberdeen Business School, Edward Wright Building, Aberdeen AB24 3QY, Scotland, UK.\nCited by (CrossRef): 12 articles Check for updates\nCitation tools\nCiting literature\nAbstract\nThe easy exchange of rich information between often geographically dispersed parties is an important precursor of successful tourism transactions. Internet-based technologies, in particular the World Wide Web, offer possibilities to both buyers and sellers to exchange information without the constraints of geography and time diminishing its richness. The disabled, representing a significant part of any marketplace, may, however, have difficulties accessing the content of the Web and therefore sharing the benefits of rich information exchange. This is the principal concern of ‘Web content accessibility’. Focusing on the tourism sector, in particular UK-based hotels, this paper examines the accessibility of their websites. However, recognising that it is not just access to information that is important for the disabled, but also the quality of that information, the paper also examines the extent to which the information contained on websites serves their particular needs. Utilising the accessibility testing software ‘Bobby’, disappointingly low levels of Web content accessibility were found amongst the sample of websites. Against a framework of information needs developed from criteria provided by disability organisations, the sample also revealed disappointingly low levels of specific (relevant) information for the disabled. Copyright © 2006 John Wiley & Sons, Ltd.\nArticles related to the one you are viewing\nCiting Literature\nNumber of times cited: 12\n1\nStephanie Morris, Scholah Kazi, Emerging trends regarding accessible accommodation in Dubai luxury hotels, Worldwide Hospitality and Tourism Themes, 2014, 6, 4, 317\nCrossRef\n2\nWei Wang, Shu Cole, Perceived Onboard Service Needs of Passengers with Mobility Limitations: An Investigation among Flight Attendants, Asia Pacific Journal of Tourism Research, 2014, 19, 11, 1239\nCrossRef\n3\nEleni Michopoulou, Dimitrios Buhalis, Information provision for challenging markets: The case of the accessibility requiring market in the context of tourism, Information & Management, 2013, 50, 5, 229\nCrossRef\n4\nJennie Small, Simon Darcy, Tanya Packer, The embodied tourist experiences of people with vision impairment: Management implications beyond the visual gaze, Tourism Management, 2012, 33, 4, 941\nCrossRef\n5\nSimon Darcy, Developing Sustainable Approaches to Accessible Accommodation Information Provision: A Foundation for Strategic Knowledge Management, Tourism Recreation Research, 2011, 36, 2, 141\nCrossRef\n6\nDimitrios Buhalis, Eleni Michopoulou, Information-enabled tourism destination marketing: addressing the accessibility market, Current Issues in Tourism, 2011, 14, 2, 145\nCrossRef\n7\nSimon Darcy, Shane Pegg, Towards Strategic Intent: Perceptions of disability service provision amongst hotel accommodation managers, International Journal of Hospitality Management, 2011, 30, 2, 468\nCrossRef\n8\nVictoria Richards, Annette Pritchard, Nigel Morgan, (Re)Envisioning tourism and visual impairment, Annals of Tourism Research, 2010, 37, 4, 1097\n9\nSimon Darcy, Inherent complexity: Disability, accessible tourism and accommodation information preferences, Tourism Management, 2010, 31, 6, 816\nCrossRef\n10\nSimon Darcy, Tracy Taylor, Disability citizenship: an Australian human rights analysis of the cultural industries, Leisure Studies, 2009, 28, 4, 419\nCrossRef\n11\nAna Isabel Polo Peña, Dolores María Frías Jamilena, The relationship between business characteristics and ICT deployment in the rural tourism sector. The case of Spain, International Journal of Tourism Research, 2009, n/a\n""","0.5153248","""http://onlinelibrary.wiley.com/doi/10.1002/jtr.547/abstract""","[-2.099122,57.165019]"
"""University_of_Glasgow""","""The Accessible Electronics Laboratory  - Enlighten: Publications""","""Enlighten: Publications\nIn this section\nThe Accessible Electronics Laboratory\nHersh, M.A. , Baker, N., McCloud, M. and Weightman, B., (2004)     The Accessible Electronics Laboratory.            In: International Engineering Education Conference, Wolverhampton, England, 2004,\nFull text not currently available from Enlighten.\nAbstract\nThe Disability Discrimination Act requires schools, universities and colleges not to discriminate against disabled students and to make reasonable adjustments. Laboratories are an integral part of all engineering courses. Therefore they have to be accessible to all students and reasonable adjustments should include making laboratories accessible and not excusing disabled students from them. Adjustments to make laboratories more accessible to disabled students generally benefit all students and staff. Making laboratories accessible increases the pool of talent that can be attracted into the engineering profession. Many disabled people have had to exercise a lot of ingenuity to cope with an inaccessible world. The engineering profession could benefit from this ingenuity.\nItem Type:\n""","1.0","""http://eprints.gla.ac.uk/31537/""","[-4.28836,55.871751]"
"""Coventry_University""","""Measuring mobility and transport services: The METPEX project — Coventry University""","""RIS\nWoodcock, A. , Susilo, Y., Diana, M., Abenoza, R., Pirra, M., & Tovey, M. (2018). Measuring mobility and transport services: The METPEX project . In N. Stanton (Ed.), Advances in Human Aspects of Transportation - Proceedings of the AHFE 2017 International Conference on Human Factors in Transportation, 2017 (Vol. 597, pp. 1036-1045). (Advances in Intelligent Systems and Computing; Vol. 597). Springer Verlag. DOI: 10.1007/978-3-319-60441-1_98\nMeasuring mobility and transport services : The METPEX project. / Woodcock, Andree ; Susilo, Yusak; Diana, Marco; Abenoza, Roberto; Pirra, Miriam; Tovey, Michael.\nAdvances in Human Aspects of Transportation - Proceedings of the AHFE 2017 International Conference on Human Factors in Transportation, 2017. ed. / Neville Stanton. Vol. 597 Springer Verlag, 2018. p. 1036-1045 (Advances in Intelligent Systems and Computing; Vol. 597).\nResearch output: Chapter in Book/Report/Conference proceeding › Conference proceeding\nWoodcock, A , Susilo, Y, Diana, M, Abenoza, R, Pirra, M & Tovey, M 2018, Measuring mobility and transport services: The METPEX project . in N Stanton (ed.), Advances in Human Aspects of Transportation - Proceedings of the AHFE 2017 International Conference on Human Factors in Transportation, 2017. vol. 597, Advances in Intelligent Systems and Computing, vol. 597, Springer Verlag, pp. 1036-1045, International Conference on Applied Human Factors and Ergonomics, Los Angeles, United States, 17-21 July. DOI: 10.1007/978-3-319-60441-1_98\nWoodcock A , Susilo Y, Diana M, Abenoza R, Pirra M, Tovey M. Measuring mobility and transport services: The METPEX project . In Stanton N, editor, Advances in Human Aspects of Transportation - Proceedings of the AHFE 2017 International Conference on Human Factors in Transportation, 2017. Vol. 597. Springer Verlag. 2018. p. 1036-1045. (Advances in Intelligent Systems and Computing). Available from, DOI: 10.1007/978-3-319-60441-1_98\nWoodcock, Andree ; Susilo, Yusak; Diana, Marco; Abenoza, Roberto; Pirra, Miriam; Tovey, Michael / Measuring mobility and transport services : The METPEX project.\nAdvances in Human Aspects of Transportation - Proceedings of the AHFE 2017 International Conference on Human Factors in Transportation, 2017. ed. / Neville Stanton. Vol. 597 Springer Verlag, 2018. p. 1036-1045 (Advances in Intelligent Systems and Computing; Vol. 597).\nResearch output: Chapter in Book/Report/Conference proceeding › Conference proceeding\n@inbook{9e75b9356b5548bbacaad83f780520da,\ntitle     = \""Measuring mobility and transport services: The METPEX project\"",\nabstract  = \""Public transport is key to access social, economic, civic and cultural life. However, there is still a need to better understand (1) the needs of all transport users and (2) transport provision in cities and regions. The development of an inclusive, validated, passenger experience measurement instrument is the first step in understanding the whole journey, multi-modal journeys. Such a validated tool would enable resources to be focused on areas which travelers felt most important. Such information could be used to create high quality, user centred, integrated, accessible public transport services, capable of attracting and retaining public transport users whilst meeting sustainability targets. This paper describes the METPEX project and the derivation of a set of Key Performance Indicators.\"",\nkeywords  = \""Journey mode, KPIs, Measurement instrument, Passenger experience, User needs\"",\nauthor    = \""Andree Woodcock and Yusak Susilo and Marco Diana and Roberto Abenoza and Miriam Pirra and Michael Tovey\"",\nyear      = \""2018\"",\n""","0.91442966","""https://pureportal.coventry.ac.uk/en/publications/measuring-mobility-and-transport-services-the-metpex-project""","[-1.505722,52.407999]"
"""Imperial_College_London""","""Modeling the bilateral micro-searching behavior for urban taxi services using the absorbing markov chain approach - Wong - 2005 - Journal of Advanced Transportation - Wiley Online Library""","""Journal of Advanced Transportation\nModeling the bilateral micro-searching behavior for urban taxi services using the absorbing markov chain approach\nAuthors\nK.I. Wong and M.G.H. Bell, Centre for Transport Studies, Department of Civil and Environmental Engineering, Imperial College London, England, UK\nS. C. Wong,\nS.C. Wong, Department of Civil Engineering, The University of Hong Kong, Hong Kong. P.R., China\nM. G. H. Bell,\nK.I. Wong and M.G.H. Bell, Centre for Transport Studies, Department of Civil and Environmental Engineering, Imperial College London, England, UK\nHai Yang\nHai Yang, Department of Civil Engineering, The Hong Kong University of Science and Technology, Hong Kong, P.R., China\nFirst published:\nCited by (CrossRef): 22 articles Check for updates\nCitation tools\nFunding Information\nAbstract\nThis paper develops a mathematical model that is based on the absorbing Markov chain approach to describe taxi movements, taking into account the stochastic searching processes of taxis in a network. The local searching behavior of taxis is specified by a logit form, and the O-D demand of passengers is estimated as a logit model with a choice of taxi meeting point. The relationship between customer and taxi waiting times is modeled by a double-ended queuing system. The problem is solved with a set of non-linear equations, and some interesting results are presented. The research provides a novel and potentially useful formulation for describing the urban taxi services in a network.\nCroucher Foundation of Hong Kong\nResearch Grants Council of the Hong Kong Special Administrative Region. Grant Number: HKUST6107/03E and HKU7134/03E\nRelated content\nArticles related to the one you are viewing\nCiting Literature\nNumber of times cited: 22\n1\nZhong Zheng, Soora Rasouli, Harry Timmermans, Modeling taxi driver anticipatory behavior, Computers, Environment and Urban Systems, 2018\nCrossRef\n2\nYuxiong Ji, Yuchuan Du, Yue Liu, H. Michael Zhang, Empirical Behavioral Study of Airport-Serving Taxi Drivers Using Automatic Vehicle Location Data, Journal of Urban Planning and Development, 2017, 143, 1, 04016026\nCrossRef\n3\nLuliang Tang, Fei Sun, Zihan Kan, Chang Ren, Luling Cheng, Uncovering Distribution Patterns of High Performance Taxis from Big Trace Data, ISPRS International Journal of Geo-Information, 2017, 6, 12, 134\nCrossRef\n4\nYing Shi, Zhaotong Lian, Optimization and strategic behavior in a passenger–taxi service system, European Journal of Operational Research, 2016, 249, 3, 1024\nCrossRef\n5\nR.C.P. Wong, W.Y. Szeto, S.C. Wong, A two-stage approach to modeling vacant taxi movements, Transportation Research Part C: Emerging Technologies, 2015, 59, 147\nCrossRef\n6\nR.C.P. Wong, W.Y. Szeto, S.C. Wong, A Two-Stage Approach to Modeling Vacant Taxi Movements, Transportation Research Procedia, 2015, 7, 254\nCrossRef\n7\nLuis M. Martinez, Gonçalo H. A. Correia, José M. Viegas, An agent-based simulation model to assess the impacts of introducing a shared-taxi system: an application to Lisbon (Portugal), Journal of Advanced Transportation, 2015, 49, 3, 475\nWiley Online Library\n8\nR. C. P. Wong, W. Y. Szeto, S. C. Wong, Behavior of taxi customers in hailing vacant taxis: a nested logit model for policy analysis, Journal of Advanced Transportation, 2015, 49, 8, 867\nWiley Online Library\n9\nR.C.P. Wong, W.Y. Szeto, S.C. Wong, A cell-based logit-opportunity taxi customer-search model, Transportation Research Part C: Emerging Technologies, 2014, 48, 84\nCrossRef\n10\nR.C.P. Wong, W.Y. Szeto, S.C. Wong, Bi-level decisions of vacant taxi drivers traveling towards taxi stands in customer-search: Modeling methodology and policy implications, Transport Policy, 2014, 33, 73\nCrossRef\n11\nR.C.P. Wong, W.Y. Szeto, S.C. Wong, Hai Yang, Modelling multi-period customer-searching behaviour of taxi drivers, Transportmetrica B: Transport Dynamics, 2014, 2, 1, 40\nCrossRef\n12\nWai Yuen Szeto, Ryan Cheuk Pong Wong, Sze Chun Wong, Hai Yang, A time-dependent logit-based taxi customer-search model, International Journal of Urban Sciences, 2013, 17, 2, 184\nCrossRef\n13\nXianbiao Hu, Song Gao, Yi-Chang Chiu, Dung-Ying Lin, Modeling Routing Behavior for Vacant Taxicabs in Urban Traffic Networks, Transportation Research Record: Journal of the Transportation Research Board, 2012, 2284, 81\nCrossRef\n14\nJosep Maria Salanova, Miquel Estrada, Georgia Aifadopoulou, Evangelos Mitsakis, A review of the modeling of taxi services, Procedia - Social and Behavioral Sciences, 2011, 20, 150\nCrossRef\n16\nHai Yang, Teng Yang, Equilibrium properties of taxi markets with search frictions, Transportation Research Part B: Methodological, 2011, 45, 4, 696\nCrossRef\n17\nR. M. N. T. Sirisoma, S. C. Wong, W. H. K. Lam, D. Wang, H. Yang, P. Zhang, Empirical evidence for taxi customer-search model, Proceedings of the Institution of Civil Engineers - Transport, 2010, 163, 4, 203\nCrossRef\n18\nHai Yang, Cowina W.Y. Leung, S.C. Wong, Michael G.H. Bell, Equilibria of bilateral taxi–customer searching and meeting on networks, Transportation Research Part B: Methodological, 2010, 44, 8-9, 1067\nCrossRef\n19\nHai Yang, C.S. Fung, K.I. Wong, S.C. Wong, Nonlinear pricing of taxi services, Transportation Research Part A: Policy and Practice, 2010, 44, 5, 337\nCrossRef\n20\nK.I. Wong, S.C. Wong, Hai Yang, J.H. Wu, Modeling urban taxi services with multiple user classes and vehicle modes, Transportation Research Part B: Methodological, 2008, 42, 10, 985\n""","0.7311665","""http://onlinelibrary.wiley.com/doi/10.1002/atr.5670390107/abstract""","[-0.178219,51.500505]"
"""University_of_Surrey""","""Finding effective pathways to sustainable mobility: bridging the science–policy gap: Journal of Sustainable Tourism: Vol 24, No 3""","""关键词: 气候变化 ,  社会技术因素 ,  科技神话 ,  运输禁忌 ,  理想期货\nIntroduction\nDemand is increasing for all transport modes. The transport sector, including tourism and all other transport motivations, is growing more rapidly than most other sectors and is currently responsible for approximately 23% of global energy-related CO2 emissions (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ). Because of the sector's rising contributions to climate change, considerable effort has recently been invested by researchers to try to understand if people are willing to voluntarily change their tourism and transport behaviour (e.g. Becken, 2007 Becken, S. (2007). Tourists' perception of international air travel's impact on the global climate and potential climate change policies. Journal of Sustainable Tourism, 15, 351–368. [Taylor & Francis Online]   [Google Scholar] ; Higham, Cohen, Cavaliere, Reis, & Finkler, 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ; Kroesen, 2013 Kroesen, M. (2013). Exploring people's viewpoints on air travel and climate change: Understanding inconsistencies. Journal of Sustainable Tourism, 21(2), 271–290. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Miller, Rathouse, Scarles, Holmes, & Tribe, 2010 Miller, G., Rathouse, K., Scarles, C., Holmes, K., & Tribe, J. (2010). Public understanding of sustainable tourism. Annals of Tourism Research, 37(3), 627–645. [Crossref] , [Web of Science ®]   [Google Scholar] ). The weight of evidence clearly shows that, while awareness of the impact of mobility on climate change, and particularly that of air travel, is growing, there has been little if any actual behavioural change by tourists to travel less or to change travel modes (Higham, Cohen, Peeters, & Gössling, 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). This realisation has been captured in numerous publications evidencing or attempting to explain an awareness–attitude or attitude–behaviour gap (e.g. Antimova, Nawijn, & Peeters, 2012 Antimova, R., Nawijn, J., & Peeters, P. (2012). The awareness/attitude gap in sustainable tourism: A theoretical perspective. Tourism Review, 67(3), 7–16. [Crossref]   [Google Scholar] ; Cohen, Higham, & Reis, 2013 Cohen, S.A., Higham, J., & Reis, A. (2013). Sociological barriers to sustainable discretionary air travel behaviour. Journal of Sustainable Tourism, 21(7), 982–998. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hares, Dickinson, & Wilkes, 2010 Hares, A., Dickinson, J., & Wilkes, K. (2010). Climate change and the air travel decisions of UK tourists. Journal of Transport Geography, 18(3), 466–473. [Crossref] , [Web of Science ®]   [Google Scholar] ; Juvan & Dolnicar, 2014 Juvan, E., & Dolnicar, S. (2014). The attitude-behaviour gap in sustainable tourism. Annals of Tourism Research, 48, 76–95. [Crossref] , [Web of Science ®]   [Google Scholar] ). Meanwhile, calls were sounding that the very concept of voluntary behaviour change itself was trapped within the constraints of neoliberalism (Barr, Gilg, & Shaw, 2011 Barr, S., Gilg, A., & Shaw, G. (2011). Citizens, consumers and sustainability: (Re)framing environmental practice in an age of climate change. Global Environmental Change, 21, 1224–1233. [Crossref] , [Web of Science ®]   [Google Scholar] ; Schwanen, Banister, & Anable, 2011 Schwanen, T., Banister, D., & Anable, J. (2011). Scientific research about climate change mitigation in transport: A critical review. Transportation Research Part A, 45, 993–1006. [Crossref]   [Google Scholar] ). These calls urged the academy to pay closer attention to the political, social and material systems in which consumption practices are structured, arguing that the “carbon capability” (Whitmarsh, Seyfang, & O'Neill, 2011 Whitmarsh, L., Seyfang, G., & O'Neill, S. (2011). Public engagement with carbon and climate change: To what extent is the public “carbon capable”? Global Environmental Change, 21, 56–65. [Crossref] , [Web of Science ®]   [Google Scholar] ) of the public is limited by the “systems of provision” (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), which create what Schwanen et al. ( 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ) refer to as “path dependencies”.\nA turn towards path dependencies does not suggest that attempts to achieve public behavioural change should be abandoned. Rather it emphasises that devolving the problem of tourism and transport's impacts on climate change to individuals is a limited framing. In addition to social marketing efforts aimed downstream at effecting behavioural change in publics (see Hall, in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , this issue), and a continued drive by industry towards marginal efficiency gains available in aviation technologies (Cumpsty et al., 2010 Cumpsty, N., Alonso, J., Eury, S., Maurice, L., Nas, B., Ralph, M., & Sawyer, R. (2010). Report of the independent experts on medium and long term goals for aviation fuel burn reduction from technology. Montreal: ICAO.  [Google Scholar] ; Peeters & Middel, 2007 Peeters, P.M., & Middel, J. (2007). Historical and future development of air transport fuel efficiency. In R. Sausen, A. Blum, D.S. Lee, & C. Brüning (Eds.), Proceedings of an International Conference on Transport, Atmosphere and Climate (TAC), Oxford, United Kingdom, 26–29 June 2006 (pp. 42–47). Oberpfaffenhofen: DLR Institut für Physic der Atmosphäre.  [Google Scholar] ), it is imperative that research focuses on how the radical socio-technical transitions that are necessary to put the tourism and transport sectors on a sustainable emissions path can be achieved. Technical solutions alone will be too little and too late (Chèze, Chevallier, & Gastineau, 2013 Chèze, B., Chevallier, J., & Gastineau, P. (2013). Will technological progress be sufficient to stabilize CO2 emissions from air transport in the mid-term? (No. Les cahiers de l'économie – no. 94). Rueil-Malmaison: Centre Économie et Gestion.  [Google Scholar] ; Peeters, Higham, Kutzner, Cohen, & Gössling, under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ). This includes not only a recognition that the present socio-technical landscape is dominated by neoliberal, techno-centric and ecological modernisation values (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hopkins & Higham, 2012 Hopkins, D., & Higham, J.E.S. (2012). Framework conventions for climate change: An analysis of global framework conventions with reference to resource governance and environmental management approaches in New Zealand. In A. Holden & D. Fennell (Eds.), A handbook of tourism and the environment (pp. 227–240). London: Routledge.  [Google Scholar] ), but also the need for a concerted effort by tourism and transport researchers to become active advocates of pathways to structural change, influence policy learning and provide politicians with tools to simulate policy-making and its effects.\nThe Freiburg 2014 workshop\nThe Freiburg 2014 workshop, held in Freiburg im Breisgau in Germany (1–4 July 2014), sought to address the inability of policy-makers and other stakeholders to change the tourism mobility system towards sustainable development. Its objectives stemmed directly from the Freiburg 2012 workshop, the results of which were disseminated in the Journal of Sustainable Tourism (volume 21, issue 7; see Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) and in an edited book (Cohen, Higham, Peeters, & Gössling, 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] ).\nA key outcome from Freiburg 2012 was the conclusion that the public are generally unwilling or unable to change tourism and transport behaviour based on an awareness of environmental impacts, and specifically climate change. It was concluded that “the autonomy of individual pro-environmental response, when set within the systems of provision in late-capitalist consumer society, is fraught with challenge” (Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , p. 14) and that the “low sustainability of the current tourism system is embedded in structures that make it easy and often cheaper to travel unsustainably … raising a wide range of questions regarding transport infrastructures, taxation, management and governance” (Cohen et al. 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] , p. 301). This conclusion illustrated the need to move beyond voluntary behavioural change in order to achieve sustainable mobility, and to explore the socio-technical landscapes in which individuals are embedded, through which public behaviour is conditioned and patterned.\nA further crucial conclusion from Freiburg 2012 was that policy-makers had shown limited interest in adopting policy measures that would achieve significant changes in sustainable transport behaviour. This lack of political initiative, wherein it was clear that politicians have far more links to industry than to science, and particularly to the social sciences, suggested that the reasons behind the inaction in transport governance needed to be urgently and critically explored. Overall, it was evident that while a comprehensive understanding of the psychologies of tourism and transport consumption is necessary to inform policy-makers, this alone would not be enough to bring the sectors onto a climatically sustainable pathway, and that radical transitions in the systems of provision and deeper understandings of political psychologies are needed (Cohen et al., 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] ; Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nThese insights formed the basis for the Freiburg 2014 workshop, which expanded its discussions to focus on public behaviour change, but also on how to change the behaviour of policy-makers, industry stakeholders and researchers themselves, to help achieve changes in tourism and transport systems for environmental reasons. Central to this endeavour was the question of how to bridge the science–policy gap: it was abundantly clear that despite the substantial and expanding body of research on tourism and climate change (Hall et al., 2015 Hall, C.M., Amelung, B., Cohen, S., Eijgelaar, E., Gössling, S., Higham, J., … Scott, D. (2015). On climate change skepticism and denial in tourism. Journal of Sustainable Tourism, 23(1), 4–25. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), and on transport and climate change (Schwanen, et al., 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ), this corpus of knowledge was having little to no effect in practice on governance.\nThe Freiburg 2014 workshop is the basis for this special issue presenting 10 papers, including this overview paper, exploring the dimensions and details of the science–policy gap in sustainable mobility. Having established the context in which the workshop was set, and before introducing the papers in this special issue, we now discuss three essential themes that are vital to understanding why, despite clear scientific evidence as to the growing environmental impacts of tourism transport, and particularly air travel, there is large-scale inertia in structural transitions and a lack of political willpower to enact meaningful policy change: (1) the importance of addressing socio-technical factors, (2) the barriers posed by placing faith in “technology myths” and (3) the need to overcome “transport taboos” in policy-making. The paper concludes by setting a research agenda that forms the basis for the forthcoming Freiburg 2016 workshop (28 June to 1 July 2016).\nSocio-technical factors\nCurrent growth trajectories indicate that transport emissions will double by 2050: the global fleet of light-duty vehicles is expected to double during that time period, and “demand for freight transport (road, rail, shipping, and air) and passenger aviation is projected to surge as well” (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] , p. 911). Air travel in particular offers an example of largely intractable public travel behaviours that are entrenched in Europe and North America and being rapidly adopted in the emerging regions of the world (Freire-Medeiros & Name, 2013 Freire-Medeiros, B., & Name, L. (2013). Flying for the very first time: Mobilities, social class and environmental concerns in a Rio de Janeiro favela. Mobilities, 8(2), 167–184. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The development of low-cost, high-volume aviation, initially in Europe and North America, and latterly in Brazil, Russia, India and China, is now powering similar air travel growth trajectories in the emerging neoliberal economies of Mexico, South Africa, Indonesia and Turkey (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ). Growth in air travel over the last two decades has been rapid (Gössling & Upham, 2009 Gössling, S., & Upham, P. (Eds.). (2009). Climate change and aviation: Issues, challenges and solutions. London: Earthscan.  [Google Scholar] ), and the current growth trajectory is projected to continue at a rate of 3.3% per annum to 2030 (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nCalls for tourism to move onto a sustainable emissions path (Becken, 2007 Becken, S. (2007). Tourists' perception of international air travel's impact on the global climate and potential climate change policies. Journal of Sustainable Tourism, 15, 351–368. [Taylor & Francis Online]   [Google Scholar] ) have been especially challenged by growing demand for air travel. This growth has been driven by significant structural changes in the transportation sector (Ryley, Davison, Bristow, & Pridmore, 2010 Ryley, T., Davison, L., Bristow, A., & Pridmore, A. (2010). Public engagement on aviation taxes in the United Kingdom. International Journal of Sustainable Transportation, 4(2), 112–128. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). Neoliberalism has been embraced by the aviation industry through airline deregulation, the deployment of frequent flyer loyalty programmes and notably by the unrestrained growth of low-cost carriers (LCCs) (Duval, 2013 Duval, D.T. (2013). Critical issues in air transport and tourism. Tourism Geographies, 15(3), 494–510. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). Network airlines offering full service and traditional routes have been drawn into intense competition with LCCs, transforming the travel market through the uptake of technology to substantially reduce labour costs (Holloway, Humphreys, & Davidson, 2009 Holloway, J.C., Humphreys, C., & Davidson, R. (2009). The business of tourism (8th ed.). Harlow: Pearson Education.  [Google Scholar] ). The LCCs have developed direct sales systems (i.e. internet booking systems), online check-in and product itemisation (e.g. priority boarding, seat allocation, baggage allowances, in-flight food and entertainment services) to de-personalise airport and air travel experiences, while increasing aircraft utilisation and flight loadings, while reducing fares (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ; Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ). The LCC business model includes operating with a single aircraft type, providing a single-class product, serving secondary airports and avoiding the costs of frequent flyer programmes (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ).\nThe consequential growth in demand for low-cost air travel is recognised as “one of the biggest revolutions in tourism and travel since the package holiday's arrival half a century earlier” (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] , p. 176). The success of the LCCs is reflected in the increasingly ordinary nature of air travel in certain sections of some societies (Randles & Mander, 2009a Randles, S., & Mander, S. (2009a). Practice(s) and ratchet(s): A sociological examination of frequent flying. In S. Gössling & P. Upham (Eds.), Climate change and aviation: Issues, challenges and solutions (pp. 245–271). London: Earthscan.  [Google Scholar] ; Urry, 2010 Urry, J. (2010). Sociology and climate change. The Sociological Review, 57(2), 84–100. [Crossref]   [Google Scholar] ). New structures of air travel provision have created flying as a highly accessible consumer product, shifting leisure travel into the domain of everyday consumer capitalism (Young, Higham, & Reis, 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). Indeed the extent to which these structures have shaped and influenced everyday consumer practices has, in some cases, reached absurd proportions. Ben Schlappig – “the man who flies around the world for free” – is “… one of the biggest stars among an elite group of obsessive flyers whose mission is to outwit the airlines” (Wofford, 2015 Wofford, B. (2015). Up in the air: Meet the man who flies around the world for free. Rolling Stone Magazine, 20 July 2014.  [Google Scholar] , p. 3). Perfecting the art of “travel hacking” – known among its members as “The Hobby” – Schlappig seeks perfection in the art of non-stop air travel and consumer luxury that is paid for by a “… gargantuan cache of frequent flier miles that grows only bigger by the day” (Wofford, 2015 Wofford, B. (2015). Up in the air: Meet the man who flies around the world for free. Rolling Stone Magazine, 20 July 2014.  [Google Scholar] , p. 5). Schlappig's claim is to be “beating the airlines at their own game”; through the gaming of frequent flyer programmes using techniques that he shares with a half million strong following through the “FlyerTalk” website.\nThe gaming of frequent flyer programmes offers an, albeit extreme, insight into consumer air travel behaviour that is anchored in and enabled by the socio-technical system. It forms part of a wider pattern of increasing affordability and uptake of air travel across an expanding range of social classes and societies (Randles & Mander, 2009b Randles, S., & Mander, S. (2009b). Aviation, consumption and the climate change debate: ‘Are you going to tell me off for flying?’ Technology Analysis & Strategic Management, 21(1), 93–113. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). It is this growth in demand for air travel that contributes significantly to driving up tourism transport emissions (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). With current and projected growth in aviation emissions has come recognition that the freedom to engage in unrestrained air travel comes with significant environmental costs (IPCC, 2013 IPCC. (2013). Climate change 2013: The physical science basis. Retrieved 28 November 2013 from IPCC web site: http://www.ipcc.ch/report/ar5/wg1/#.Uu70df0p8ds  [Google Scholar] ). While the environmental costs of air travel are now widely understood and accepted by the travelling public, the necessary responses in terms of consumer demand have not followed (Gössling, 2009 Gössling, S., & Upham, P. (Eds.). (2009). Climate change and aviation: Issues, challenges and solutions. London: Earthscan.  [Google Scholar] ; Higham, Cohen, Peeters, & Gössling, 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nIt is ironic that the increasingly aeromobile middle classes can be the same people who claim to be environmentally aware and moral consumers (Dolnicar, Juvan, Ring, & Leisch, in press Dolnicar, S., Juvan, E., Ring, A. & Leisch, F. (in press). Tourist segments' justifications for behaving in an environmentally unsustainable way. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.10248861 [Crossref] , [Web of Science ®]   [Google Scholar] ; Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). Within the context of air travel the “flyers’ dilemma” describes the tension between the self-identity of consumers who feel moral responsibility for their consumer decisions, and the high environmental costs of flying (Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ; Rosenthal, 2010 Rosenthal, E. (2010, May 24). Can we kick our addiction to flying? Retrieved 13 September 2010 from The Guardian web site: http://www.guardian.co.uk/environment/2010/may/24/kick-addiction-flying/  [Google Scholar] ). The anxieties arising from the “flyers’ dilemma” have been empirically examined in various European societies (Higham, et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ). Various studies have highlighted that air travel practices are largely unconstrained because flying is a cheap, convenient and socially desirable form of leisure consumption (Cohen & Higham, 2011 Cohen, S.A., & Higham, J.E.S. (2011). Eyes wide shut? UK consumer perceptions on aviation climate impacts and travel decisions to New Zealand. Current Issues in Tourism, 14(4), 323–335. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham & Cohen, 2011 Higham, J.E.S., & Cohen, S.A. (2011). Canary in the coalmine: Norwegian attitudes towards climate change and extreme long-haul air travel to Aotearoa/New Zealand. Tourism Management, 32(1), 98–105. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nIt emerges that a focus on the demand-side of travel, in an attempt to address issues of sustainability through behaviour change, should not ignore the fundamental socio-structural factors that underpin the tourism system (Cornelissen, 2005 Cornelissen, S. (2005). The global tourism system. Aldershot: Ashgate.  [Google Scholar] ). Young, Markham, Reis, and Higham ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) argue that the locus of responsibility is critical to debates around sustainable aviation and sustainable mobility more broadly. Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , p. 1101) observes that “mutual reinforcement between modes of governance and intervention … creates a path dependency in which solutions to sustainable tourism mobility are only identified within ‘green growth’ arguments for greater efficiency and market-based solutions.” Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) argues that as long as the focus of government tourism policies remains situated in a GDP growth paradigm, the structures of transport provision will remain unchanged and the environment required to empower a consumer-led shift to a sustainable transport emissions path will not exist.\nYoung et al. ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) suggest that appealing to individuals to reduce or otherwise moderate their tourist travel consumption practices is a flawed response. It lacks the necessary government policy response to an industry that is environmentally damaging. Appealing to consumer sacrifice ignores the fundamental socio-structural underpinnings of an unsustainable travel industry. Young et al. ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) critically consider the social, institutional and economic forces that produce excessive and unsustainable travel consumption. They highlight, first and foremost, that within the existing structures of the aviation industry, currently no alternative options are available to avoid the high environmental costs of air travel (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). No low-emission form of aviation exists to serve flyers who are concerned about climate change and aviation emissions (Peeters & Middel, 2007 Peeters, P.M., & Middel, J. (2007). Historical and future development of air transport fuel efficiency. In R. Sausen, A. Blum, D.S. Lee, & C. Brüning (Eds.), Proceedings of an International Conference on Transport, Atmosphere and Climate (TAC), Oxford, United Kingdom, 26–29 June 2006 (pp. 42–47). Oberpfaffenhofen: DLR Institut für Physic der Atmosphäre.  [Google Scholar] ), and neither is there any real prospect of major gains in aviation fuel efficiency in the short–medium term future (Peeters et al., under review) Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] .\nWhile the airline industry has made significant gains in efficiency since the advent of jet aviation (Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ), current technologies are locked in for periods of time that reach well beyond the urgent time frame required to achieve radical emission reductions (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ). Jet aviation is highly efficient in terms of time/distance/cost thresholds (Howitt, Revol, Smith, & Rodger, 2010 Howitt, O.J.A., Revol, V.G.N., Smith, I.J., & Rodger, C.J. (2010). Carbon emissions from international cruise ship passengers' travel to and from New Zealand. Energy Policy, 38, 2552–2560. [Crossref] , [Web of Science ®]   [Google Scholar] ), but those energy efficiencies have been overwhelmed in real terms by growth in demand (Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ), such that the environmental costs of air travel have become unacceptably high (Gössling, Hall, Peeters, and Scott 2010 Scott, D., Peeters, P. & Gössling, S. (2010). Can tourism deliver its aspiration greenhouse gas emission reduction targets? Journal of Sustainable Tourism, 18(3), 393–408. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The current structure of the aviation industry and the absence of further substantial technical gains in aircraft efficiency (Scott, Peeters, & Gössling, 2010 Scott, D. (2016). The Paris Climate Change Conference and the tourism industry. Journal of Sustainable Tourism, under review  [Google Scholar] ) are such that aviation emissions are expected to double within a 25–45 year time frame (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nDespite growing sustainability concerns, aviation, like the automobile (Dennis & Urry, 2009 Dennis, K., & Urry, J. (2009). After the car. Cambridge: John Wiley & Sons.  [Google Scholar] ), has become an integral part of contemporary mobility in many societies (Sheller & Urry, 2004 Sheller, M., & Urry, J. (Eds.). (2004). Tourism mobilities: Places to play, places in play. London: Routledge.  [Google Scholar] ; Urry, 2012 Urry, J. (2012). Social networks, mobile lives and social inequalities. Journal of Transport Geography, 21, 24–30. [Crossref] , [Web of Science ®]   [Google Scholar] ). Flying now out-competes other transport modes not only on convenience and time efficiency but – most critically – in terms of cost (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ). The time, cost and convenience advantages of air travel are structural factors that explain the deeply embedded nature of air travel practices (Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The aviation system allows the production of tourism (and other forms of mobility) to be accelerated in terms of fit within the capitalist working day, week and calendar year (Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ). Given the fundamentally energy-intensive nature of the tourism system (Becken, 2016 Becken, S. (2016). Peak oil: A hidden issue? Social representations of professional tourism perspectives. Journal of Sustainable Tourism, 24(1), 31–51. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), mitigating tourism transport emissions has proved a very imposing challenge (Schwanen et al., 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ). Not least, neither car nor rail nor even high-speed rail can match the cost, convenience and flexibility of response that air travel can, especially if sea crossings are involved.\nIt is also apparent that travellers, even those who are concerned about their personal leisure travel emissions, are able to disregard their environmental concerns and take advantage of cheap and convenient air travel opportunities (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ; Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ). Even for those who are deeply concerned about climate change (see Cohen, & Higham, 2011 Cohen, S.A., & Higham, J.E.S. (2011). Eyes wide shut? UK consumer perceptions on aviation climate impacts and travel decisions to New Zealand. Current Issues in Tourism, 14(4), 323–335. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham & Cohen, 2011 Higham, J.E.S., & Cohen, S.A. (2011). Canary in the coalmine: Norwegian attitudes towards climate change and extreme long-haul air travel to Aotearoa/New Zealand. Tourism Management, 32(1), 98–105. [Crossref] , [Web of Science ®]   [Google Scholar] ), the time and costs advantages of air travel undermine the competitiveness of alternative, more sustainable transport modes (Higham et al., 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The time/distance/cost dimensions of air travel have also allowed the consumption of distant tourist destinations to fit within narrow windows of time (e.g. the EasyJet generation of weekend “escape artists”). The act of flying has become integral to significant parts of the contemporary tourism transport system (Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nAs Young et al. ( 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ) observe, aviation has proven to be resistant to consumer-led change, in contrast to other aspects of consumer behaviour, such as food purchases, recycling, the use of public land transport and the conscious uptake of active transport modes, which are relatively open to modification by individuals (Barr, Shaw, Coles, & Prillwitz, 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Lassen, 2010 Lassen, C. (2010). Environmentalist in business class: An analysis of air travel and environmental attitude. Transport Reviews, 30(6), 733–751. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The intractability of air travel behaviour change occurs not only because air travel has become a desirable and affordable gateway to tourism in many societies (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ), but also because “… the environmental risks associated with air travel are global and systemic, as opposed to specific and individual, and tend not to be prioritised within a flyers' environmental consciousness” (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 60). As a result climate concerns may be temporarily suspended with impunity so that consumers can continue to consume leisure travel that involves flying (Watson, 2014 Watson, C. (Ed.). (2014). Beyond flying: Rethinking air travel in a globally connected world. Cambridge: Green Books.  [Google Scholar] ).\nThe current structures of aviation provision foster the willingness of the public to temporarily suspend their climate concerns when engaging in tourism practices (Barr et al., 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Cohen, Higham, & Reis, 2013 Cohen, S.A., Higham, J., & Reis, A. (2013). Sociological barriers to sustainable discretionary air travel behaviour. Journal of Sustainable Tourism, 21(7), 982–998. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The aviation industry facilitates a range of spatio-temporal fixes (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ) that invite the consumer to offset their aviation carbon emissions through various schemes (Barr et al., 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Gössling et al., 2007 Gössling, S., Broderick, J., Upham, P., Peeters, P., Strasdas, W., Ceron, J.-P., & Dubois, G. (2007). Voluntary carbon offsetting schemes for aviation: Efficiency and credibility. Journal of Sustainable Tourism, 15(3), 223–248. [Taylor & Francis Online]   [Google Scholar] ). Offset schemes encourage concerned travellers to assume responsibility for a profligate industry, by incurring an additional cost to mitigate the externalities of aviation consumption (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). In doing so offsetting “… actually plays into the hands of an environmentally destructive industry by allowing it to legitimate its practices while simultaneously absolving itself from responsibility for the environmental destruction from which it profits” (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 52). This absolution of individual responsibility is an important factor in the accelerating aviation emissions problem (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ). It relates closely to what Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) refers to as “structures of provision”; the social and institutional structures that underpin unsustainable consumption practices. Expecting the consumer to accept responsibility and respond individually to unsustainable contemporary travel mobilities, in the absence of meaningful industry and policy responses, has proven to be futile (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nTechnology myths\nA further vital reason why there is inertia in policy responses to growing aviation emissions is the ongoing industry-led myth, perpetuated by the media and transport policy-makers, that decarbonisation is in progress using radical technological innovation. Gotesky ( 1952 Gotesky, R. (1952). The nature of myth and society. American Anthropologist, 54(4), 523–531. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 530) describes the function of a myth as to “preserve institutions and institutional process”. A “myth” is defined as an idea, story or narrative believed by many people, including decision-makers, even though unfounded or false. As Edelman ( 1998 Edelman, M. (1998). Language, myths and rhetoric. Society, 35(2), 131–139. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 131) reminds us, “[p]olitical language can evoke a set of mythic beliefs in subtle and powerful ways.” Misleading information from the transport and tourism sectors is not new. Gössling and Peeters ( 2007 Gössling, S., Broderick, J., Upham, P., Peeters, P., Strasdas, W., Ceron, J.-P., & Dubois, G. (2007). Voluntary carbon offsetting schemes for aviation: Efficiency and credibility. Journal of Sustainable Tourism, 15(3), 223–248. [Taylor & Francis Online]   [Google Scholar] , p. 402), found four major industry discourses: “air travel is energy efficient; air travel's share of total emissions is negligible; fuel use is constantly minimised and new technology will solve the problem.” All four were deconstructed as being not representative of reality. This section explores the existence and role of “technology myths” in the discourse of sustainable aviation.\nMyths also play a role in other transport modes. Within automobility, for instance, Volkswagen created a green myth around low-emission diesel cars, even though this was largely based on cheating regulations (Franco, Sánchez, German, & Mock, 2014 Franco, V., Sánchez, F.P., German, J., & Mock, P. (2014). Real-world exhaust emissions from modern diesel cars. Berlin: International Council on Clean Transportation Europe.  [Google Scholar] ). So why concentrate on aviation within the domain of sustainable tourism? First because the tourism sector is a central part of passenger aviation, although we cannot be sure exactly how central it is: leisure travel is interlinked with business travel, visiting friends and relatives and other visit motivations. The tourism sector consequently needs to be seen as integral to air transport; the tourism industry cannot be absolved of responsibility for aviation emissions more generally.\nThe second reason is that air transport, though a relatively small part of tourism in terms of total trips (19% in 2010), represents a high share (52% in 2010) of tourism's global emissions, a share that is growing (62% in 2015), which means that aviation's emissions are increasing faster than those of accommodation, car and rail (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). There is a large body of literature showing that future emissions of aviation are growing fast and that this growth is inevitable given transport volume growth projections (Mayor & Tol, 2010 Mayor, K., & Tol, R.S.J. (2010). Scenarios of carbon dioxide emissions from aviation. Global Environmental Change, 20(1), 65–73. [Crossref] , [Web of Science ®]   [Google Scholar] ; Owen, Lee, & Lim, 2010 Owen, B., Lee, D.S., & Lim, L. (2010). Flying into the future: Aviation emissions scenarios to 2050. Environmental Science & Technology, 44(7), 2255–2260. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ; Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ; Sgouridis, Bonnefoy, & Hansman, 2010 Sgouridis, S., Bonnefoy, P.A., & Hansman, R.J. (2010). Air transportation in a carbon constrained world: Long-term dynamics of policies and strategies for mitigating the carbon footprint of commercial aviation. Transportation Research Part A: Policy and Practice, 45(10), 1077–1091. [Crossref] , [Web of Science ®]   [Google Scholar] ; Vorster, Ungerer, & Volschenk, 2012 Vorster, S., Ungerer, M., & Volschenk, J. (2012). 2050 scenarios for long-haul tourism in the evolving global climate change regime. Sustainability, 5(1), 1–51. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nFrom a recent study (Peeters et al., under review) Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] it was found that the aviation industry creates “technology myths” that may hamper political initiatives that would enforce mitigation on the aviation sector. Industry commonly wields terms such as “efficiency”, “constantly minimised” or “negligible shares” as discursive devices to perpetuate the myth that technological innovation will neutralise the problem of aviation emissions. Technology myths were identified by Peeters et al. ( under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ) for airframe design (laminar flow, composite structures and blended wing body), engines/propulsion (solar flight, electric flight and propfan) and alternative fuels (Jatropha, animal fats, hydrogen and micro-algae). For these 10 technologies media coverage in newspapers was measured over the past two decades and content analysed.\nLaminar flow and composite structures are widely applied already, with the newest types of planes, like the Boeing B787 and Airbus A350, using, for instance, composites in up to 50% of the construction by weight (Lee, 2010 Lee, J.J. (2010). Can we accelerate the improvement of energy efficiency in aircraft systems? Energy Conversion and Management, 51(1), 189–196. [Crossref] , [Web of Science ®]   [Google Scholar] ). But composite structures allow for weight savings of between 14% and 25% (Raymer et al., 2011 Raymer, D.P., Wilson, J., Perkins, H.D., Rizzi, A., Zhang, M., & Puentes, A.R. (2011). Advanced technology subsonic transport study n+3 technologies and design concepts (No. NASA/TM-2011-217130). Cleveland, OH: Glenn Research Center.  [Google Scholar] ) for the structure to which it is applied. So overall weight reduction of the Boeing B787 would be between 7% and 13%. The impact of this weight savings on fuel efficiency depends on how the designer uses the gains, but it may translate in the end to an approximate 5% fuel efficiency improvement (Peeters, 2000 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ). This Boeing 787 example clearly shows the strength of myths: the impressive 50% share of new materials is hyped in the media to a lay audience and impressed upon politicians, even though composite structures only offer small and evolutionary efficiency improvements, although coupled with improved engine design and materials, the Boeing 787 and Airbus A350 can offer fuel savings of up to 25% per seat mile over the 15–20 year old aircraft that they are replacing. The latter is an impressive figure, but it also means that fares can be reduced, encouraging more people to travel.\nBlended wing body aircraft have a long history of promises but have never emerged and are unlikely to in the near to mid-term future, or even later. Solar flight has recently attracted much attention. Basic physics tells us that it will never play a serious transport role (Noth, 2008 Noth, A. (2008). Design of solar powered airplanes for continuous flight (Unpublished PhD). ETH Zürich.  [Google Scholar] ), but the ICAO ( 2014 ICAO. (2014). 2013 environmental report. Destination green. Montreal: Author.  [Google Scholar] , p. 12) propagates discourses suggesting that it could solve environmental problems: “the Solar Impulse demonstrated that a solar-powered airplane can fly day and night without fuel.” Public interest in electric flight has followed the same strong rise since the mid-2000s. The main issue with electric flight is the requirement for high performance batteries. Current lithium batteries have a power density that falls short of the requirements for full electric flight by a factor of 100 (Kivits, Charles, & Ryan, 2010 Kivits, R., Charles, M.B., & Ryan, N. (2010). A post-carbon aviation future: Airports and the transition to a cleaner aviation sector. Futures, 42, 199–211. [Crossref] , [Web of Science ®]   [Google Scholar] ). This makes the anticipated 2035 realisation of electric flight (The Australian, 2/11/2012), extremely unlikely.\nOf the four alternative fuels, Jatropha, animal fats and hydrogen were hyped by the media between 2008 and 2011 but are now little mentioned, with significant interest only in micro-algae. Still the sector widely cites alternative fuels as promising future replacements for fossil fuels (Air Transport Action Group [ATAG], 2011 Air Transport Action Group (ATAG). (2011). Powering the future of flight. The six easy steps to growing a viable aviation biofuels industry. Geneva: Author.  [Google Scholar] ; Airbus, 2011 Airbus. (2011). Delivering the future. Global market forecast 2011–2030. Paris: Author.  [Google Scholar] ; Boeing, 2012 Boeing. (2012). Current market outlook 2012-2031. Seattle, WA. Boeing Commercial Airplanes.  [Google Scholar] ; IATA, 2012 IATA. (2012). A global approach to reducing aviation emissions. First stop: Carbon-neutral growth from 2020. Montreal: Author.  [Google Scholar] ; ICAO, 2014 ICAO. (2014). 2013 environmental report. Destination green. Montreal: Author.  [Google Scholar] ; WTTC, 2009 WTTC. (2009). Leading the challenge on climate change. London: Author.  [Google Scholar] ). Jatropha faces issues of high water use (Rosillo-Calle, Thrän, Seiffert, & Teelucksingh, 2012 Rosillo-Calle, F., Thrän, D., Seiffert, M., & Teelucksingh, S. (2012). The potential role of biofuels in commercial air transport – biojetfuel. London: IEA Bioenergy Task 40 Sustainable International Bioenergy Trade.  [Google Scholar] ) and adverse socio-economic impacts (Ariza-Montobbio & Lele, 2010 Ariza-Montobbio, P., & Lele, S. (2010). Jatropha plantations for biodiesel in Tamil Nadu, India: Viability, livelihood trade-offs, and latent conflict. Ecological Economics, 70, 189–195. [Crossref] , [Web of Science ®]   [Google Scholar] ); animal fats face technical problems preventing them from being mixed at higher than 20% shares with kerosene (Vera-Morales & Schäfer, 2009 Vera-Morales, M., & Schäfer, A. (2009). Fuel-cycle assessment of alternative aviation fuels. Cambridge: Omega.  [Google Scholar] ); hydrogen is an old but unresolved idea (Brewer, 1991 Brewer, G.D. (1991). Hydrogen aircraft technology. London: CRC Press.  [Google Scholar] ); micro-algae suffer from land-use issues, at least in the European region (Skarka, 2012 Skarka, J. (2012). Microalgae biomass potential in Europe land availability as a key issue. Technikfolgenabschätzung – Theorie und Praxis, 21, 72–79.  [Google Scholar] ), and water use, low or negative life cycle CO2 emission reductions (Quinn & Davis, 2014 Quinn, J.C., & Davis, R. (2014). The potentials and challenges of algae based biofuels: A review of the techno-economic, life cycle, and resource assessment modeling. Bioresource Technology, 184, 444–452. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ), cost and more profitable alternative land uses (Coplin, 2012 Coplin, L.G. (2012). Sustainable development of algal biofuels in the United States. Washington, DC: National Academies Press.  [Google Scholar] ).\nPolicy-makers are required to make decisions that often have long-term effects and are clouded in uncertainties. The quality of such decision-making for the transport sector is significantly degraded by technology myths created by industry and perpetuated through the media (Peeters et al., under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ). Such myths foster political inertia in the development of effective sustainable transport policy measures, discouraging potentially difficult but necessary decisions (Peeters, Gössling, & Lane, 2009 Peeters, P., Gössling, S., & Lane, B. (2009). Moving towards low-carbon tourism. New opportunities for destinations and tour operators. In S. Gössling, C.M. Hall, & D.B. Weaver (Eds.), Sustainable tourism futures. Perspectives on systems, restructuring and innovations (pp. 240–257). New York, NY: Routledge.  [Google Scholar] ).\nTransport taboos\nThe final major theme we discuss in this section of paper is the notion of “taboo” issues in tourism and transport policy-making. The question of why politicians have not acted in more significant ways on climate change mitigation in these sectors, as well as more generally, given that sound evidence of climate change was presented 25 years ago in the Intergovernmental Panel on Climate Change first assessment report (IPCC, 1990 IPCC. (1990). IPCC first assessment report 1990. Cambridge: Cambridge University Press.  [Google Scholar] ), is itself a relatively underexplored area of research. The adverse consequences of climate change for ecosystems and humans have since been confirmed and well documented (IPCC, 2014 IPCC. (2014). Climate change 2014: Synthesis report. Contribution of Working Groups 1, 2, and 3 to the fifth assessment report of the Intergovernmental Panel on Climate Change. Cambridge: Cambridge University Press.  [Google Scholar] ), and climate change is no longer considered a future phenomenon but rather a current and ongoing process, as, for instance, recognised by reinsurers (Munich Re, 2014 Munich Re. (2014). Overall picture of natural catastrophes in 2013 dominated by weather extremes in Europe and Supertyphoon Haiyan. Retrieved 8 January 2015 from http://www.munichre.com/en/media_relations/press_releases/2014/2014_01_07_press_release.aspx   [Google Scholar] ). As an outcome of the IPCC reports, political consensus has been achieved to stabilise greenhouse gas emissions at a level that will prevent global warming from exceeding 2 °C compared to pre-industrial temperatures, an objective confirmed during various Conferences of Parties (UNFCCC, 2014 UNFCCC. (2014). Various documents. Retrieved 17 July 2015 from www.unfccc.int   [Google Scholar] ) and recently recognised at the landmark Paris Agreement (Scott, 2016 Scott, D. (2016). The Paris Climate Change Conference and the tourism industry. Journal of Sustainable Tourism, under review  [Google Scholar] ).\nFor the transport sector, responsible for about 25% of global emissions, the European Commission (EC) has outlined emission reduction goals of -60% by 2050 compared to 1990, with an interim goal of -20% by 2030 compared to 2008 (EC, 2011 European Commission (EC). (2011). White paper: Roadmap to a single European transport area – towards a competitive and resource efficient transport system. COM(2011) 144 final. Brussels: Author.  [Google Scholar] ). These emission reductions are considered in line with the 2 °C guardrail. Yet, while climate policy objectives have been defined for the EU, and while these could also be defined for any country based on national greenhouse gas inventories, there is limited evidence of transport policies that would help to achieve such significant emission reductions, and, controversially, the EC has even outlined that curbing mobility is not considered a viable option (EC, 2011 European Commission (EC). (2011). White paper: Roadmap to a single European transport area – towards a competitive and resource efficient transport system. COM(2011) 144 final. Brussels: Author.  [Google Scholar] ). In countries and regions outside Europe, and specifically for international aviation as a significant sub-sector of tourism, transport-related mitigation policies have thus remained insignificant (Gössling, Scott, & Hall, 2013 Gössling, S., Scott, D., & Hall, C.M. (2013). Challenges of tourism in a low-carbon economy. Wiley Interdisciplinary Reviews: Climate Change, 4(6), 525–538. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nYet, if energy-intense forms of mobility do not decline, it is highly unlikely that absolute reductions in greenhouse gas emissions can be achieved (Anable, Brand, Tran, & Eyre, 2012 Anable, J., Brand, C., Tran, M., & Eyre, N. (2012). Modelling transport energy demand: A socio-technical approach. Energy Policy, 41, 125–138. [Crossref] , [Web of Science ®]   [Google Scholar] ; Banister, 2008 Banister, D. (2008). The sustainable mobility paradigm. Transportation Policy, 15(1), 73–80. [Crossref] , [Web of Science ®]   [Google Scholar] , 2011 Banister, D. (2011). Cities, mobility and climate change. Journal of Transport Geography, 19(6), 1538–1546. [Crossref] , [Web of Science ®]   [Google Scholar] ; Chapman, 2007 Chapman, L. (2007). Transport and climate change: A review. Journal of Transport Geography, 15(5), 354–367. [Crossref] , [Web of Science ®]   [Google Scholar] ; IEA, 2012 IEA. (2012). World energy outlook 2011. Paris: Author.  [Google Scholar] ; Schäfer et al., 2009 Schäfer, A., Heywood, J.B., Jacoby, H.D., & Waitz, I.A., (2009). Transportation in a climate-constrained world. Cambridge, MA: MIT Press.  [Google Scholar] ; UNWTO, UNEP, & WMO, 2008 UNWTO, UNEP, & WMO. (2008). Climate change and tourism: Responding to global challenges. Madrid, Paris & Geneva: UNWTO, UNEP & WMO.  [Google Scholar] ). This has resulted in a situation where there are clear policy goals, and a wide range of market-based, command-and-control and soft policy measures available to achieve these goals (e.g. Friman, Larhult, & Gärling, 2012 Friman, M., Larhult, L., & Gärling, T. (2012). An analysis of soft transport policy measures implemented in Sweden to reduce private car use. Transportation, 40(1), 109–129. [Crossref] , [Web of Science ®]   [Google Scholar] ; OECD & UNEP, 2011 OECD & UNEP. (2011). Climate change and tourism policy in OECD countries. Paris: Author.  [Google Scholar] ; Sterner, 2007 Sterner, T. (2007). Fuel taxes: An important instrument for climate policy. Energy Policy, 35, 3194–3202. [Crossref] , [Web of Science ®]   [Google Scholar] ), but a dearth of implementation, with evidence that only soft policies focusing on voluntary behavioural change appear to be considered politically viable to reduce emissions from transportation. This paradox has been described as an “implementation gap” (Banister & Hickman, 2013 Banister, D., & Hickman, R. (2013). Transport futures: Thinking the unthinkable. Transportation Policy, 29, 283–293. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 292), and led to growing academic interest in barriers to significant climate policy.\nVarious explanations have been provided to explain why governments have been reluctant to implement policies. From a governance viewpoint, Rietveld et al. ( 2005 Rietveld, P., & Stough, R. (Eds.). 2005. Barriers to sustainable transport: Institutions, regulation and sustainability. Abingdon: Spon Press.  [Google Scholar] ) have suggested that institutions rule and structure public and private actions, and that these can be informal, formal, governance-, and resource allocation/employment related. Informal institutions would comprise values, norms, practices, habits and traditions, and are considered conditioners of behaviour (see Schwanen & Lucas, 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] in the context of automobility). Formal institutions include “codified statutes, constitutions, provisions, laws, regulations, and high level administrative orders” (Rietveld et al., 2005 Rietveld, P., & Stough, R. (Eds.). 2005. Barriers to sustainable transport: Institutions, regulation and sustainability. Abingdon: Spon Press.  [Google Scholar] , p. 3). Governance institutions are a third type of institution focused on rules, including laws, regulations and policy directives, such as planning and zoning issues, or transactions involving actors and agents. Finally, resource allocation refers to government agencies, firms and non-profit associations allocating financial resources. These four categories can be used to identify and address barriers from various viewpoints, including, for instance, the notion that transport planning cannot be questioned, as transportation is important for society and economic growth. This has, for instance, been discussed by Miciukiewicz and Vigar ( 2012 Miciukiewicz, K., & Vigar, G. (2012). Mobility and social cohesion in the splintered city: Challenging technocentric transport research and policy-making practices. Urban Studies, 49(9), 1941–1957. [Crossref] , [Web of Science ®]   [Google Scholar] ) in terms of technological fixation among transport researchers and subsequent technocentric policy-making. In a similar vein, Hutton ( 2013 Hutton, B. (2013). Planning sustainable transport. London: Routledge.  [Google Scholar] ) describes how the turn from meeting predicted transport growth to managing transport demand has only recently been considered in UK transport policy. Ultimately, “barriers” thus often resemble embedded beliefs of ecological modernisation, i.e. the assumption that transport growth can be balanced environmentally, based on technological progress, as also evident in UNEP's ( 2011 UNEP. (2011). Towards a green economy: Pathways to sustainable development and poverty eradication. Retrieved 29 September 2015 from www.unep.org/greeneconomy   [Google Scholar] ) Green Growth focus, which may be seen as another ecological modernisation paradigm without real-world implications for emission growth (Hall, 2009 Hall, C.M. (2009). Degrowing tourism: Décroissance, sustainable consumption and steady-state tourism. Anatolia: An International Journal of Tourism and Hospitality Research, 20(1), 46–61. [Taylor & Francis Online]   [Google Scholar] , 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , 2015 Hall, C.M. (2015). Economic greenwash: On the absurdity of tourism and green growth. In M.V. Reddy & K. Wilkes (Eds.), Tourism in the green economy. (pp. 339–358). London: Earthscan by Routledge.  [Google Scholar] ).\nNew understandings of the reasons for inaction on climate change in transport contexts are thus required: “barriers” were discussed with regard to their cognitive and affective dimensions during the Freiburg 2014 workshop, where they were framed as “taboos” (Gössling & Cohen, 2014 Gössling, S., & Cohen, S. (2014). Why sustainable transport policies will fail: European Union climate policy in the light of transport taboos. Journal of Transport Geography, 39, 197–207. [Crossref] , [Web of Science ®]   [Google Scholar] ). Taboos are different from barriers of implementation, because they cannot be addressed politically without considerable (political) danger to the person taking up a given issue. To touch a taboo implies to violate an existing norm, i.e. a situation that is usually in the interest of powerful individuals, (lobby) organisations, or the broader public or community. A “transport taboo” thus describes an issue that cannot be raised without risks, possibly jeopardising the political future of any person raising the taboo. A wide range of transport taboos that politicians are unwilling to touch has been identified, such as the watering down of transport policy by lobbyism, the skewed share of transport emissions contributed by higher income classes and the broader societal glamorisation of high-energy transport consumption (Cohen & Gössling, 2015 Cohen, S.A., & Gössling, S. (2015). A darker side of hypermobility. Environment and Planning A, 47, 1661–1679. [Crossref] , [Web of Science ®]   [Google Scholar] ; for further details on transport taboos see Gössling & Cohen, 2014 Gössling, S., & Cohen, S. (2014). Why sustainable transport policies will fail: European Union climate policy in the light of transport taboos. Journal of Transport Geography, 39, 197–207. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nTransport taboos are consequently issues that would appear obvious, as some solutions are ready at hand, demanding political action; yet, they are characterised by silence. A further example may be the 20 years of OECD reports recommending to remove fossil fuel subsidies, and to introduce carbon pricing ( 1991 OECD. (1991). Energy prices, taxes and carbon dioxide emissions (OECD economic studies 17). Paris: Author.  [Google Scholar] , 1999 OECD. (1999). Project on environmentally sustainable transport (EST) (Report NV/EPOC/PPC/T(99)3/FINAL/REV1). Paris: Author.  [Google Scholar] , 2008, 2015 OECD. (2015). OECD companion to the inventory of support measures for fossil fuels 2015. Retrieved 12 November 2015 from http://www.keepeek.com/Digital-Asset-Management/oecd/energy/oecd-companion-to-the-inventory-of-support-measures-for-fossil-fuels-2015_9789264239616-en#page1   [Google Scholar] ). However, the issue remains politically untouched, because this would lead to outrage by industry associations, who are powerful agents in public discourse. Yet, overcoming taboos is essential if more sustainable tourism and transport policies are to be implemented, specifically regarding climate change. At the very least, this would require political parties to stop using public sentiment to undermine sustainable transport policy initiatives by political opponents in order to gain votes on less popular measures related to climate change mitigation.\nThe papers in this special issue\nThis special issue presents nine further papers that explore avenues for behaviour change by various stakeholders in tourism and transport in order to bridge the science–policy gap in sustainable mobility. The contributions cover a wide spectrum of interests and stakeholders, and connect in various ways to the themes discussed above, notably socio-technical factors and transport taboos.\nThe first three papers investigate the role of researchers in the sustainable mobility debate, and their capacities and shortcomings to contribute to behaviour change. Hall ( in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) does this by employing social marketing themes. He questions both the theoretical knowledge base of sustainable tourism and the positioning of sustainability within the wider (tourism) literature, noting that, despite a growing body of research on sustainable tourism and mobilities, concern about these topics in tourism research is still minor, particularly in popular areas such as achieving growth in visitor numbers and expenditure. Substantive change is not evident in most destinations, or among organisations that have adopted sustainable tourism. As a way forward, Hall discusses the need for more advocacy- and participatory-based approaches so that scientists can better communicate with policy-makers and work collaboratively/co-productively with them and other upstream stakeholders. Downstream and (more activist and interventionist) upstream social marketing to the public, taking in the lessons learned from other disciplines and debates (such as that on anti-smoking), may (re)glamorise/make fashionable forms of more sustainable tourism or encourage more conventional but low transport intensity local tourism.\nMelissen and Koens ( in press Melissen, F., & Koens, K. (in press). Adding researchers' behaviour to the research agenda: Bridging the science–policy gap in sustainable tourism mobility. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1071384 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) echo Hall, arguing that researchers should not only focus on understanding the structures behind tourist behaviour, but also on how to mobilise policy and business stakeholders to contribute to the sustainable development of tourism. They find several factors or tensions (Jones, Jones, & Walsh, 2008 Jones, N., Jones, H., & Walsh, C. (2008). Political science? Strengthening science-policy dialogue in developing countries. London: Overseas Development Institute.  [Google Scholar] ) hindering researcher behaviour change towards bridging the science–policy gap in sustainable tourism mobility, and make a case for adding researchers' behaviour to the corresponding research agenda. Researchers may need to position themselves closer to the policy arena, without politicising science or moving from engaged to activist research.\nMounting sustainable tourism and transport advocacy will also lead to more attention to the environmental sustainability imperatives of researchers themselves, and the institutions for which they work. Thus the paper by Hopkins, Higham, Tapp, and Duncan ( in press Hopkins, D., Higham, J., Tapp, S., & Duncan, T. (in press). Academic mobility in the Anthropocene era: A comparative study of university policy at three New Zealand institutions. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1071383 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) in this issue links to the transport taboo of questioning academic mobility and associated carbon emissions. Set in a New Zealand university context it investigates the perceived lock-in of academic performance being dependent on, or equal to, international mobility. This can be linked back to the pioneering work of the late Karl Georg Høyer (Høyer, 2000 Høyer, K.G. (2000). Sustainable tourism – or sustainable mobility? Journal of Sustainable Tourism, 8(2), 147–161. [Taylor & Francis Online]   [Google Scholar] ; Høyer & Næss, 2001 Høyer, K.G., & Næss, P. (2001). Conference tourism: A problem for the environment as well as for research? Journal of Sustainable Tourism, 9(6), 451–470. [Taylor & Francis Online]   [Google Scholar] ). The authors find academic travel to be embedded in university policy, for example through international partnerships, the need to present research at international conferences and recruitment processes. Acknowledging New Zealand's particular geographical location, they still recommend that academic institutions consider and address the carbon emissions related to academic mobility, and to integrate sustainability more systematically into university (travel) policy.\nAnother highly mobile group possibly caught in a (socio-cultural) lock-in of flight-dependent practices is that of the younger generation in many western countries taking a gap year. Luzecka ( in press Luzecka, P. (in press). “Take a gap year!” A social practice perspective on air travel and potential transitions towards sustainable tourism mobility. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1115513 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) explores ways in which conventions related to “appropriate” gap year destinations are developed, sustained and reproduced. Numerous social mechanisms were found to facilitate overseas travel in the context of gap years, including a shared perception of difference and physical distance, which links personal development to the challenges of distant countries. This paper highlights the fact that some long-haul destinations are actually easier or cheaper to travel to than more nearby destinations. Luzecka warns that widening participation in gap year travel may further the normalisation – and psychological lock-in – of long-haul travel. The length of gap years would make them suitable for slow, and potentially more sustainable, travel, but for such a sustainable mobility transition, the socio-cultural forces that shape current gap year practices need to be taken into account.\nThe two following papers acknowledge the reality of consumers not accepting responsibility and responding to unsustainable travel mobilities, and seek ways to influence consumer decision-making through the provision of carbon information with travel products. Araña and León ( in press Araña, J.E., & León, C.J. (in press). Are tourists animal spirits? Evidence from a field experiment exploring the use of non-market based interventions advocating sustainable tourism. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1101128 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) experiment with the role of emotions and time pressure in decision-making, hypothesising that many traveller decisions are of an experiential nature. In a Spanish social tourism programme setting, they carried out a field experiment in which they manipulate the choice context of travellers, when deciding where to travel. Travel plans involved different levels of CO2 emissions, contexts about the emotional state, and decision time. Araña and León find that emotional states and the decision context can indeed affect the sustainability of travel choices. These findings have several implications for pro-environmental behaviour policies and campaigns in tourism and the effectiveness of tax incentives. Subjects showing more empathy with future generations are more likely to accept low-carbon travel options.\nEijgelaar, Nawijn, Barten, Okuhn, and Dijkstra ( in press Eijgelaar, E., Nawijn, J., Barten, C., Okuhn, L., & Dijkstra, L. (in press). Consumer attitudes and preferences on holiday carbon footprint information in the Netherlands. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1101129 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) investigate carbon labels, as a soft measure towards more sustainable travel choices that is more likely to receive industry acceptance than direct volume-reducing measures. Their case is linked to the desire of Dutch tour operators to offer such a label. Hence, they explore preferences for carbon label design in tourism. The authors tested label designs in a number of consecutive research phases under Dutch consumers. They find a number of preconditions for a tourism carbon label: it should be simple in design and connect to existing well-known EU labels for energy efficiency. But at the same time they note that sustainability is still of low priority during holiday decision-making.\nAn interesting case in science–policy interaction is Antarctica, as the continent is not controlled by a state, but through a 29-party governance regime, the Antarctic Treaty System. As little progress was made on tourism issues through the Treaty's meetings, the International Association of Antarctica Tour Operators filled this gap by self-regulating Antarctic tourism. However, their capability to self-regulate is increasingly questioned. Student, Lamers, and Amelung ( in press Student, J., Lamers, M., & Amelung, B. (in press). Towards a tipping point? Exploring the capacity to self-regulate Antarctic tourism using agent-based modelling. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1107079 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) apply agent-based modelling (ABM) to identify the challenges for the self-regulation of this carbon-intensive form of tourism. They find a number of potentially destabilising factors connected to likely future tourism development, whereby optimum group size and membership cost are crucial. More importantly, they stress the strength of ABM as a method to safely experiment with uncertainties in Antarctica, and help to provide insights for an environmentally optimised application in (sustainable Antarctic tourism) policy.\nThe final two papers investigate sustainable transport policy-making and practices at destinations. This local/regional level work reveals key aspects of the realpolitik of policy-making in ways not open to most researchers at national or international levels. Scuttari, Volgger, and Pechlaner ( in press Scuttari, A., Volgger,M., & Pechlaner, H. (in press). Transition management towards sustainable mobility in alpine destinations: Realities and realpolitik in Italy's South Tyrol region. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.XXXXXX [Crossref] , [Web of Science ®]   [Google Scholar] ) seek ways for governing the transition towards more sustainable tourism mobility by applying a system approach. Their research in a South Tyrolean context indicates that the transition towards more sustainable transport solutions is complex and requires both public and private sector unpredictability and risk aversion to be taken into account. Scuttari et al. identify three conditions relevant to performing the transition, each one linked to a different sub-system (socio-ecological, socio-technical and governance). The transition to sustainable solutions is a complex task that can only be successful when all sub-systems – tourism, transport, governance and social-ecological – interact in informal or formal partnership. To be successful, three conditions were identified: (1) an improved understanding of what sustainable transport entails, (2) adoption of the best available technology and (3) courage and leadership to take risks with new solutions without knowing what they will bring with absolute certainty. The latter means that science should provide a better understanding of how to mitigate risk aversion or even better, how to shift the focus of risk aversion towards avoiding the risks of unsustainable development and climate change, as opposed to the risks associated with introducing, managing and using novel transport systems.\nFinally, Stanford and Guiver (in press) Stanford, D., & Guiver, J. (in press). Driving pro-environmental change in tourist destinations: Encouraging sustainable travel in national parks via partnership project creation and implementation. Journal of Sustainable Tourism, DOI: 10.1080.09669582.2015.1122018 [Web of Science ®]   [Google Scholar] explore public–private partnership led projects providing alternatives to car travel in three UK national parks as mechanisms of modal shift and pro-environmental change. They identify a number of success factors and provide practical advice on understanding and guiding future multi-partnership pro-environmental change processes in complex networks. As in the previous paper, strong local governance structures, awareness creating, trust and learning are important, and the effective communication of benefits to stakeholders appeared most significant.\nConclusions and research agenda\nThis paper has discussed three areas crucial for understanding why, despite clear scientific evidence about the growing environmental impacts of tourism transport, there is large-scale inertia in structural transitions and a lack of political willpower to enact meaningful policy change. These included the importance of addressing socio-technical factors, the barriers posed by placing faith in “technology myths” and the need to overcome “transport taboos” in policy-making. These areas shed significant light on why a science–policy gap in sustainable mobility exists, and on the issues that must be overcome if this gap is to be bridged. The societal challenge of transitioning the tourism and transport sectors to a sustainable emissions path must not be devolved entirely to the public and the marketplace, as suggested by neoliberal values. It is vital that both governments and the tourism and transport industries take a more cautious approach to the technological optimism that is fostering policy inertia: technological innovation alone will not save the day anytime soon. They must invest more in research, in operational procedures and in encouraging the development of and marketing alternatives to air travel. But most importantly, policy-makers must take a more open approach to implementing sustainable transport policies that affect the structures of provision; they will need to be lobbied to touch issues that may be politically risky, but nonetheless have been shown in other contexts to be successful in fostering desirable sustainable transport outcomes.\nBuilding on these insights, the Freiburg 2016 workshop will consequently focus on desirable transport futures, i.e. visions of desirable sustainable transport systems that have the potential to be actively taken up by wide cross sections of society (c.f. Banister & Hickman, 2013 Banister, D., & Hickman, R. (2013). Transport futures: Thinking the unthinkable. Transportation Policy, 29, 283–293. [Crossref] , [Web of Science ®]   [Google Scholar] ). A starting point for this is the analysis of sustainable transport transitions that are now underway, and the analysis of the structural, political, institutional and social/psychological factors underlying those transitions. The e-bike revolution is one example of societal change involving a low-carbon technological innovation, with uptake and adoption motivated by convenience, speed, health and cost. Many cities in Europe have re-discovered the bicycle as a transport mode with diverse benefits, and there now exists widespread and growing demand for infrastructures that facilitate cycling and other active forms of transport (Gössling & Choi, 2015 Gössling, S., & Choi, A.S. (2015). Transport transitions in Copenhagen: Comparing the cost of cars and bicycles. Ecological Economics, 113, 106–113. [Crossref] , [Web of Science ®]   [Google Scholar] ; Pucher & Buehler, 2012 Pucher, J., & Buehler, R. (2012). City cycling. Cambridge, MA: MIT Press.  [Google Scholar] ). Cycling cities have become a desirable transport future but the health and cost benefits of cycling demand that issues of cyclist safety are addressed in infrastructural provision. The rise of car sharing systems, in place of car ownership, is another timely example of a mobility transition, but is not without rebound effects, as the media suggests that it has cannibalisation effects at the expense of public transport (Schwarz, 2015 Schwarz, K. (2015, December 11). Carsharing: Wie umweltschädlich sind Car2go und Drivenow wirklich? [Carsharing: How environmentally harmful are Car2go and Drivenow actually?]. The Huffington Post (German Edition). Retrieved 1 December 2015 from http://www.huffingtonpost.de/2015/02/13/carsharing-klima-umfrage-wirtschaftswoche_n_6675908.html   [Google Scholar] ).\nIn contrast to desirable futures, while flying is highly desirable for many, continued growth in air travel on a global scale is incompatible with a sustainable transport future, and it is here that the need for urgent transition is now widely accepted. However, before entertaining alternatives to the current unsustainable transport system, it is essential to know what desirable transport futures may look like. The critical analysis of mobility transitions, including barriers confronting the achievement of desirable transport futures, is needed. A concerted research effort in the following key areas is consequently required:\nDesired transport\nFuture visions must be built upon desired transport systems. Critical examination is needed of spontaneous market uptake of desired new transport systems and their rebound effects. How have e-bike, car sharing, high-speed rail and successful public transport systems emerged in certain societies? What factors have acted as facilitators of change and how were barriers overcome? How important are individuals, i.e. specific people, in initiating change? What were the key roles of stakeholders and were low-carbon transitions an objective from the outset or a coincidental outcome? What lessons can be learned from these revolutions and what wider roles may such successes play in developing towards low-carbon transport and tourism?\nThe role of fashion\nMany trends in tourism and travel are fashion driven. Certain destinations can rise and fall substantially in a very short time. In the Netherlands long-haul travel grew rapidly until 2008 when it stabilised and started to decline. What factors influence significant changes in established patterns of consumption? Why has “environmental consciousness”, with the exception of European car purchasing, proved largely ineffective in driving low-carbon transport transitions? What potential do social marketing, celebrity endorsement and role model advocacy offer, and how can the effectiveness of these strategies be maximised? What other strategies may exist to influence and encourage the fashionability of more sustainable forms of tourism (e.g. caravanning, train journeys, “loca-tourism”, slow tourism and staycationing)? How can industry, government, the media and public organisations engage in such processes, and what in particular is the role of science and researchers?\nEconomic issues\nThe economic arguments for growth in aviation are well established, even though many assessments appear to remain partial. But what are the economic arguments that support the development of sustainable transport systems? What economic growth scenarios may be associated with low-carbon transitions to rail and electric vehicle fleets, and the new infrastructures required to facilitate active transport modes? What do economic models predict for the redistribution of travel flows under low-carbon transport scenarios? Contributions to a more complete understanding of the economics of low-carbon transport scenarios are critically important to the shift towards desired transport futures.\nPublic health and well-being\nCritical issues arise when contemplating how the sustainable transportation agenda is coupled with questions of public health and well-being. Policy outcomes driven by a public health and well-being agenda may have the potential for achieving significant environmental benefits. What potential do desirable transport futures offer to overcome personal well-being and public health risks, such as the negative health dimensions of frequent flying (Cohen & Gössling, 2015) Cohen, S.A., & Gössling, S. (2015). A darker side of hypermobility. Environment and Planning A, 47, 1661–1679. [Crossref] , [Web of Science ®]   [Google Scholar] , or the risk of pandemic associated with long-haul flights? And where mobility transitions are already in progress, can, for instance, the rising public health costs of serious injuries and deaths of cyclists be mitigated by dedicated cycle infrastructure?\nIssues of equity and ethics\nThere is a need to move beyond the Eurocentrism (c.f. Cohen & Cohen, 2015 Cohen, E., & Cohen, S.A. (2015). Beyond Eurocentrism in tourism: A paradigm shift to mobilities. Tourism Recreation Research, 40(2), 157–168. [Taylor & Francis Online]   [Google Scholar] ) that has framed debate around these issues. While the West has contributed disproportionately to the environmental crisis, emissions of unsustainable transportation are globally dispersed. Insights that are theoretically, methodologically and practically informed are required to understand sustainable transportation issues as they apply to emerging world regions (e.g. Dillimono & Dickinson, 2015 Dillimono, H.D., & Dickinson, J.E. (2015). Travel, tourism, climate change, and behavioral change: Travelers' perspectives from a developing country, Nigeria. Journal of Sustainable Tourism, 23(3), 437–454. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] on Nigerian perspectives). Growth from the middle classes in parts of Asia, Latin America, South America and Africa is also driving up global transport emissions. What equity issues arise in association with the transition from old to new transport systems? What ethical issues arise with the growth of unsustainable transportation in less developed countries? How will the continuing but declining emission footprint be distributed between countries and sectors globally? What will be the impacts of emission trading, taxation regimes, subsidies and infrastructure planning, and how will they vary between regions in light of the recent Paris Agreement?\nAdvocacy- and participatory-based approaches\nThe need exists for advocacy- and participatory-based approaches (see Bramwell, Higham, Lane, & Miller, 2016 Bramwell, B., Higham, J., Lane, B., & Miller, G. (2016). Advocacy or neutrality? Disseminating research findings and driving change toward sustainable tourism in a fast changing world. Journal of Sustainable Tourism, 24(1), 1–7. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hall, in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) to enable more effective communication with policy communities, and to facilitate collaboration and co-production with policy-makers and other upstream stakeholders. What opportunities exist to implement new or underutilised methods such as simulation, serious gaming and in-depth multi-stakeholder approaches? What potential do new approaches offer to communicate the science of climate change, and internalise the challenges inherent in responding to climate change? How can new approaches serve to highlight the contribution of travel in global GHG emissions, the inherent difficulties in responding to unsustainable transportation, and potential pathways to desirable transport futures? These key areas form the basis for a continued research agenda aimed at transitioning the tourism and transport sectors to a sustainable emissions path.\nAcknowledgments\nThe convenors of the Freiburg 2014 workshop (1–4 July 2014) gratefully acknowledge the members of its Scientific Advisory Board, all of whom contributed to the success of the workshop: Dr Stewart Barr (University of Exeter, UK), Dr Jo Guiver (University of Central Lancashire, UK), Professor Michael Hall (University of Canterbury, NZ), Dr Julia Hibbert (Bournemouth University, UK), Professor Daniel Scott (University of Waterloo, Canada) and Professor David Banister (TSI, University of Oxford, UK).\nDisclosure statement\nScott A. Cohen is a reader at the University of Surrey (United Kingdom).\nJames Higham\nJames Higham is a professor at the Department of Tourism, University of Otago (New Zealand).\nStefan Gössling\nStefan Gössling is a professor at Lund University and Linnaeus University (Sweden). He is also research coordinator at the Western Norway Research Institute's Research Centre for Sustainable Tourism.\nPaul Peeters\nPaul Peeters is an associate professor at NHTV Breda University of Applied Sciences (The Netherlands).\nEke Eijgelaar\nEke Eijgelaar is a researcher at the Centre for Sustainable Tourism and Transport at NHTV Breda University for Applied Sciences (The Netherlands).\nArticle Metrics\n""","0.28661898","""http://www.tandfonline.com/doi/full/10.1080/09669582.2015.1136637""","[-0.589514,51.242722]"
"""Imperial_College_London""","""Rapid Blood-Pressure Lowering in Patients with Acute Intracerebral Hemorrhage | NEJM""","""Rapid Blood-Pressure Lowering in Patients with Acute Intracerebral Hemorrhage\nCraig S. Anderson, M.D., Ph.D.,\nEmma Heeley, Ph.D.,\nR. John Simes, M.D., Ph.D.,\nStephen M. Davis, M.D.,\nand John Chalmers, M.D., Ph.D.\net al.,\nfor the INTERACT2 Investigators *\nInvestigators in the second Intensive Blood Pressure Reduction in Acute Cerebral Hemorrhage Trial (INTERACT2) are listed in the Supplementary Appendix , available at NEJM.org.\nAbstract\nBackground\nWhether rapid lowering of elevated blood pressure would improve the outcome in patients with intracerebral hemorrhage is not known.\nMethods\nWe randomly assigned 2839 patients who had had a spontaneous intracerebral hemorrhage within the previous 6 hours and who had elevated systolic blood pressure to receive intensive treatment to lower their blood pressure (with a target systolic level of <140 mm Hg within 1 hour) or guideline-recommended treatment (with a target systolic level of <180 mm Hg) with the use of agents of the physician's choosing. The primary outcome was death or major disability, which was defined as a score of 3 to 6 on the modified Rankin scale (in which a score of 0 indicates no symptoms, a score of 5 indicates severe disability, and a score of 6 indicates death) at 90 days. A prespecified ordinal analysis of the modified Rankin score was also performed. The rate of serious adverse events was compared between the two groups.\nResults\nAmong the 2794 participants for whom the primary outcome could be determined, 719 of 1382 participants (52.0%) receiving intensive treatment, as compared with 785 of 1412 (55.6%) receiving guideline-recommended treatment, had a primary outcome event (odds ratio with intensive treatment, 0.87; 95% confidence interval [CI], 0.75 to 1.01; P=0.06). The ordinal analysis showed significantly lower modified Rankin scores with intensive treatment (odds ratio for greater disability, 0.87; 95% CI, 0.77 to 1.00; P=0.04). Mortality was 11.9% in the group receiving intensive treatment and 12.0% in the group receiving guideline-recommended treatment. Nonfatal serious adverse events occurred in 23.3% and 23.6% of the patients in the two groups, respectively.\nConclusions\nIn patients with intracerebral hemorrhage, intensive lowering of blood pressure did not result in a significant reduction in the rate of the primary outcome of death or severe disability. An ordinal analysis of modified Rankin scores indicated improved functional outcomes with intensive lowering of blood pressure. (Funded by the National Health and Medical Research Council of Australia; INTERACT2 ClinicalTrials.gov number, NCT00716079 .)\nIntroduction\nAcute intracerebral hemorrhage, which is the least treatable form of stroke, affects more than 1 million people worldwide annually, 1,2 with the outcome determined by the volume and growth of the underlying hematoma. 3-5 Blood pressure often becomes elevated after intracerebral hemorrhage, 6 frequently reaching very high levels, and is a predictor of outcome. 7-11 On the basis of the results of the pilot-phase study, Intensive Blood Pressure Reduction in Acute Cerebral Hemorrhage Trial 1 (INTERACT1), 12-14 we conducted the main-phase study, INTERACT2, 15 to determine the safety and effectiveness of early intensive lowering of blood pressure in patients with intracerebral hemorrhage.\nMethods\nTrial Design\nINTERACT2 was an international, multicenter, prospective, randomized, open-treatment, blinded end-point trial. Details of the design have been published previously 15,16 and are summarized in the Supplementary Appendix , available with the full text of this article at NEJM.org. In brief, we compared the effect of a management strategy targeting a lower systolic blood pressure within 1 hour with the current guideline-recommended strategy, which targets a higher systolic blood pressure, in patients who had a systolic blood pressure between 150 and 220 mm Hg and who did not have a definite indication for or contraindication to blood-pressure–lowering treatment that could be commenced within 6 hours after the onset of spontaneous intracranial hemorrhage; the diagnosis of intracranial hemorrhage was confirmed by means of computed tomography (CT) or magnetic resonance imaging (MRI). Patients were excluded if there was a structural cerebral cause for the intracerebral hemorrhage, if they were in a deep coma (defined as a score of 3 to 5 on the Glasgow Coma Scale [GCS], 17 in which scores range from 3 to 15, with lower scores indicating reduced levels of consciousness), if they had a massive hematoma with a poor prognosis, or if early surgery to evacuate the hematoma was planned. Written informed consent was obtained from each patient or legal surrogate (before randomization or as soon as possible afterward) in accordance with national regulations.\nInvestigators entered baseline data into a database associated with a secure Web-based randomization system. The data were checked to confirm the eligibility of the patient, and several key clinical variables were recorded before the system assigned a participant to intensive or guideline-recommended management of blood pressure with the use of a minimization algorithm to ensure that the groups were balanced with respect to country, hospital, and time (≤4 hours vs. >4 hours) since the onset of the intracerebral hemorrhage. In participants who were assigned to receive intensive treatment to lower their blood pressure (intensive-treatment group), intravenous treatment and therapy with oral agents were to be initiated according to prespecified treatment protocols that were based on the local availability of agents, with the goal of achieving a systolic blood-pressure level of less than 140 mm Hg within 1 hour after randomization and of maintaining this level for the next 7 days. In participants who were assigned to receive guideline-recommended treatment (standard-treatment group), blood-pressure–lowering treatment was to be administered if their systolic blood pressure was higher than 180 mm Hg; no lower level was stipulated. 18-20 All participants were to receive oral antihypertensive agents (or topical nitrates) within 7 days (or at discharge from the hospital if that occurred before 7 days), even if the agents had to be administered through a nasogastric tube; combination treatment with an angiotensin-converting–enzyme inhibitor and a diuretic was recommended if that treatment was not contraindicated and if no different drugs were specifically required, with the goal of achieving a systolic blood pressure of less than 140 mm Hg during follow-up for the prevention of recurrent stroke.\nAssessments\nDemographic and clinical characteristics were recorded at the time of enrollment. The severity of the stroke was assessed with the use of the GCS 17 and the National Institutes of Health Stroke Scale 21 (NIHSS, on which scores range from 0 to 42, with higher scores indicating a more severe neurologic deficit) at baseline, at 24 hours, and at 7 days (or at the time of discharge, if that occurred before 7 days). Brain CT (or MRI) was performed according to standard techniques at baseline (to confirm the diagnosis) in all patients, and at 24±3 hours in a subgroup of patients who were being treated at sites at which repeat scanning was either part of routine practice or approved for research. Participants were followed up in person or by telephone at 28 days and at 90 days by trained local staff who were unaware of the group assignments. Participants who did not receive the assigned treatment or who did not adhere to the protocol were followed up in full, and their data were included in the analyses according to the intention-to-treat principle.\nOutcome Measures\nThe primary outcome measure was the proportion of participants with a poor outcome, defined as death or major disability. Major disability was defined as a score of 3 to 5 on the modified Rankin scale at 90 days after randomization. Scores on the modified Rankin scale range from 0 to 6, with a score of 0 indicating no symptoms; a score of 5 indicating severe disability, confinement to bed, or incontinence; and a score of 6 indicating death. The protocol specified “death or severe disability in patients treated within 4 hours of onset of intracranial hemorrhage” as the key secondary outcome. 15 However, during the course of the trial, ordinal approaches to the analysis of the modified Rankin scores gained acceptance in stroke trials. Therefore, in the final statistical analysis plan, 16 which was written before the initiation of data analysis, the key secondary outcome was redefined as physical function across all seven levels of the modified Rankin scale, as determined with the use of an ordinal analysis. 22\nOther secondary outcomes were all-cause mortality and cause-specific mortality (classified at a central location, according to the definitions provided in the Supplementary Appendix , by independent adjudication experts who reviewed submitted medical documents); five dimensions of health-related quality of life (mobility, self-care, usual activities, pain or discomfort, and anxiety or depression), as assessed with the use of the European Quality of Life–5 Dimensions (EQ-5D) questionnaire, 23 with each dimension graded according to one of three levels of severity (no problems, moderate problems, or extreme problems); the duration of the initial hospitalization; residence in a residential care facility at 90 days; poor outcomes at 7 days and at 28 days; and serious adverse events. The health statuses from each subscale of the EQ-5D were transformed into a single utility value as a fraction of 1 (with 0 representing death and 1 representing perfect health), with the use of population-based preference weights for the United Kingdom. 24\nThe safety outcomes of primary interest were early neurologic deterioration (defined as an increase from baseline to 24 hours of 4 or more points on the NIHSS or a decrease of 2 or more points on the GCS) and episodes of severe hypotension with clinical consequences that required corrective therapy with intravenous fluids or vasopressor agents. The difference in the volume of the hematoma from baseline to 24 hours was assessed in a prespecified subgroup of participants who underwent repeat brain imaging.\nStudy Oversight\nThe study was conceived and designed by the executive committee (see the Supplementary Appendix ), whose members, along with selected principal investigators from various countries, developed the protocol (which is available at NEJM.org) and conducted the study. The study was approved by the ethics committee at each participating site. The corresponding author wrote the first draft of the manuscript, and other authors provided input. All the authors made the decision to submit the manuscript for publication. Experienced research staff monitored the study for quality and for the integrity of the accumulation of clinical data according to the study protocol. Monitoring for serious adverse events was performed routinely, and any events that occurred were confirmed according to regulatory and Good Clinical Practice requirements, as outlined in the Supplementary Appendix . There was no commercial support for the study. Study data were collected, monitored, and analyzed by the INTERACT2 Project Office and by statisticians at the George Institute for Global Health, who vouch for the accuracy and completeness of the data and the fidelity of the study to the protocol.\nStatistical Analysis\nWe estimated that with a sample of 2800 participants, the study would have at least 90% power to detect a 14% relative reduction (a difference of 7 percentage points) in the primary outcome, from 50% in the standard-treatment group to 43% in the intensive-treatment group, assuming a between-group difference in systolic blood pressure of 13 mm Hg, a rate of nonadherence to treatment of 10%, and an overall loss to follow-up of 3%, with a type I error rate of 5% and with the use of a two-sided significance test. The data were analyzed with the use of SAS software, version 9.2, according to the intention-to-treat principle. 16\nThe primary analysis of the effect of treatment on the primary outcome was unadjusted and is reported as an odds ratio with associated 95% confidence intervals. We tested for significance using a standard chi-square test of proportions (with a two-sided alpha level of 5%). The scores on the modified Rankin scale were also analyzed with the use of an unadjusted proportional-odds regression model across all levels of the scale, after we checked that the assumption of a common proportional odds was not violated. 25 For sensitivity purposes, the primary outcome was analyzed after adjustment for randomization strata and prognostic baseline variables (age, region, NIHSS score, time from onset of the intracranial hemorrhage to randomization, volume and location of the hematoma, and presence or absence of intraventricular hemorrhage). The primary outcome was also analyzed according to various alternative cutoff points on the modified Rankin scale that have been used previously: a score of 0, 1, 2, or 3 as compared with scores of 4, 5, and 6 grouped together 26 and a score of 0 or 1 as compared with a score of 2, 3, 4, 5, or 6. 27\nWe assessed the heterogeneity of the treatment effect on the primary outcome in eight prespecified subgroups by adding an interaction term in an unadjusted logistic-regression model. The effects of treatment on relative and absolute changes in hematoma volume were assessed by means of an analysis of covariance. The baseline volume of the hematoma and the time from the onset of the intracerebral hemorrhage to the CT were included as covariates, since both predict hematoma growth. 4 The relative change in hematoma volume was log-transformed to remove skewness after the addition of the value 1.1 to eliminate negative values. The nominal level of significance for all analyses was P<0.048, since two interim analyses were performed in which the Haybittle–Peto efficacy stopping rule was used. 16\nResults\nTable 1.\nTable 1. Baseline Characteristics of the Participants.\nFrom October 2008 through August 2012, a total of 2839 participants (mean age, 63.5 years; 62.9% men) were enrolled at 144 hospitals in 21 countries; 1403 participants were randomly assigned to receive early intensive treatment to lower their blood pressure, and 1436 were assigned to receive guideline-recommended treatment (Fig. S1 in the Supplementary Appendix ). The baseline characteristics were balanced between the two groups ( Table 1 ). The primary outcome was determined for 1382 of the participants (98.5%) in the intensive-treatment group and for 1412 (98.3%) in the standard-treatment group.\nBlood-Pressure–Lowering Treatment and Achieved Blood-Pressure Levels\nTable 2.\nTable 2. Treatment of Patients with Intracerebral Hemorrhage.\nAs shown in Table 2 , the median time from the onset of the intracerebral hemorrhage to the initiation of intravenous treatment was shorter in the intensive-treatment group than in the standard-therapy group (4.0 hours [interquartile range, 2.9 to 5.1] vs. 4.5 hours [interquartile range, 3.0 to 7.0], P<0.001); the median time from randomization to the initiation of treatment was also shorter in the intensive-treatment group (6 minutes [interquartile range, 0 to 39] vs. 19 minutes [interquartile range, 0 to 167]). More patients in the intensive-treatment group than in the standard-treatment group received two or more intravenous agents to lower their blood pressure (26.6% vs. 8.1%, P<0.001). The mean systolic blood-pressure levels differed significantly between the two groups from 15 minutes to day 7 after randomization (Fig. S2 in the Supplementary Appendix ); at 1 hour, the mean systolic blood pressure was 150 mm Hg in the intensive-treatment group (with 462 patients [33.4%] achieving the target blood pressure of <140 mm Hg) as compared with 164 mm Hg in the standard-treatment group (a difference of 14 mm Hg, P<0.001). As shown in Table 2 , there were no significant differences between the two groups with respect to other aspects of medical care during the 7 days after randomization, except that a decision to withdraw active treatment and care was made in the case of more participants in the intensive-treatment group than in the standard-treatment group (75 participants [5.4%] vs. 46 participants [3.3%], P=0.005).\nClinical Outcomes and Serious Adverse Events\nTable 3.\nTable 3. Primary, Secondary, and Safety Outcomes at 90 Days.\nAt 90 days, 719 of the participants (52.0%) in the intensive-treatment group, as compared with 785 (55.6%) in the standard-treatment group, had a poor outcome (odds ratio with intensive treatment, 0.87; 95% confidence interval [CI], 0.75 to 1.01; P=0.06) ( Table 3 ). The ordinal analysis showed a significant favorable shift in the distribution of scores on the modified Rankin scale with intensive blood-pressure–lowering treatment (pooled odds ratio for shift to higher modified Rankin score, 0.87; 95% CI, 0.77 to 1.00; P=0.04) ( Table 3 , and Fig. S3 in the Supplementary Appendix ). Adjusted analyses showed consistency in the treatment effect with respect to the primary and key secondary outcomes in logistic-regression models that included prognostic variables and various cutoff points on the modified Rankin scale (Table S1 in the Supplementary Appendix ).\nIn the assessment of the five domains of the EQ-5D, participants in the intensive-treatment group reported fewer problems and had significantly better overall health-related quality of life at 90 days than did those in the standard-therapy group (mean [±SD] utility score, 0.60±0.39 vs. 0.55±0.40; P=0.002) ( Table 3 ).\nFigure 1.\nFigure 1. Effect of Early Intensive Blood-Pressure–Lowering Treatment on the Primary Outcome, According to Prespecified Subgroups.\nThe primary outcome of the study was death or major disability, defined as a score of 3 to 6 on the modified Rankin scale (in which a score of 0 indicates no symptoms, a score of 5 indicates severe disability, and a score of 6 indicates death) at 90 days. Each percentage is based on the number of people in that subgroup. The black squares represent point estimates (with the area of the square proportional to the number of events), and the horizontal lines represent 95% confidence intervals. The diamond incorporates the point estimate, represented by the vertical dashed line, as well as the 95% confidence intervals, of the overall effects within categories. Scores on the National Institutes of Health Stroke Scale (NIHSS) range from 0 (normal neurologic status) to 42 (coma with quadriplegia).\nThe rate of death from any cause was similar in the intensive-treatment group and the standard-treatment group (11.9% and 12.0%, respectively) ( Table 3 ), as was the percentage of these deaths attributed to the direct effect of the intracerebral hemorrhage (61.4% and 65.3%, respectively). The effects of intensive lowering of blood pressure were consistent across all prespecified subgroups ( Figure 1 ). There were no significant differences between the two groups in any of the other outcomes studied. The numbers of serious adverse events, including episodes of severe hypotension (which occurred in <1% of the participants), were also balanced between the two groups ( Table 3 ).\nHematoma Outcomes\nThe prespecified subgroup of participants who underwent repeat brain imaging for an assessment of the between-group difference in hematoma growth from baseline to 24 hours consisted of 491 of the 1399 participants with 90-day outcome data (35.1%) in the intensive-treatment group and 473 of the 1430 participants with 90-day outcome data (33.1%) in the standard-treatment group. The mean hematoma volumes were 15.7±15.7 ml and 15.1±14.9 ml in the two groups, respectively, at baseline and 18.2±19.1 ml and 20.6±24.9 ml, respectively, at 24 hours (Table S2 and Fig. S4 in the Supplementary Appendix ). The difference in hematoma growth between the groups in the 24 hours after baseline was not significant (relative difference, 4.5% [95% CI, −3.1 to 12.7; P=0.27], and absolute difference, 1.4 ml [95% CI, −0.6 to 3.4; P=0.18], after adjustment for prognostic variables).\nDiscussion\nIn this trial involving patients with intracranial hemorrhage, early intensive lowering of blood pressure, as compared with the more conservative level of blood-pressure control currently recommended in guidelines, did not result in a significant reduction in the rate of the primary outcome of death or major disability. However, in an ordinal analysis of the primary outcome, in which the statistical power for assessing physical functioning was enhanced, there were significantly better functional outcomes among patients assigned to intensive treatment to lower their blood pressure than among patients assigned to guideline-recommended treatment. 22,28 Furthermore, there was significantly better physical and psychological well-being among patients who received intensive treatment. These results are consistent with observational epidemiologic findings associating high blood-pressure levels with poor outcomes among patients with intracerebral hemorrhage 7-11 and indicate that early intensive lowering of blood pressure in this patient population is safe.\nThere was no clear evidence of heterogeneity in the effect of treatment in any prespecified subgroup — not even in the subgroup defined according to region (China vs. elsewhere). Moreover, there was no evidence of a significant effect modification according to a history or no history of hypertension — a finding that is relevant because it has been postulated that patients with hypertension have an upward shift in cerebral autoregulation and possibly an increased risk of cerebral ischemia related to intensive lowering of blood pressure. 8 However, given the critical nature and rapid evolution of bleeding in the brain, a somewhat surprising finding was the absence of a significant difference in the effect of treatment between patients who underwent randomization early (within 4 hours after the intracerebral hemorrhage) and those who underwent randomization later. This could reflect either the limited power of the subgroup analyses or true independence of the effect of the intervention from the time of initiation of treatment. Since early intensive lowering of blood pressure did not have a clear effect on reducing the growth of the hematoma, a key determinant of early death, there may be other mechanisms at play, such as neuroprotection or a reduction in edema, that result in the later positive clinical outcomes with this treatment. The ongoing Antihypertensive Treatment of Acute Cerebral Hemorrhage (ATACH) II trial 29 is expected to provide additional information on the role of intensive lowering of blood pressure within 4.5 hours after the onset of a intracerebral hemorrhage, but future evaluations of the treatment in patients with intracerebral hemorrhage that are conducted in the prehospital setting or at more extended periods after onset than were tested in INTERACT2 may be warranted.\nThe current trial has several strengths, including the large sample size, central concealment of treatment assignments, and high rates of follow-up and adherence to treatment. Furthermore, the collection of data on serious adverse events, including hypotension, ensured that any potential harms were reliably detected and quantified. In addition, the range of drug therapies used and of outcomes assessed in participants from a variety of hospitals in different countries enhances the generalizability of the final results.\nSome limitations should also be noted. First, although the option to use a range of available drug therapies rather than a single agent was a strength of the study, it introduced complexity in assessing the ways in which the effects may have varied across different agents. Moreover, in the open (unblinded) assignment of interventions that led to earlier and more intensive, as compared with less intensive, control of blood pressure, the outcomes may have been confounded by differences in the management strategies that were used for the two groups after randomization, other than those that were documented. Second, although we used established scales and objective criteria, some bias may have been introduced in the assessment of key outcomes. Third, the difference in the blood-pressure levels achieved between the two groups may have been attenuated by the use of an active-comparator control group and the concomitant use of additional agents with blood-pressure–lowering properties (e.g., mannitol) or hemostatic properties (e.g., recombinant tissue factor VIIa); if this is so, however, the magnitude of the benefit of early intensive blood-pressure–lowering treatment could be greater in settings in which only the very highest levels of blood pressure are treated in the hyperacute phase of stroke.\nIn summary, early intensive lowering of blood pressure did not result in a significant reduction in the rate of the primary outcome of death or major disability, but an ordinal analysis of scores on the modified Rankin scale did suggest that intensive treatment improved functional outcomes. Intensive lowering of blood pressure was not associated with an increase in the rates of death or serious adverse events.\nFunding and Disclosures\nSupported by a program grant (571281), project grants (512402 and 1004170), a Senior Principal Research Fellowship (to Dr. Anderson), and a Principal Research Fellowship (to Dr. Neal) from the National Health and Medical Research Council (NHMRC) of Australia. Drs. Neal, Arima, and Parsons are recipients of Future Fellowships from the Australian Research Council. Dr. Hata is a recipient of a Postgraduate Fellowship from the High Blood Pressure Research Council of Australia.\nDr. Huang reports receiving reimbursement for travel expenses from Osaka Pharmaceuticals; Dr. Jiguang Wang, receiving consulting fees from Novartis, Omron Healthcare, Pfizer, and Takeda, grant support from Novartis, Omron Healthcare, and Pfizer, lecture fees from A&D Pharma, Omron Healthcare, Novartis, Pfizer, and Servier, and reimbursement for travel expenses from Pfizer and Takeda; Dr. Lavados, receiving grant support from Lundbeck, payment for manuscript preparation from BMJ, and payment for advisory board membership from Bristol-Myers Squibb; and Dr. Davis, receiving lecture fees from Boehringer Ingelheim, Sanofi-Aventis, and EVER Neuro Pharma. No other potential conflict of interest relevant to this article was reported.\nDisclosure forms provided by the authors are available with the full text of this article at NEJM.org.\nThis article was published on May 29, 2013, at NEJM.org.\nWe thank the patients who participated in this trial and their relatives; the clinical and research teams of the various emergency departments, intensive care units, stroke units, and neurology departments; Vlado Perkovic, Stephen MacMahon, and Gary Ford for their support; the staff of Apollo Medical Imaging Technology in Melbourne, Australia, for their support of the MiStar software used in the CT analyses; and Beijing MedSept Consulting for developing the interactive voice-activated system that was used for the randomization process in China.\nAuthor Affiliations\nThe authors' affiliations are listed in the Appendix.\nAddress reprint requests to Dr. Anderson at the George Institute for Global Health, Royal Prince Alfred Hospital and the University of Sydney, P.O. Box M201, Missenden Rd., Sydney NSW 2050, Australia, or at canderson@georgeinstitute.org.au .\nInvestigators in the second Intensive Blood Pressure Reduction in Acute Cerebral Hemorrhage Trial (INTERACT2) are listed in the Supplementary Appendix , available at NEJM.org.\nAppendix\nThe authors' affiliations are as follows: George Institute for Global Health (C.S.A., E.H., C.D., R.L., B.N., J.H., H.A., S.H., Q.L., M.W., J.C.) and National Health and Medical Research Council Clinical Trials Centre (R.J.S.), University of Sydney, and the Neurology Department, Royal Prince Alfred Hospital (C.S.A., C.D.), Sydney, the Department of Neurology, John Hunter Hospital, University of Newcastle, Newcastle, NSW (M.P.), and Melbourne Brain Centre, Royal Melbourne Hospital and University of Melbourne, Melbourne, VIC (S.M.D.) — all in Australia; the Department of Neurology, Peking University First Hospital, Beijing (Y.H.), the Shanghai Institute of Hypertension, Rui Jin Hospital, Shanghai Jiaotong University, Shanghai (Jiguang Wang), the Department of Neurology, Baotou Central Hospital, Baotou (Y.L.), and the Department of Neurology, Yutian County Hospital, Tangshan, Hebei Province (Jinchao Wang) — all in China; the Department of Neurology, Assistance Publique–Hôpitaux de Paris–Hôpital Lariboisière and DHU NeuroVasc Paris–Sorbonne, Université Paris Diderot–Sorbonne Paris Cité, Paris (C.S.); the Department of Cardiovascular Sciences and NIHR Biomedical Research Unit for Cardiovascular Sciences, University of Leicester, Leicester, United Kingdom (T.R.); Servicio de Neurología, Departamento de Medicina, Clínica Alemana, Universidad del Desarrollo, and Universidad de Chile, Santiago (P.L.) — both in Chile; the Department of Environmental Medicine, Graduate School of Medical Sciences, Kyushu University, Fukuoka, Japan (J.H.); and the Department of Epidemiology, Johns Hopkins University, Baltimore (M.W.).\nSupplementary Material\nReferences (29)\n1. Qureshi AI, Tuhrim S, Broderick JP, Batjer HH, Hondo H, Hanley DF. Spontaneous intracerebral hemorrhage. N Engl J Med 2001;344:1450-1460\n""","0.27555382","""http://www.nejm.org/doi/10.1056/NEJMoa1214609""","[-0.178219,51.500505]"
"""Imperial_College_London""","""Design and Evaluation of the Kinect-Wheelchair Interface Controlled (KWIC) Smart Wheelchair for Pediatric Powered Mobility Training: Assistive Technology: Vol 27, No 3""","""Altmetric\nOriginal Articles\nDesign and Evaluation of the Kinect-Wheelchair Interface Controlled (KWIC) Smart Wheelchair for Pediatric Powered Mobility Training\nAccepted author version posted online: 26 May 2015\nPublished online: 26 May 2015\nGet access /doi/full/10.1080/10400435.2015.1012607?needAccess=true\nAbstract\nBackground: Children with severe disabilities are sometimes unable to access powered mobility training. Thus, we developed the Kinect-Wheelchair Interface Controlled (KWIC) smart wheelchair trainer that converts a manual wheelchair into a powered wheelchair. The KWIC Trainer uses computer vision to create a virtual tether with adaptive shared-control between the wheelchair and a therapist during training. It also includes a mixed-reality video game system. Methods: We performed a year-long usability study of the KWIC Trainer at a local clinic, soliciting qualitative and quantitative feedback on the device after extended use. Results: Eight therapists used the KWIC Trainer for over 50 hours with 8 different children. Two of the children obtained their own powered wheelchair as a result of the training. The therapists indicated the device allowed them to provide mobility training for more children than would have been possible with a demo wheelchair, and they found use of the device to be as safe as or safer than conventional training. They viewed the shared control algorithm as counter-productive because it made it difficult for the child to discern when he or she was controlling the chair. They were enthusiastic about the video game integration for increasing motivation and engagement during training. They emphasized the need for additional access methods for controlling the device. Conclusion: The therapists confirmed that the KWIC Trainer is a useful tool for increasing access to powered mobility training and for engaging children during training sessions. However, some improvements would enhance its applicability for routine clinical use.\n""","1.0","""http://www.tandfonline.com/doi/full/10.1080/10400435.2015.1012607""","[-0.178219,51.500505]"
"""University_of_Surrey""","""Finding effective pathways to sustainable mobility: bridging the science–policy gap: Journal of Sustainable Tourism: Vol 24, No 3""","""关键词: 气候变化 ,  社会技术因素 ,  科技神话 ,  运输禁忌 ,  理想期货\nIntroduction\nDemand is increasing for all transport modes. The transport sector, including tourism and all other transport motivations, is growing more rapidly than most other sectors and is currently responsible for approximately 23% of global energy-related CO2 emissions (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ). Because of the sector's rising contributions to climate change, considerable effort has recently been invested by researchers to try to understand if people are willing to voluntarily change their tourism and transport behaviour (e.g. Becken, 2007 Becken, S. (2007). Tourists' perception of international air travel's impact on the global climate and potential climate change policies. Journal of Sustainable Tourism, 15, 351–368. [Taylor & Francis Online]   [Google Scholar] ; Higham, Cohen, Cavaliere, Reis, & Finkler, 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ; Kroesen, 2013 Kroesen, M. (2013). Exploring people's viewpoints on air travel and climate change: Understanding inconsistencies. Journal of Sustainable Tourism, 21(2), 271–290. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Miller, Rathouse, Scarles, Holmes, & Tribe, 2010 Miller, G., Rathouse, K., Scarles, C., Holmes, K., & Tribe, J. (2010). Public understanding of sustainable tourism. Annals of Tourism Research, 37(3), 627–645. [Crossref] , [Web of Science ®]   [Google Scholar] ). The weight of evidence clearly shows that, while awareness of the impact of mobility on climate change, and particularly that of air travel, is growing, there has been little if any actual behavioural change by tourists to travel less or to change travel modes (Higham, Cohen, Peeters, & Gössling, 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). This realisation has been captured in numerous publications evidencing or attempting to explain an awareness–attitude or attitude–behaviour gap (e.g. Antimova, Nawijn, & Peeters, 2012 Antimova, R., Nawijn, J., & Peeters, P. (2012). The awareness/attitude gap in sustainable tourism: A theoretical perspective. Tourism Review, 67(3), 7–16. [Crossref]   [Google Scholar] ; Cohen, Higham, & Reis, 2013 Cohen, S.A., Higham, J., & Reis, A. (2013). Sociological barriers to sustainable discretionary air travel behaviour. Journal of Sustainable Tourism, 21(7), 982–998. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hares, Dickinson, & Wilkes, 2010 Hares, A., Dickinson, J., & Wilkes, K. (2010). Climate change and the air travel decisions of UK tourists. Journal of Transport Geography, 18(3), 466–473. [Crossref] , [Web of Science ®]   [Google Scholar] ; Juvan & Dolnicar, 2014 Juvan, E., & Dolnicar, S. (2014). The attitude-behaviour gap in sustainable tourism. Annals of Tourism Research, 48, 76–95. [Crossref] , [Web of Science ®]   [Google Scholar] ). Meanwhile, calls were sounding that the very concept of voluntary behaviour change itself was trapped within the constraints of neoliberalism (Barr, Gilg, & Shaw, 2011 Barr, S., Gilg, A., & Shaw, G. (2011). Citizens, consumers and sustainability: (Re)framing environmental practice in an age of climate change. Global Environmental Change, 21, 1224–1233. [Crossref] , [Web of Science ®]   [Google Scholar] ; Schwanen, Banister, & Anable, 2011 Schwanen, T., Banister, D., & Anable, J. (2011). Scientific research about climate change mitigation in transport: A critical review. Transportation Research Part A, 45, 993–1006. [Crossref]   [Google Scholar] ). These calls urged the academy to pay closer attention to the political, social and material systems in which consumption practices are structured, arguing that the “carbon capability” (Whitmarsh, Seyfang, & O'Neill, 2011 Whitmarsh, L., Seyfang, G., & O'Neill, S. (2011). Public engagement with carbon and climate change: To what extent is the public “carbon capable”? Global Environmental Change, 21, 56–65. [Crossref] , [Web of Science ®]   [Google Scholar] ) of the public is limited by the “systems of provision” (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), which create what Schwanen et al. ( 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ) refer to as “path dependencies”.\nA turn towards path dependencies does not suggest that attempts to achieve public behavioural change should be abandoned. Rather it emphasises that devolving the problem of tourism and transport's impacts on climate change to individuals is a limited framing. In addition to social marketing efforts aimed downstream at effecting behavioural change in publics (see Hall, in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , this issue), and a continued drive by industry towards marginal efficiency gains available in aviation technologies (Cumpsty et al., 2010 Cumpsty, N., Alonso, J., Eury, S., Maurice, L., Nas, B., Ralph, M., & Sawyer, R. (2010). Report of the independent experts on medium and long term goals for aviation fuel burn reduction from technology. Montreal: ICAO.  [Google Scholar] ; Peeters & Middel, 2007 Peeters, P.M., & Middel, J. (2007). Historical and future development of air transport fuel efficiency. In R. Sausen, A. Blum, D.S. Lee, & C. Brüning (Eds.), Proceedings of an International Conference on Transport, Atmosphere and Climate (TAC), Oxford, United Kingdom, 26–29 June 2006 (pp. 42–47). Oberpfaffenhofen: DLR Institut für Physic der Atmosphäre.  [Google Scholar] ), it is imperative that research focuses on how the radical socio-technical transitions that are necessary to put the tourism and transport sectors on a sustainable emissions path can be achieved. Technical solutions alone will be too little and too late (Chèze, Chevallier, & Gastineau, 2013 Chèze, B., Chevallier, J., & Gastineau, P. (2013). Will technological progress be sufficient to stabilize CO2 emissions from air transport in the mid-term? (No. Les cahiers de l'économie – no. 94). Rueil-Malmaison: Centre Économie et Gestion.  [Google Scholar] ; Peeters, Higham, Kutzner, Cohen, & Gössling, under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ). This includes not only a recognition that the present socio-technical landscape is dominated by neoliberal, techno-centric and ecological modernisation values (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hopkins & Higham, 2012 Hopkins, D., & Higham, J.E.S. (2012). Framework conventions for climate change: An analysis of global framework conventions with reference to resource governance and environmental management approaches in New Zealand. In A. Holden & D. Fennell (Eds.), A handbook of tourism and the environment (pp. 227–240). London: Routledge.  [Google Scholar] ), but also the need for a concerted effort by tourism and transport researchers to become active advocates of pathways to structural change, influence policy learning and provide politicians with tools to simulate policy-making and its effects.\nThe Freiburg 2014 workshop\nThe Freiburg 2014 workshop, held in Freiburg im Breisgau in Germany (1–4 July 2014), sought to address the inability of policy-makers and other stakeholders to change the tourism mobility system towards sustainable development. Its objectives stemmed directly from the Freiburg 2012 workshop, the results of which were disseminated in the Journal of Sustainable Tourism (volume 21, issue 7; see Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) and in an edited book (Cohen, Higham, Peeters, & Gössling, 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] ).\nA key outcome from Freiburg 2012 was the conclusion that the public are generally unwilling or unable to change tourism and transport behaviour based on an awareness of environmental impacts, and specifically climate change. It was concluded that “the autonomy of individual pro-environmental response, when set within the systems of provision in late-capitalist consumer society, is fraught with challenge” (Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , p. 14) and that the “low sustainability of the current tourism system is embedded in structures that make it easy and often cheaper to travel unsustainably … raising a wide range of questions regarding transport infrastructures, taxation, management and governance” (Cohen et al. 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] , p. 301). This conclusion illustrated the need to move beyond voluntary behavioural change in order to achieve sustainable mobility, and to explore the socio-technical landscapes in which individuals are embedded, through which public behaviour is conditioned and patterned.\nA further crucial conclusion from Freiburg 2012 was that policy-makers had shown limited interest in adopting policy measures that would achieve significant changes in sustainable transport behaviour. This lack of political initiative, wherein it was clear that politicians have far more links to industry than to science, and particularly to the social sciences, suggested that the reasons behind the inaction in transport governance needed to be urgently and critically explored. Overall, it was evident that while a comprehensive understanding of the psychologies of tourism and transport consumption is necessary to inform policy-makers, this alone would not be enough to bring the sectors onto a climatically sustainable pathway, and that radical transitions in the systems of provision and deeper understandings of political psychologies are needed (Cohen et al., 2014 Cohen, S.A., Higham, J.E.S., Peeters, P., & Gössling, S. (2014). Understanding and governing sustainable tourism mobility: Psychological and behavioural approaches. London: Routledge.  [Google Scholar] ; Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham et al., 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nThese insights formed the basis for the Freiburg 2014 workshop, which expanded its discussions to focus on public behaviour change, but also on how to change the behaviour of policy-makers, industry stakeholders and researchers themselves, to help achieve changes in tourism and transport systems for environmental reasons. Central to this endeavour was the question of how to bridge the science–policy gap: it was abundantly clear that despite the substantial and expanding body of research on tourism and climate change (Hall et al., 2015 Hall, C.M., Amelung, B., Cohen, S., Eijgelaar, E., Gössling, S., Higham, J., … Scott, D. (2015). On climate change skepticism and denial in tourism. Journal of Sustainable Tourism, 23(1), 4–25. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), and on transport and climate change (Schwanen, et al., 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ), this corpus of knowledge was having little to no effect in practice on governance.\nThe Freiburg 2014 workshop is the basis for this special issue presenting 10 papers, including this overview paper, exploring the dimensions and details of the science–policy gap in sustainable mobility. Having established the context in which the workshop was set, and before introducing the papers in this special issue, we now discuss three essential themes that are vital to understanding why, despite clear scientific evidence as to the growing environmental impacts of tourism transport, and particularly air travel, there is large-scale inertia in structural transitions and a lack of political willpower to enact meaningful policy change: (1) the importance of addressing socio-technical factors, (2) the barriers posed by placing faith in “technology myths” and (3) the need to overcome “transport taboos” in policy-making. The paper concludes by setting a research agenda that forms the basis for the forthcoming Freiburg 2016 workshop (28 June to 1 July 2016).\nSocio-technical factors\nCurrent growth trajectories indicate that transport emissions will double by 2050: the global fleet of light-duty vehicles is expected to double during that time period, and “demand for freight transport (road, rail, shipping, and air) and passenger aviation is projected to surge as well” (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] , p. 911). Air travel in particular offers an example of largely intractable public travel behaviours that are entrenched in Europe and North America and being rapidly adopted in the emerging regions of the world (Freire-Medeiros & Name, 2013 Freire-Medeiros, B., & Name, L. (2013). Flying for the very first time: Mobilities, social class and environmental concerns in a Rio de Janeiro favela. Mobilities, 8(2), 167–184. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The development of low-cost, high-volume aviation, initially in Europe and North America, and latterly in Brazil, Russia, India and China, is now powering similar air travel growth trajectories in the emerging neoliberal economies of Mexico, South Africa, Indonesia and Turkey (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ). Growth in air travel over the last two decades has been rapid (Gössling & Upham, 2009 Gössling, S., & Upham, P. (Eds.). (2009). Climate change and aviation: Issues, challenges and solutions. London: Earthscan.  [Google Scholar] ), and the current growth trajectory is projected to continue at a rate of 3.3% per annum to 2030 (Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nCalls for tourism to move onto a sustainable emissions path (Becken, 2007 Becken, S. (2007). Tourists' perception of international air travel's impact on the global climate and potential climate change policies. Journal of Sustainable Tourism, 15, 351–368. [Taylor & Francis Online]   [Google Scholar] ) have been especially challenged by growing demand for air travel. This growth has been driven by significant structural changes in the transportation sector (Ryley, Davison, Bristow, & Pridmore, 2010 Ryley, T., Davison, L., Bristow, A., & Pridmore, A. (2010). Public engagement on aviation taxes in the United Kingdom. International Journal of Sustainable Transportation, 4(2), 112–128. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). Neoliberalism has been embraced by the aviation industry through airline deregulation, the deployment of frequent flyer loyalty programmes and notably by the unrestrained growth of low-cost carriers (LCCs) (Duval, 2013 Duval, D.T. (2013). Critical issues in air transport and tourism. Tourism Geographies, 15(3), 494–510. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). Network airlines offering full service and traditional routes have been drawn into intense competition with LCCs, transforming the travel market through the uptake of technology to substantially reduce labour costs (Holloway, Humphreys, & Davidson, 2009 Holloway, J.C., Humphreys, C., & Davidson, R. (2009). The business of tourism (8th ed.). Harlow: Pearson Education.  [Google Scholar] ). The LCCs have developed direct sales systems (i.e. internet booking systems), online check-in and product itemisation (e.g. priority boarding, seat allocation, baggage allowances, in-flight food and entertainment services) to de-personalise airport and air travel experiences, while increasing aircraft utilisation and flight loadings, while reducing fares (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ; Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ). The LCC business model includes operating with a single aircraft type, providing a single-class product, serving secondary airports and avoiding the costs of frequent flyer programmes (Boeing, 2014 Boeing. (2014). Current market outlook 2014-2033. Seattle, WA: Boeing Commercial Airplanes.  [Google Scholar] ).\nThe consequential growth in demand for low-cost air travel is recognised as “one of the biggest revolutions in tourism and travel since the package holiday's arrival half a century earlier” (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] , p. 176). The success of the LCCs is reflected in the increasingly ordinary nature of air travel in certain sections of some societies (Randles & Mander, 2009a Randles, S., & Mander, S. (2009a). Practice(s) and ratchet(s): A sociological examination of frequent flying. In S. Gössling & P. Upham (Eds.), Climate change and aviation: Issues, challenges and solutions (pp. 245–271). London: Earthscan.  [Google Scholar] ; Urry, 2010 Urry, J. (2010). Sociology and climate change. The Sociological Review, 57(2), 84–100. [Crossref]   [Google Scholar] ). New structures of air travel provision have created flying as a highly accessible consumer product, shifting leisure travel into the domain of everyday consumer capitalism (Young, Higham, & Reis, 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). Indeed the extent to which these structures have shaped and influenced everyday consumer practices has, in some cases, reached absurd proportions. Ben Schlappig – “the man who flies around the world for free” – is “… one of the biggest stars among an elite group of obsessive flyers whose mission is to outwit the airlines” (Wofford, 2015 Wofford, B. (2015). Up in the air: Meet the man who flies around the world for free. Rolling Stone Magazine, 20 July 2014.  [Google Scholar] , p. 3). Perfecting the art of “travel hacking” – known among its members as “The Hobby” – Schlappig seeks perfection in the art of non-stop air travel and consumer luxury that is paid for by a “… gargantuan cache of frequent flier miles that grows only bigger by the day” (Wofford, 2015 Wofford, B. (2015). Up in the air: Meet the man who flies around the world for free. Rolling Stone Magazine, 20 July 2014.  [Google Scholar] , p. 5). Schlappig's claim is to be “beating the airlines at their own game”; through the gaming of frequent flyer programmes using techniques that he shares with a half million strong following through the “FlyerTalk” website.\nThe gaming of frequent flyer programmes offers an, albeit extreme, insight into consumer air travel behaviour that is anchored in and enabled by the socio-technical system. It forms part of a wider pattern of increasing affordability and uptake of air travel across an expanding range of social classes and societies (Randles & Mander, 2009b Randles, S., & Mander, S. (2009b). Aviation, consumption and the climate change debate: ‘Are you going to tell me off for flying?’ Technology Analysis & Strategic Management, 21(1), 93–113. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). It is this growth in demand for air travel that contributes significantly to driving up tourism transport emissions (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). With current and projected growth in aviation emissions has come recognition that the freedom to engage in unrestrained air travel comes with significant environmental costs (IPCC, 2013 IPCC. (2013). Climate change 2013: The physical science basis. Retrieved 28 November 2013 from IPCC web site: http://www.ipcc.ch/report/ar5/wg1/#.Uu70df0p8ds  [Google Scholar] ). While the environmental costs of air travel are now widely understood and accepted by the travelling public, the necessary responses in terms of consumer demand have not followed (Gössling, 2009 Gössling, S., & Upham, P. (Eds.). (2009). Climate change and aviation: Issues, challenges and solutions. London: Earthscan.  [Google Scholar] ; Higham, Cohen, Peeters, & Gössling, 2013 Higham, J.E.S., Cohen, S.A., Peeters, P., & Gössling, S. (2013). Psychological and behavioural approaches to understanding and governing sustainable mobility. Journal of Sustainable Tourism, 21(7), 949–967. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nIt is ironic that the increasingly aeromobile middle classes can be the same people who claim to be environmentally aware and moral consumers (Dolnicar, Juvan, Ring, & Leisch, in press Dolnicar, S., Juvan, E., Ring, A. & Leisch, F. (in press). Tourist segments' justifications for behaving in an environmentally unsustainable way. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.10248861 [Crossref] , [Web of Science ®]   [Google Scholar] ; Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). Within the context of air travel the “flyers’ dilemma” describes the tension between the self-identity of consumers who feel moral responsibility for their consumer decisions, and the high environmental costs of flying (Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ; Rosenthal, 2010 Rosenthal, E. (2010, May 24). Can we kick our addiction to flying? Retrieved 13 September 2010 from The Guardian web site: http://www.guardian.co.uk/environment/2010/may/24/kick-addiction-flying/  [Google Scholar] ). The anxieties arising from the “flyers’ dilemma” have been empirically examined in various European societies (Higham, et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ). Various studies have highlighted that air travel practices are largely unconstrained because flying is a cheap, convenient and socially desirable form of leisure consumption (Cohen & Higham, 2011 Cohen, S.A., & Higham, J.E.S. (2011). Eyes wide shut? UK consumer perceptions on aviation climate impacts and travel decisions to New Zealand. Current Issues in Tourism, 14(4), 323–335. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham & Cohen, 2011 Higham, J.E.S., & Cohen, S.A. (2011). Canary in the coalmine: Norwegian attitudes towards climate change and extreme long-haul air travel to Aotearoa/New Zealand. Tourism Management, 32(1), 98–105. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nIt emerges that a focus on the demand-side of travel, in an attempt to address issues of sustainability through behaviour change, should not ignore the fundamental socio-structural factors that underpin the tourism system (Cornelissen, 2005 Cornelissen, S. (2005). The global tourism system. Aldershot: Ashgate.  [Google Scholar] ). Young, Markham, Reis, and Higham ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) argue that the locus of responsibility is critical to debates around sustainable aviation and sustainable mobility more broadly. Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , p. 1101) observes that “mutual reinforcement between modes of governance and intervention … creates a path dependency in which solutions to sustainable tourism mobility are only identified within ‘green growth’ arguments for greater efficiency and market-based solutions.” Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) argues that as long as the focus of government tourism policies remains situated in a GDP growth paradigm, the structures of transport provision will remain unchanged and the environment required to empower a consumer-led shift to a sustainable transport emissions path will not exist.\nYoung et al. ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) suggest that appealing to individuals to reduce or otherwise moderate their tourist travel consumption practices is a flawed response. It lacks the necessary government policy response to an industry that is environmentally damaging. Appealing to consumer sacrifice ignores the fundamental socio-structural underpinnings of an unsustainable travel industry. Young et al. ( 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ) critically consider the social, institutional and economic forces that produce excessive and unsustainable travel consumption. They highlight, first and foremost, that within the existing structures of the aviation industry, currently no alternative options are available to avoid the high environmental costs of air travel (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). No low-emission form of aviation exists to serve flyers who are concerned about climate change and aviation emissions (Peeters & Middel, 2007 Peeters, P.M., & Middel, J. (2007). Historical and future development of air transport fuel efficiency. In R. Sausen, A. Blum, D.S. Lee, & C. Brüning (Eds.), Proceedings of an International Conference on Transport, Atmosphere and Climate (TAC), Oxford, United Kingdom, 26–29 June 2006 (pp. 42–47). Oberpfaffenhofen: DLR Institut für Physic der Atmosphäre.  [Google Scholar] ), and neither is there any real prospect of major gains in aviation fuel efficiency in the short–medium term future (Peeters et al., under review) Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] .\nWhile the airline industry has made significant gains in efficiency since the advent of jet aviation (Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ), current technologies are locked in for periods of time that reach well beyond the urgent time frame required to achieve radical emission reductions (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ). Jet aviation is highly efficient in terms of time/distance/cost thresholds (Howitt, Revol, Smith, & Rodger, 2010 Howitt, O.J.A., Revol, V.G.N., Smith, I.J., & Rodger, C.J. (2010). Carbon emissions from international cruise ship passengers' travel to and from New Zealand. Energy Policy, 38, 2552–2560. [Crossref] , [Web of Science ®]   [Google Scholar] ), but those energy efficiencies have been overwhelmed in real terms by growth in demand (Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ), such that the environmental costs of air travel have become unacceptably high (Gössling, Hall, Peeters, and Scott 2010 Scott, D., Peeters, P. & Gössling, S. (2010). Can tourism deliver its aspiration greenhouse gas emission reduction targets? Journal of Sustainable Tourism, 18(3), 393–408. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hall, 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The current structure of the aviation industry and the absence of further substantial technical gains in aircraft efficiency (Scott, Peeters, & Gössling, 2010 Scott, D. (2016). The Paris Climate Change Conference and the tourism industry. Journal of Sustainable Tourism, under review  [Google Scholar] ) are such that aviation emissions are expected to double within a 25–45 year time frame (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ).\nDespite growing sustainability concerns, aviation, like the automobile (Dennis & Urry, 2009 Dennis, K., & Urry, J. (2009). After the car. Cambridge: John Wiley & Sons.  [Google Scholar] ), has become an integral part of contemporary mobility in many societies (Sheller & Urry, 2004 Sheller, M., & Urry, J. (Eds.). (2004). Tourism mobilities: Places to play, places in play. London: Routledge.  [Google Scholar] ; Urry, 2012 Urry, J. (2012). Social networks, mobile lives and social inequalities. Journal of Transport Geography, 21, 24–30. [Crossref] , [Web of Science ®]   [Google Scholar] ). Flying now out-competes other transport modes not only on convenience and time efficiency but – most critically – in terms of cost (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ). The time, cost and convenience advantages of air travel are structural factors that explain the deeply embedded nature of air travel practices (Higham, Cohen, & Cavaliere, 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The aviation system allows the production of tourism (and other forms of mobility) to be accelerated in terms of fit within the capitalist working day, week and calendar year (Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ). Given the fundamentally energy-intensive nature of the tourism system (Becken, 2016 Becken, S. (2016). Peak oil: A hidden issue? Social representations of professional tourism perspectives. Journal of Sustainable Tourism, 24(1), 31–51. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), mitigating tourism transport emissions has proved a very imposing challenge (Schwanen et al., 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] ). Not least, neither car nor rail nor even high-speed rail can match the cost, convenience and flexibility of response that air travel can, especially if sea crossings are involved.\nIt is also apparent that travellers, even those who are concerned about their personal leisure travel emissions, are able to disregard their environmental concerns and take advantage of cheap and convenient air travel opportunities (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ; Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ). Even for those who are deeply concerned about climate change (see Cohen, & Higham, 2011 Cohen, S.A., & Higham, J.E.S. (2011). Eyes wide shut? UK consumer perceptions on aviation climate impacts and travel decisions to New Zealand. Current Issues in Tourism, 14(4), 323–335. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Higham & Cohen, 2011 Higham, J.E.S., & Cohen, S.A. (2011). Canary in the coalmine: Norwegian attitudes towards climate change and extreme long-haul air travel to Aotearoa/New Zealand. Tourism Management, 32(1), 98–105. [Crossref] , [Web of Science ®]   [Google Scholar] ), the time and costs advantages of air travel undermine the competitiveness of alternative, more sustainable transport modes (Higham et al., 2014 Higham, J., Cohen, S., & Cavaliere, C. (2014). Climate change, discretionary air travel and the ‘flyers’ dilemma'. Journal of Travel Research, 53(4), 462–475. [Crossref] , [Web of Science ®]   [Google Scholar] ). The time/distance/cost dimensions of air travel have also allowed the consumption of distant tourist destinations to fit within narrow windows of time (e.g. the EasyJet generation of weekend “escape artists”). The act of flying has become integral to significant parts of the contemporary tourism transport system (Young et al., 2015 Young, M., Markham, F., Reis, A.C., & Higham, J.E.S. (2015). Flights of fantasy: A reformulation of the flyers' dilemma. Annals of Tourism Research, 54, 1–15. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nAs Young et al. ( 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ) observe, aviation has proven to be resistant to consumer-led change, in contrast to other aspects of consumer behaviour, such as food purchases, recycling, the use of public land transport and the conscious uptake of active transport modes, which are relatively open to modification by individuals (Barr, Shaw, Coles, & Prillwitz, 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Lassen, 2010 Lassen, C. (2010). Environmentalist in business class: An analysis of air travel and environmental attitude. Transport Reviews, 30(6), 733–751. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The intractability of air travel behaviour change occurs not only because air travel has become a desirable and affordable gateway to tourism in many societies (Casey, 2010 Casey, M.E. (2010). Low cost air travel: Welcome aboard? Tourist Studies, 10(2), 175–191. [Crossref]   [Google Scholar] ), but also because “… the environmental risks associated with air travel are global and systemic, as opposed to specific and individual, and tend not to be prioritised within a flyers' environmental consciousness” (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 60). As a result climate concerns may be temporarily suspended with impunity so that consumers can continue to consume leisure travel that involves flying (Watson, 2014 Watson, C. (Ed.). (2014). Beyond flying: Rethinking air travel in a globally connected world. Cambridge: Green Books.  [Google Scholar] ).\nThe current structures of aviation provision foster the willingness of the public to temporarily suspend their climate concerns when engaging in tourism practices (Barr et al., 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Cohen, Higham, & Reis, 2013 Cohen, S.A., Higham, J., & Reis, A. (2013). Sociological barriers to sustainable discretionary air travel behaviour. Journal of Sustainable Tourism, 21(7), 982–998. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). The aviation industry facilitates a range of spatio-temporal fixes (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ) that invite the consumer to offset their aviation carbon emissions through various schemes (Barr et al., 2010 Barr, S., Shaw, G., Coles, T., & Prillwitz, J. (2010). ‘A holiday is a holiday’: Practicing sustainability, home and away. Journal of Transport Geography, 18(3), 474–481. [Crossref] , [Web of Science ®]   [Google Scholar] ; Gössling et al., 2007 Gössling, S., Broderick, J., Upham, P., Peeters, P., Strasdas, W., Ceron, J.-P., & Dubois, G. (2007). Voluntary carbon offsetting schemes for aviation: Efficiency and credibility. Journal of Sustainable Tourism, 15(3), 223–248. [Taylor & Francis Online]   [Google Scholar] ). Offset schemes encourage concerned travellers to assume responsibility for a profligate industry, by incurring an additional cost to mitigate the externalities of aviation consumption (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] ). In doing so offsetting “… actually plays into the hands of an environmentally destructive industry by allowing it to legitimate its practices while simultaneously absolving itself from responsibility for the environmental destruction from which it profits” (Young et al., 2014 Young, M., Higham, J.E.S., & Reis, A. (2014). Up in the air: A conceptual critique of flying addiction. Annals of Tourism Research, 41, 51–64. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 52). This absolution of individual responsibility is an important factor in the accelerating aviation emissions problem (Creutzig et al., 2015 Creutzig, F., Jochem, P., Edelenbosch, O.Y., Mattauch, L., van Vuuren, D.P., McCollum, D., & Minx, J. (2015). Transport: A roadblock to climate change mitigation? Science, 350(6263), 911–912. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ). It relates closely to what Hall ( 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) refers to as “structures of provision”; the social and institutional structures that underpin unsustainable consumption practices. Expecting the consumer to accept responsibility and respond individually to unsustainable contemporary travel mobilities, in the absence of meaningful industry and policy responses, has proven to be futile (Higham et al., 2016 Higham, J.E.S., Cohen, S.A., Cavaliere, C.T., Reis, A.C., & Finkler, W. (2016). Climate change, tourist air travel and radical emissions reduction. Journal of Cleaner Production, 111, 336–347. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nTechnology myths\nA further vital reason why there is inertia in policy responses to growing aviation emissions is the ongoing industry-led myth, perpetuated by the media and transport policy-makers, that decarbonisation is in progress using radical technological innovation. Gotesky ( 1952 Gotesky, R. (1952). The nature of myth and society. American Anthropologist, 54(4), 523–531. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 530) describes the function of a myth as to “preserve institutions and institutional process”. A “myth” is defined as an idea, story or narrative believed by many people, including decision-makers, even though unfounded or false. As Edelman ( 1998 Edelman, M. (1998). Language, myths and rhetoric. Society, 35(2), 131–139. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 131) reminds us, “[p]olitical language can evoke a set of mythic beliefs in subtle and powerful ways.” Misleading information from the transport and tourism sectors is not new. Gössling and Peeters ( 2007 Gössling, S., Broderick, J., Upham, P., Peeters, P., Strasdas, W., Ceron, J.-P., & Dubois, G. (2007). Voluntary carbon offsetting schemes for aviation: Efficiency and credibility. Journal of Sustainable Tourism, 15(3), 223–248. [Taylor & Francis Online]   [Google Scholar] , p. 402), found four major industry discourses: “air travel is energy efficient; air travel's share of total emissions is negligible; fuel use is constantly minimised and new technology will solve the problem.” All four were deconstructed as being not representative of reality. This section explores the existence and role of “technology myths” in the discourse of sustainable aviation.\nMyths also play a role in other transport modes. Within automobility, for instance, Volkswagen created a green myth around low-emission diesel cars, even though this was largely based on cheating regulations (Franco, Sánchez, German, & Mock, 2014 Franco, V., Sánchez, F.P., German, J., & Mock, P. (2014). Real-world exhaust emissions from modern diesel cars. Berlin: International Council on Clean Transportation Europe.  [Google Scholar] ). So why concentrate on aviation within the domain of sustainable tourism? First because the tourism sector is a central part of passenger aviation, although we cannot be sure exactly how central it is: leisure travel is interlinked with business travel, visiting friends and relatives and other visit motivations. The tourism sector consequently needs to be seen as integral to air transport; the tourism industry cannot be absolved of responsibility for aviation emissions more generally.\nThe second reason is that air transport, though a relatively small part of tourism in terms of total trips (19% in 2010), represents a high share (52% in 2010) of tourism's global emissions, a share that is growing (62% in 2015), which means that aviation's emissions are increasing faster than those of accommodation, car and rail (Gössling & Peeters, 2015 Gössling, S., & Peeters, P. (2015). Assessing tourism's global environmental impact 1900–2050. Journal of Sustainable Tourism, 23(5), 639–659. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). There is a large body of literature showing that future emissions of aviation are growing fast and that this growth is inevitable given transport volume growth projections (Mayor & Tol, 2010 Mayor, K., & Tol, R.S.J. (2010). Scenarios of carbon dioxide emissions from aviation. Global Environmental Change, 20(1), 65–73. [Crossref] , [Web of Science ®]   [Google Scholar] ; Owen, Lee, & Lim, 2010 Owen, B., Lee, D.S., & Lim, L. (2010). Flying into the future: Aviation emissions scenarios to 2050. Environmental Science & Technology, 44(7), 2255–2260. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ; Peeters & Dubois, 2010 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ; Sgouridis, Bonnefoy, & Hansman, 2010 Sgouridis, S., Bonnefoy, P.A., & Hansman, R.J. (2010). Air transportation in a carbon constrained world: Long-term dynamics of policies and strategies for mitigating the carbon footprint of commercial aviation. Transportation Research Part A: Policy and Practice, 45(10), 1077–1091. [Crossref] , [Web of Science ®]   [Google Scholar] ; Vorster, Ungerer, & Volschenk, 2012 Vorster, S., Ungerer, M., & Volschenk, J. (2012). 2050 scenarios for long-haul tourism in the evolving global climate change regime. Sustainability, 5(1), 1–51. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nFrom a recent study (Peeters et al., under review) Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] it was found that the aviation industry creates “technology myths” that may hamper political initiatives that would enforce mitigation on the aviation sector. Industry commonly wields terms such as “efficiency”, “constantly minimised” or “negligible shares” as discursive devices to perpetuate the myth that technological innovation will neutralise the problem of aviation emissions. Technology myths were identified by Peeters et al. ( under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ) for airframe design (laminar flow, composite structures and blended wing body), engines/propulsion (solar flight, electric flight and propfan) and alternative fuels (Jatropha, animal fats, hydrogen and micro-algae). For these 10 technologies media coverage in newspapers was measured over the past two decades and content analysed.\nLaminar flow and composite structures are widely applied already, with the newest types of planes, like the Boeing B787 and Airbus A350, using, for instance, composites in up to 50% of the construction by weight (Lee, 2010 Lee, J.J. (2010). Can we accelerate the improvement of energy efficiency in aircraft systems? Energy Conversion and Management, 51(1), 189–196. [Crossref] , [Web of Science ®]   [Google Scholar] ). But composite structures allow for weight savings of between 14% and 25% (Raymer et al., 2011 Raymer, D.P., Wilson, J., Perkins, H.D., Rizzi, A., Zhang, M., & Puentes, A.R. (2011). Advanced technology subsonic transport study n+3 technologies and design concepts (No. NASA/TM-2011-217130). Cleveland, OH: Glenn Research Center.  [Google Scholar] ) for the structure to which it is applied. So overall weight reduction of the Boeing B787 would be between 7% and 13%. The impact of this weight savings on fuel efficiency depends on how the designer uses the gains, but it may translate in the end to an approximate 5% fuel efficiency improvement (Peeters, 2000 Peeters, P., & Dubois, G. (2010). Tourism travel under climate change mitigation constraints. Journal of Transport Geography, 18(3), 447–457. [Crossref] , [Web of Science ®]   [Google Scholar] ). This Boeing 787 example clearly shows the strength of myths: the impressive 50% share of new materials is hyped in the media to a lay audience and impressed upon politicians, even though composite structures only offer small and evolutionary efficiency improvements, although coupled with improved engine design and materials, the Boeing 787 and Airbus A350 can offer fuel savings of up to 25% per seat mile over the 15–20 year old aircraft that they are replacing. The latter is an impressive figure, but it also means that fares can be reduced, encouraging more people to travel.\nBlended wing body aircraft have a long history of promises but have never emerged and are unlikely to in the near to mid-term future, or even later. Solar flight has recently attracted much attention. Basic physics tells us that it will never play a serious transport role (Noth, 2008 Noth, A. (2008). Design of solar powered airplanes for continuous flight (Unpublished PhD). ETH Zürich.  [Google Scholar] ), but the ICAO ( 2014 ICAO. (2014). 2013 environmental report. Destination green. Montreal: Author.  [Google Scholar] , p. 12) propagates discourses suggesting that it could solve environmental problems: “the Solar Impulse demonstrated that a solar-powered airplane can fly day and night without fuel.” Public interest in electric flight has followed the same strong rise since the mid-2000s. The main issue with electric flight is the requirement for high performance batteries. Current lithium batteries have a power density that falls short of the requirements for full electric flight by a factor of 100 (Kivits, Charles, & Ryan, 2010 Kivits, R., Charles, M.B., & Ryan, N. (2010). A post-carbon aviation future: Airports and the transition to a cleaner aviation sector. Futures, 42, 199–211. [Crossref] , [Web of Science ®]   [Google Scholar] ). This makes the anticipated 2035 realisation of electric flight (The Australian, 2/11/2012), extremely unlikely.\nOf the four alternative fuels, Jatropha, animal fats and hydrogen were hyped by the media between 2008 and 2011 but are now little mentioned, with significant interest only in micro-algae. Still the sector widely cites alternative fuels as promising future replacements for fossil fuels (Air Transport Action Group [ATAG], 2011 Air Transport Action Group (ATAG). (2011). Powering the future of flight. The six easy steps to growing a viable aviation biofuels industry. Geneva: Author.  [Google Scholar] ; Airbus, 2011 Airbus. (2011). Delivering the future. Global market forecast 2011–2030. Paris: Author.  [Google Scholar] ; Boeing, 2012 Boeing. (2012). Current market outlook 2012-2031. Seattle, WA. Boeing Commercial Airplanes.  [Google Scholar] ; IATA, 2012 IATA. (2012). A global approach to reducing aviation emissions. First stop: Carbon-neutral growth from 2020. Montreal: Author.  [Google Scholar] ; ICAO, 2014 ICAO. (2014). 2013 environmental report. Destination green. Montreal: Author.  [Google Scholar] ; WTTC, 2009 WTTC. (2009). Leading the challenge on climate change. London: Author.  [Google Scholar] ). Jatropha faces issues of high water use (Rosillo-Calle, Thrän, Seiffert, & Teelucksingh, 2012 Rosillo-Calle, F., Thrän, D., Seiffert, M., & Teelucksingh, S. (2012). The potential role of biofuels in commercial air transport – biojetfuel. London: IEA Bioenergy Task 40 Sustainable International Bioenergy Trade.  [Google Scholar] ) and adverse socio-economic impacts (Ariza-Montobbio & Lele, 2010 Ariza-Montobbio, P., & Lele, S. (2010). Jatropha plantations for biodiesel in Tamil Nadu, India: Viability, livelihood trade-offs, and latent conflict. Ecological Economics, 70, 189–195. [Crossref] , [Web of Science ®]   [Google Scholar] ); animal fats face technical problems preventing them from being mixed at higher than 20% shares with kerosene (Vera-Morales & Schäfer, 2009 Vera-Morales, M., & Schäfer, A. (2009). Fuel-cycle assessment of alternative aviation fuels. Cambridge: Omega.  [Google Scholar] ); hydrogen is an old but unresolved idea (Brewer, 1991 Brewer, G.D. (1991). Hydrogen aircraft technology. London: CRC Press.  [Google Scholar] ); micro-algae suffer from land-use issues, at least in the European region (Skarka, 2012 Skarka, J. (2012). Microalgae biomass potential in Europe land availability as a key issue. Technikfolgenabschätzung – Theorie und Praxis, 21, 72–79.  [Google Scholar] ), and water use, low or negative life cycle CO2 emission reductions (Quinn & Davis, 2014 Quinn, J.C., & Davis, R. (2014). The potentials and challenges of algae based biofuels: A review of the techno-economic, life cycle, and resource assessment modeling. Bioresource Technology, 184, 444–452. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ), cost and more profitable alternative land uses (Coplin, 2012 Coplin, L.G. (2012). Sustainable development of algal biofuels in the United States. Washington, DC: National Academies Press.  [Google Scholar] ).\nPolicy-makers are required to make decisions that often have long-term effects and are clouded in uncertainties. The quality of such decision-making for the transport sector is significantly degraded by technology myths created by industry and perpetuated through the media (Peeters et al., under review Peeters, P., Higham, J.E.S., Kutzner, D., Cohen, S., & Gössling, S. Are technology myths stalling aviation climate policy? Transportation Research Part D: Transport and Environment, under review. [Web of Science ®]   [Google Scholar] ). Such myths foster political inertia in the development of effective sustainable transport policy measures, discouraging potentially difficult but necessary decisions (Peeters, Gössling, & Lane, 2009 Peeters, P., Gössling, S., & Lane, B. (2009). Moving towards low-carbon tourism. New opportunities for destinations and tour operators. In S. Gössling, C.M. Hall, & D.B. Weaver (Eds.), Sustainable tourism futures. Perspectives on systems, restructuring and innovations (pp. 240–257). New York, NY: Routledge.  [Google Scholar] ).\nTransport taboos\nThe final major theme we discuss in this section of paper is the notion of “taboo” issues in tourism and transport policy-making. The question of why politicians have not acted in more significant ways on climate change mitigation in these sectors, as well as more generally, given that sound evidence of climate change was presented 25 years ago in the Intergovernmental Panel on Climate Change first assessment report (IPCC, 1990 IPCC. (1990). IPCC first assessment report 1990. Cambridge: Cambridge University Press.  [Google Scholar] ), is itself a relatively underexplored area of research. The adverse consequences of climate change for ecosystems and humans have since been confirmed and well documented (IPCC, 2014 IPCC. (2014). Climate change 2014: Synthesis report. Contribution of Working Groups 1, 2, and 3 to the fifth assessment report of the Intergovernmental Panel on Climate Change. Cambridge: Cambridge University Press.  [Google Scholar] ), and climate change is no longer considered a future phenomenon but rather a current and ongoing process, as, for instance, recognised by reinsurers (Munich Re, 2014 Munich Re. (2014). Overall picture of natural catastrophes in 2013 dominated by weather extremes in Europe and Supertyphoon Haiyan. Retrieved 8 January 2015 from http://www.munichre.com/en/media_relations/press_releases/2014/2014_01_07_press_release.aspx   [Google Scholar] ). As an outcome of the IPCC reports, political consensus has been achieved to stabilise greenhouse gas emissions at a level that will prevent global warming from exceeding 2 °C compared to pre-industrial temperatures, an objective confirmed during various Conferences of Parties (UNFCCC, 2014 UNFCCC. (2014). Various documents. Retrieved 17 July 2015 from www.unfccc.int   [Google Scholar] ) and recently recognised at the landmark Paris Agreement (Scott, 2016 Scott, D. (2016). The Paris Climate Change Conference and the tourism industry. Journal of Sustainable Tourism, under review  [Google Scholar] ).\nFor the transport sector, responsible for about 25% of global emissions, the European Commission (EC) has outlined emission reduction goals of -60% by 2050 compared to 1990, with an interim goal of -20% by 2030 compared to 2008 (EC, 2011 European Commission (EC). (2011). White paper: Roadmap to a single European transport area – towards a competitive and resource efficient transport system. COM(2011) 144 final. Brussels: Author.  [Google Scholar] ). These emission reductions are considered in line with the 2 °C guardrail. Yet, while climate policy objectives have been defined for the EU, and while these could also be defined for any country based on national greenhouse gas inventories, there is limited evidence of transport policies that would help to achieve such significant emission reductions, and, controversially, the EC has even outlined that curbing mobility is not considered a viable option (EC, 2011 European Commission (EC). (2011). White paper: Roadmap to a single European transport area – towards a competitive and resource efficient transport system. COM(2011) 144 final. Brussels: Author.  [Google Scholar] ). In countries and regions outside Europe, and specifically for international aviation as a significant sub-sector of tourism, transport-related mitigation policies have thus remained insignificant (Gössling, Scott, & Hall, 2013 Gössling, S., Scott, D., & Hall, C.M. (2013). Challenges of tourism in a low-carbon economy. Wiley Interdisciplinary Reviews: Climate Change, 4(6), 525–538. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nYet, if energy-intense forms of mobility do not decline, it is highly unlikely that absolute reductions in greenhouse gas emissions can be achieved (Anable, Brand, Tran, & Eyre, 2012 Anable, J., Brand, C., Tran, M., & Eyre, N. (2012). Modelling transport energy demand: A socio-technical approach. Energy Policy, 41, 125–138. [Crossref] , [Web of Science ®]   [Google Scholar] ; Banister, 2008 Banister, D. (2008). The sustainable mobility paradigm. Transportation Policy, 15(1), 73–80. [Crossref] , [Web of Science ®]   [Google Scholar] , 2011 Banister, D. (2011). Cities, mobility and climate change. Journal of Transport Geography, 19(6), 1538–1546. [Crossref] , [Web of Science ®]   [Google Scholar] ; Chapman, 2007 Chapman, L. (2007). Transport and climate change: A review. Journal of Transport Geography, 15(5), 354–367. [Crossref] , [Web of Science ®]   [Google Scholar] ; IEA, 2012 IEA. (2012). World energy outlook 2011. Paris: Author.  [Google Scholar] ; Schäfer et al., 2009 Schäfer, A., Heywood, J.B., Jacoby, H.D., & Waitz, I.A., (2009). Transportation in a climate-constrained world. Cambridge, MA: MIT Press.  [Google Scholar] ; UNWTO, UNEP, & WMO, 2008 UNWTO, UNEP, & WMO. (2008). Climate change and tourism: Responding to global challenges. Madrid, Paris & Geneva: UNWTO, UNEP & WMO.  [Google Scholar] ). This has resulted in a situation where there are clear policy goals, and a wide range of market-based, command-and-control and soft policy measures available to achieve these goals (e.g. Friman, Larhult, & Gärling, 2012 Friman, M., Larhult, L., & Gärling, T. (2012). An analysis of soft transport policy measures implemented in Sweden to reduce private car use. Transportation, 40(1), 109–129. [Crossref] , [Web of Science ®]   [Google Scholar] ; OECD & UNEP, 2011 OECD & UNEP. (2011). Climate change and tourism policy in OECD countries. Paris: Author.  [Google Scholar] ; Sterner, 2007 Sterner, T. (2007). Fuel taxes: An important instrument for climate policy. Energy Policy, 35, 3194–3202. [Crossref] , [Web of Science ®]   [Google Scholar] ), but a dearth of implementation, with evidence that only soft policies focusing on voluntary behavioural change appear to be considered politically viable to reduce emissions from transportation. This paradox has been described as an “implementation gap” (Banister & Hickman, 2013 Banister, D., & Hickman, R. (2013). Transport futures: Thinking the unthinkable. Transportation Policy, 29, 283–293. [Crossref] , [Web of Science ®]   [Google Scholar] , p. 292), and led to growing academic interest in barriers to significant climate policy.\nVarious explanations have been provided to explain why governments have been reluctant to implement policies. From a governance viewpoint, Rietveld et al. ( 2005 Rietveld, P., & Stough, R. (Eds.). 2005. Barriers to sustainable transport: Institutions, regulation and sustainability. Abingdon: Spon Press.  [Google Scholar] ) have suggested that institutions rule and structure public and private actions, and that these can be informal, formal, governance-, and resource allocation/employment related. Informal institutions would comprise values, norms, practices, habits and traditions, and are considered conditioners of behaviour (see Schwanen & Lucas, 2011 Schwanen, T., & Lucas, K. (2011). Understanding auto motives. In K. Lucas, E. Blumenberg, & R. Weinberger (Eds.), Auto motives: Understanding car use behaviours. (pp. 3–38). Emerald: Bingley. [Crossref]   [Google Scholar] in the context of automobility). Formal institutions include “codified statutes, constitutions, provisions, laws, regulations, and high level administrative orders” (Rietveld et al., 2005 Rietveld, P., & Stough, R. (Eds.). 2005. Barriers to sustainable transport: Institutions, regulation and sustainability. Abingdon: Spon Press.  [Google Scholar] , p. 3). Governance institutions are a third type of institution focused on rules, including laws, regulations and policy directives, such as planning and zoning issues, or transactions involving actors and agents. Finally, resource allocation refers to government agencies, firms and non-profit associations allocating financial resources. These four categories can be used to identify and address barriers from various viewpoints, including, for instance, the notion that transport planning cannot be questioned, as transportation is important for society and economic growth. This has, for instance, been discussed by Miciukiewicz and Vigar ( 2012 Miciukiewicz, K., & Vigar, G. (2012). Mobility and social cohesion in the splintered city: Challenging technocentric transport research and policy-making practices. Urban Studies, 49(9), 1941–1957. [Crossref] , [Web of Science ®]   [Google Scholar] ) in terms of technological fixation among transport researchers and subsequent technocentric policy-making. In a similar vein, Hutton ( 2013 Hutton, B. (2013). Planning sustainable transport. London: Routledge.  [Google Scholar] ) describes how the turn from meeting predicted transport growth to managing transport demand has only recently been considered in UK transport policy. Ultimately, “barriers” thus often resemble embedded beliefs of ecological modernisation, i.e. the assumption that transport growth can be balanced environmentally, based on technological progress, as also evident in UNEP's ( 2011 UNEP. (2011). Towards a green economy: Pathways to sustainable development and poverty eradication. Retrieved 29 September 2015 from www.unep.org/greeneconomy   [Google Scholar] ) Green Growth focus, which may be seen as another ecological modernisation paradigm without real-world implications for emission growth (Hall, 2009 Hall, C.M. (2009). Degrowing tourism: Décroissance, sustainable consumption and steady-state tourism. Anatolia: An International Journal of Tourism and Hospitality Research, 20(1), 46–61. [Taylor & Francis Online]   [Google Scholar] , 2013 Hall, C.M. (2013). Framing behavioural approaches to understanding and governing sustainable tourism consumption: Beyond neoliberalism, ‘nudging’ and ‘green growth’? Journal of Sustainable Tourism, 21(7), 1091–1109. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , 2015 Hall, C.M. (2015). Economic greenwash: On the absurdity of tourism and green growth. In M.V. Reddy & K. Wilkes (Eds.), Tourism in the green economy. (pp. 339–358). London: Earthscan by Routledge.  [Google Scholar] ).\nNew understandings of the reasons for inaction on climate change in transport contexts are thus required: “barriers” were discussed with regard to their cognitive and affective dimensions during the Freiburg 2014 workshop, where they were framed as “taboos” (Gössling & Cohen, 2014 Gössling, S., & Cohen, S. (2014). Why sustainable transport policies will fail: European Union climate policy in the light of transport taboos. Journal of Transport Geography, 39, 197–207. [Crossref] , [Web of Science ®]   [Google Scholar] ). Taboos are different from barriers of implementation, because they cannot be addressed politically without considerable (political) danger to the person taking up a given issue. To touch a taboo implies to violate an existing norm, i.e. a situation that is usually in the interest of powerful individuals, (lobby) organisations, or the broader public or community. A “transport taboo” thus describes an issue that cannot be raised without risks, possibly jeopardising the political future of any person raising the taboo. A wide range of transport taboos that politicians are unwilling to touch has been identified, such as the watering down of transport policy by lobbyism, the skewed share of transport emissions contributed by higher income classes and the broader societal glamorisation of high-energy transport consumption (Cohen & Gössling, 2015 Cohen, S.A., & Gössling, S. (2015). A darker side of hypermobility. Environment and Planning A, 47, 1661–1679. [Crossref] , [Web of Science ®]   [Google Scholar] ; for further details on transport taboos see Gössling & Cohen, 2014 Gössling, S., & Cohen, S. (2014). Why sustainable transport policies will fail: European Union climate policy in the light of transport taboos. Journal of Transport Geography, 39, 197–207. [Crossref] , [Web of Science ®]   [Google Scholar] ).\nTransport taboos are consequently issues that would appear obvious, as some solutions are ready at hand, demanding political action; yet, they are characterised by silence. A further example may be the 20 years of OECD reports recommending to remove fossil fuel subsidies, and to introduce carbon pricing ( 1991 OECD. (1991). Energy prices, taxes and carbon dioxide emissions (OECD economic studies 17). Paris: Author.  [Google Scholar] , 1999 OECD. (1999). Project on environmentally sustainable transport (EST) (Report NV/EPOC/PPC/T(99)3/FINAL/REV1). Paris: Author.  [Google Scholar] , 2008, 2015 OECD. (2015). OECD companion to the inventory of support measures for fossil fuels 2015. Retrieved 12 November 2015 from http://www.keepeek.com/Digital-Asset-Management/oecd/energy/oecd-companion-to-the-inventory-of-support-measures-for-fossil-fuels-2015_9789264239616-en#page1   [Google Scholar] ). However, the issue remains politically untouched, because this would lead to outrage by industry associations, who are powerful agents in public discourse. Yet, overcoming taboos is essential if more sustainable tourism and transport policies are to be implemented, specifically regarding climate change. At the very least, this would require political parties to stop using public sentiment to undermine sustainable transport policy initiatives by political opponents in order to gain votes on less popular measures related to climate change mitigation.\nThe papers in this special issue\nThis special issue presents nine further papers that explore avenues for behaviour change by various stakeholders in tourism and transport in order to bridge the science–policy gap in sustainable mobility. The contributions cover a wide spectrum of interests and stakeholders, and connect in various ways to the themes discussed above, notably socio-technical factors and transport taboos.\nThe first three papers investigate the role of researchers in the sustainable mobility debate, and their capacities and shortcomings to contribute to behaviour change. Hall ( in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) does this by employing social marketing themes. He questions both the theoretical knowledge base of sustainable tourism and the positioning of sustainability within the wider (tourism) literature, noting that, despite a growing body of research on sustainable tourism and mobilities, concern about these topics in tourism research is still minor, particularly in popular areas such as achieving growth in visitor numbers and expenditure. Substantive change is not evident in most destinations, or among organisations that have adopted sustainable tourism. As a way forward, Hall discusses the need for more advocacy- and participatory-based approaches so that scientists can better communicate with policy-makers and work collaboratively/co-productively with them and other upstream stakeholders. Downstream and (more activist and interventionist) upstream social marketing to the public, taking in the lessons learned from other disciplines and debates (such as that on anti-smoking), may (re)glamorise/make fashionable forms of more sustainable tourism or encourage more conventional but low transport intensity local tourism.\nMelissen and Koens ( in press Melissen, F., & Koens, K. (in press). Adding researchers' behaviour to the research agenda: Bridging the science–policy gap in sustainable tourism mobility. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1071384 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) echo Hall, arguing that researchers should not only focus on understanding the structures behind tourist behaviour, but also on how to mobilise policy and business stakeholders to contribute to the sustainable development of tourism. They find several factors or tensions (Jones, Jones, & Walsh, 2008 Jones, N., Jones, H., & Walsh, C. (2008). Political science? Strengthening science-policy dialogue in developing countries. London: Overseas Development Institute.  [Google Scholar] ) hindering researcher behaviour change towards bridging the science–policy gap in sustainable tourism mobility, and make a case for adding researchers' behaviour to the corresponding research agenda. Researchers may need to position themselves closer to the policy arena, without politicising science or moving from engaged to activist research.\nMounting sustainable tourism and transport advocacy will also lead to more attention to the environmental sustainability imperatives of researchers themselves, and the institutions for which they work. Thus the paper by Hopkins, Higham, Tapp, and Duncan ( in press Hopkins, D., Higham, J., Tapp, S., & Duncan, T. (in press). Academic mobility in the Anthropocene era: A comparative study of university policy at three New Zealand institutions. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1071383 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) in this issue links to the transport taboo of questioning academic mobility and associated carbon emissions. Set in a New Zealand university context it investigates the perceived lock-in of academic performance being dependent on, or equal to, international mobility. This can be linked back to the pioneering work of the late Karl Georg Høyer (Høyer, 2000 Høyer, K.G. (2000). Sustainable tourism – or sustainable mobility? Journal of Sustainable Tourism, 8(2), 147–161. [Taylor & Francis Online]   [Google Scholar] ; Høyer & Næss, 2001 Høyer, K.G., & Næss, P. (2001). Conference tourism: A problem for the environment as well as for research? Journal of Sustainable Tourism, 9(6), 451–470. [Taylor & Francis Online]   [Google Scholar] ). The authors find academic travel to be embedded in university policy, for example through international partnerships, the need to present research at international conferences and recruitment processes. Acknowledging New Zealand's particular geographical location, they still recommend that academic institutions consider and address the carbon emissions related to academic mobility, and to integrate sustainability more systematically into university (travel) policy.\nAnother highly mobile group possibly caught in a (socio-cultural) lock-in of flight-dependent practices is that of the younger generation in many western countries taking a gap year. Luzecka ( in press Luzecka, P. (in press). “Take a gap year!” A social practice perspective on air travel and potential transitions towards sustainable tourism mobility. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1115513 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) explores ways in which conventions related to “appropriate” gap year destinations are developed, sustained and reproduced. Numerous social mechanisms were found to facilitate overseas travel in the context of gap years, including a shared perception of difference and physical distance, which links personal development to the challenges of distant countries. This paper highlights the fact that some long-haul destinations are actually easier or cheaper to travel to than more nearby destinations. Luzecka warns that widening participation in gap year travel may further the normalisation – and psychological lock-in – of long-haul travel. The length of gap years would make them suitable for slow, and potentially more sustainable, travel, but for such a sustainable mobility transition, the socio-cultural forces that shape current gap year practices need to be taken into account.\nThe two following papers acknowledge the reality of consumers not accepting responsibility and responding to unsustainable travel mobilities, and seek ways to influence consumer decision-making through the provision of carbon information with travel products. Araña and León ( in press Araña, J.E., & León, C.J. (in press). Are tourists animal spirits? Evidence from a field experiment exploring the use of non-market based interventions advocating sustainable tourism. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1101128 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) experiment with the role of emotions and time pressure in decision-making, hypothesising that many traveller decisions are of an experiential nature. In a Spanish social tourism programme setting, they carried out a field experiment in which they manipulate the choice context of travellers, when deciding where to travel. Travel plans involved different levels of CO2 emissions, contexts about the emotional state, and decision time. Araña and León find that emotional states and the decision context can indeed affect the sustainability of travel choices. These findings have several implications for pro-environmental behaviour policies and campaigns in tourism and the effectiveness of tax incentives. Subjects showing more empathy with future generations are more likely to accept low-carbon travel options.\nEijgelaar, Nawijn, Barten, Okuhn, and Dijkstra ( in press Eijgelaar, E., Nawijn, J., Barten, C., Okuhn, L., & Dijkstra, L. (in press). Consumer attitudes and preferences on holiday carbon footprint information in the Netherlands. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1101129 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) investigate carbon labels, as a soft measure towards more sustainable travel choices that is more likely to receive industry acceptance than direct volume-reducing measures. Their case is linked to the desire of Dutch tour operators to offer such a label. Hence, they explore preferences for carbon label design in tourism. The authors tested label designs in a number of consecutive research phases under Dutch consumers. They find a number of preconditions for a tourism carbon label: it should be simple in design and connect to existing well-known EU labels for energy efficiency. But at the same time they note that sustainability is still of low priority during holiday decision-making.\nAn interesting case in science–policy interaction is Antarctica, as the continent is not controlled by a state, but through a 29-party governance regime, the Antarctic Treaty System. As little progress was made on tourism issues through the Treaty's meetings, the International Association of Antarctica Tour Operators filled this gap by self-regulating Antarctic tourism. However, their capability to self-regulate is increasingly questioned. Student, Lamers, and Amelung ( in press Student, J., Lamers, M., & Amelung, B. (in press). Towards a tipping point? Exploring the capacity to self-regulate Antarctic tourism using agent-based modelling. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1107079 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) apply agent-based modelling (ABM) to identify the challenges for the self-regulation of this carbon-intensive form of tourism. They find a number of potentially destabilising factors connected to likely future tourism development, whereby optimum group size and membership cost are crucial. More importantly, they stress the strength of ABM as a method to safely experiment with uncertainties in Antarctica, and help to provide insights for an environmentally optimised application in (sustainable Antarctic tourism) policy.\nThe final two papers investigate sustainable transport policy-making and practices at destinations. This local/regional level work reveals key aspects of the realpolitik of policy-making in ways not open to most researchers at national or international levels. Scuttari, Volgger, and Pechlaner ( in press Scuttari, A., Volgger,M., & Pechlaner, H. (in press). Transition management towards sustainable mobility in alpine destinations: Realities and realpolitik in Italy's South Tyrol region. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.XXXXXX [Crossref] , [Web of Science ®]   [Google Scholar] ) seek ways for governing the transition towards more sustainable tourism mobility by applying a system approach. Their research in a South Tyrolean context indicates that the transition towards more sustainable transport solutions is complex and requires both public and private sector unpredictability and risk aversion to be taken into account. Scuttari et al. identify three conditions relevant to performing the transition, each one linked to a different sub-system (socio-ecological, socio-technical and governance). The transition to sustainable solutions is a complex task that can only be successful when all sub-systems – tourism, transport, governance and social-ecological – interact in informal or formal partnership. To be successful, three conditions were identified: (1) an improved understanding of what sustainable transport entails, (2) adoption of the best available technology and (3) courage and leadership to take risks with new solutions without knowing what they will bring with absolute certainty. The latter means that science should provide a better understanding of how to mitigate risk aversion or even better, how to shift the focus of risk aversion towards avoiding the risks of unsustainable development and climate change, as opposed to the risks associated with introducing, managing and using novel transport systems.\nFinally, Stanford and Guiver (in press) Stanford, D., & Guiver, J. (in press). Driving pro-environmental change in tourist destinations: Encouraging sustainable travel in national parks via partnership project creation and implementation. Journal of Sustainable Tourism, DOI: 10.1080.09669582.2015.1122018 [Web of Science ®]   [Google Scholar] explore public–private partnership led projects providing alternatives to car travel in three UK national parks as mechanisms of modal shift and pro-environmental change. They identify a number of success factors and provide practical advice on understanding and guiding future multi-partnership pro-environmental change processes in complex networks. As in the previous paper, strong local governance structures, awareness creating, trust and learning are important, and the effective communication of benefits to stakeholders appeared most significant.\nConclusions and research agenda\nThis paper has discussed three areas crucial for understanding why, despite clear scientific evidence about the growing environmental impacts of tourism transport, there is large-scale inertia in structural transitions and a lack of political willpower to enact meaningful policy change. These included the importance of addressing socio-technical factors, the barriers posed by placing faith in “technology myths” and the need to overcome “transport taboos” in policy-making. These areas shed significant light on why a science–policy gap in sustainable mobility exists, and on the issues that must be overcome if this gap is to be bridged. The societal challenge of transitioning the tourism and transport sectors to a sustainable emissions path must not be devolved entirely to the public and the marketplace, as suggested by neoliberal values. It is vital that both governments and the tourism and transport industries take a more cautious approach to the technological optimism that is fostering policy inertia: technological innovation alone will not save the day anytime soon. They must invest more in research, in operational procedures and in encouraging the development of and marketing alternatives to air travel. But most importantly, policy-makers must take a more open approach to implementing sustainable transport policies that affect the structures of provision; they will need to be lobbied to touch issues that may be politically risky, but nonetheless have been shown in other contexts to be successful in fostering desirable sustainable transport outcomes.\nBuilding on these insights, the Freiburg 2016 workshop will consequently focus on desirable transport futures, i.e. visions of desirable sustainable transport systems that have the potential to be actively taken up by wide cross sections of society (c.f. Banister & Hickman, 2013 Banister, D., & Hickman, R. (2013). Transport futures: Thinking the unthinkable. Transportation Policy, 29, 283–293. [Crossref] , [Web of Science ®]   [Google Scholar] ). A starting point for this is the analysis of sustainable transport transitions that are now underway, and the analysis of the structural, political, institutional and social/psychological factors underlying those transitions. The e-bike revolution is one example of societal change involving a low-carbon technological innovation, with uptake and adoption motivated by convenience, speed, health and cost. Many cities in Europe have re-discovered the bicycle as a transport mode with diverse benefits, and there now exists widespread and growing demand for infrastructures that facilitate cycling and other active forms of transport (Gössling & Choi, 2015 Gössling, S., & Choi, A.S. (2015). Transport transitions in Copenhagen: Comparing the cost of cars and bicycles. Ecological Economics, 113, 106–113. [Crossref] , [Web of Science ®]   [Google Scholar] ; Pucher & Buehler, 2012 Pucher, J., & Buehler, R. (2012). City cycling. Cambridge, MA: MIT Press.  [Google Scholar] ). Cycling cities have become a desirable transport future but the health and cost benefits of cycling demand that issues of cyclist safety are addressed in infrastructural provision. The rise of car sharing systems, in place of car ownership, is another timely example of a mobility transition, but is not without rebound effects, as the media suggests that it has cannibalisation effects at the expense of public transport (Schwarz, 2015 Schwarz, K. (2015, December 11). Carsharing: Wie umweltschädlich sind Car2go und Drivenow wirklich? [Carsharing: How environmentally harmful are Car2go and Drivenow actually?]. The Huffington Post (German Edition). Retrieved 1 December 2015 from http://www.huffingtonpost.de/2015/02/13/carsharing-klima-umfrage-wirtschaftswoche_n_6675908.html   [Google Scholar] ).\nIn contrast to desirable futures, while flying is highly desirable for many, continued growth in air travel on a global scale is incompatible with a sustainable transport future, and it is here that the need for urgent transition is now widely accepted. However, before entertaining alternatives to the current unsustainable transport system, it is essential to know what desirable transport futures may look like. The critical analysis of mobility transitions, including barriers confronting the achievement of desirable transport futures, is needed. A concerted research effort in the following key areas is consequently required:\nDesired transport\nFuture visions must be built upon desired transport systems. Critical examination is needed of spontaneous market uptake of desired new transport systems and their rebound effects. How have e-bike, car sharing, high-speed rail and successful public transport systems emerged in certain societies? What factors have acted as facilitators of change and how were barriers overcome? How important are individuals, i.e. specific people, in initiating change? What were the key roles of stakeholders and were low-carbon transitions an objective from the outset or a coincidental outcome? What lessons can be learned from these revolutions and what wider roles may such successes play in developing towards low-carbon transport and tourism?\nThe role of fashion\nMany trends in tourism and travel are fashion driven. Certain destinations can rise and fall substantially in a very short time. In the Netherlands long-haul travel grew rapidly until 2008 when it stabilised and started to decline. What factors influence significant changes in established patterns of consumption? Why has “environmental consciousness”, with the exception of European car purchasing, proved largely ineffective in driving low-carbon transport transitions? What potential do social marketing, celebrity endorsement and role model advocacy offer, and how can the effectiveness of these strategies be maximised? What other strategies may exist to influence and encourage the fashionability of more sustainable forms of tourism (e.g. caravanning, train journeys, “loca-tourism”, slow tourism and staycationing)? How can industry, government, the media and public organisations engage in such processes, and what in particular is the role of science and researchers?\nEconomic issues\nThe economic arguments for growth in aviation are well established, even though many assessments appear to remain partial. But what are the economic arguments that support the development of sustainable transport systems? What economic growth scenarios may be associated with low-carbon transitions to rail and electric vehicle fleets, and the new infrastructures required to facilitate active transport modes? What do economic models predict for the redistribution of travel flows under low-carbon transport scenarios? Contributions to a more complete understanding of the economics of low-carbon transport scenarios are critically important to the shift towards desired transport futures.\nPublic health and well-being\nCritical issues arise when contemplating how the sustainable transportation agenda is coupled with questions of public health and well-being. Policy outcomes driven by a public health and well-being agenda may have the potential for achieving significant environmental benefits. What potential do desirable transport futures offer to overcome personal well-being and public health risks, such as the negative health dimensions of frequent flying (Cohen & Gössling, 2015) Cohen, S.A., & Gössling, S. (2015). A darker side of hypermobility. Environment and Planning A, 47, 1661–1679. [Crossref] , [Web of Science ®]   [Google Scholar] , or the risk of pandemic associated with long-haul flights? And where mobility transitions are already in progress, can, for instance, the rising public health costs of serious injuries and deaths of cyclists be mitigated by dedicated cycle infrastructure?\nIssues of equity and ethics\nThere is a need to move beyond the Eurocentrism (c.f. Cohen & Cohen, 2015 Cohen, E., & Cohen, S.A. (2015). Beyond Eurocentrism in tourism: A paradigm shift to mobilities. Tourism Recreation Research, 40(2), 157–168. [Taylor & Francis Online]   [Google Scholar] ) that has framed debate around these issues. While the West has contributed disproportionately to the environmental crisis, emissions of unsustainable transportation are globally dispersed. Insights that are theoretically, methodologically and practically informed are required to understand sustainable transportation issues as they apply to emerging world regions (e.g. Dillimono & Dickinson, 2015 Dillimono, H.D., & Dickinson, J.E. (2015). Travel, tourism, climate change, and behavioral change: Travelers' perspectives from a developing country, Nigeria. Journal of Sustainable Tourism, 23(3), 437–454. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] on Nigerian perspectives). Growth from the middle classes in parts of Asia, Latin America, South America and Africa is also driving up global transport emissions. What equity issues arise in association with the transition from old to new transport systems? What ethical issues arise with the growth of unsustainable transportation in less developed countries? How will the continuing but declining emission footprint be distributed between countries and sectors globally? What will be the impacts of emission trading, taxation regimes, subsidies and infrastructure planning, and how will they vary between regions in light of the recent Paris Agreement?\nAdvocacy- and participatory-based approaches\nThe need exists for advocacy- and participatory-based approaches (see Bramwell, Higham, Lane, & Miller, 2016 Bramwell, B., Higham, J., Lane, B., & Miller, G. (2016). Advocacy or neutrality? Disseminating research findings and driving change toward sustainable tourism in a fast changing world. Journal of Sustainable Tourism, 24(1), 1–7. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Hall, in press Hall, C.M. (in press). Intervening in academic interventions: Using the lens of social marketing to examine the potential for successful sustainable tourism behavioural change. Journal of Sustainable Tourism, DOI: 10.1080/09669582.2015.1088861. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) to enable more effective communication with policy communities, and to facilitate collaboration and co-production with policy-makers and other upstream stakeholders. What opportunities exist to implement new or underutilised methods such as simulation, serious gaming and in-depth multi-stakeholder approaches? What potential do new approaches offer to communicate the science of climate change, and internalise the challenges inherent in responding to climate change? How can new approaches serve to highlight the contribution of travel in global GHG emissions, the inherent difficulties in responding to unsustainable transportation, and potential pathways to desirable transport futures? These key areas form the basis for a continued research agenda aimed at transitioning the tourism and transport sectors to a sustainable emissions path.\nAcknowledgments\nThe convenors of the Freiburg 2014 workshop (1–4 July 2014) gratefully acknowledge the members of its Scientific Advisory Board, all of whom contributed to the success of the workshop: Dr Stewart Barr (University of Exeter, UK), Dr Jo Guiver (University of Central Lancashire, UK), Professor Michael Hall (University of Canterbury, NZ), Dr Julia Hibbert (Bournemouth University, UK), Professor Daniel Scott (University of Waterloo, Canada) and Professor David Banister (TSI, University of Oxford, UK).\nDisclosure statement\nScott A. Cohen is a reader at the University of Surrey (United Kingdom).\nJames Higham\nJames Higham is a professor at the Department of Tourism, University of Otago (New Zealand).\nStefan Gössling\nStefan Gössling is a professor at Lund University and Linnaeus University (Sweden). He is also research coordinator at the Western Norway Research Institute's Research Centre for Sustainable Tourism.\nPaul Peeters\nPaul Peeters is an associate professor at NHTV Breda University of Applied Sciences (The Netherlands).\nEke Eijgelaar\nEke Eijgelaar is a researcher at the Centre for Sustainable Tourism and Transport at NHTV Breda University for Applied Sciences (The Netherlands).\nArticle Metrics\n""","0.24767348","""http://www.tandfonline.com/doi/full/10.1080/09669582.2015.1136637""","[-0.589514,51.242722]"
"""University_of_Birmingham""","""Employee satisfaction and use of flexible working arrangementsWork, Employment and Society - Daniel Wheatley, 2017""","""PDF\nAbstract\nThis article considers the impact of flexible working arrangements (FWAs), using the British Household Panel Survey and Understanding Society, 2001–10/11. Results of panel logit, ANCOVA and change-score analysis are indicative of positive impacts from use of a number of FWAs, including homeworking having positive effects for men and women on job and leisure satisfaction. However, findings reveal gaps in availability and use of FWAs, and highlight the gendered nature of flexible employment. Flexi-time, the most common FWA among men, has positive effects as it facilitates management of household responsibilities while maintaining full-time employment. Part-time and homeworking are also positive, consistent with men using FWAs with a greater degree of choice. Women more often are constrained in their use of FWAs, often into working reduced hours. Consequently, FWAs have negative impacts for some women, on job (part-time when used for extended periods, flexi-time), leisure (job-share, flexi-time) and life satisfaction (job-share).\nReferences\nSection:\nAtkinson C, Hall L (2009) The role of gender in varying forms of flexible working. Gender, Work and Organization 16(6): 650–66. Google Scholar Crossref\nBIS (Department for Business Innovation & Skills) (2011) Flexible working and work–life balance. Available at: http://www.bis.gov.uk/ (accessed December 2011). Google Scholar\nBranine M (2004) Job sharing and equal opportunities under the new public management in local authorities. International Journal of Public Sector Management 17(2): 136–52. Google Scholar Crossref\nBritish Household Panel Survey (BHPS) [computer file], ESRC Research Centre on Micro-social Change. Colchester: The Data Archive [distributor] (2010) Data files and associated documentation. Google Scholar\nBrown A, Charlwood A, Spencer D (2012) Not all that it might seem: why job satisfaction is worth studying despite it being a poor summary measure of job quality. Work, Employment and Society 26(6): 1007–18. Google Scholar Link\nBulger C, Matthews R, Hoffman M (2007) Work and personal life boundary management: boundary strength, work/personal life balance, and the segmentation–integration continuum. Journal of Occupational Health Psychology 12(4): 365–75. Google Scholar Crossref , Medline\nClark SC (2000) Work/family border theory: a new theory of work/family balance. Human Relations 53(6): 747–70. Google Scholar Link\nConnolly S, Gregory M (2008) Moving down: women’s part-time work and occupational change in Britain 1991–2001. Economic Journal 118(526): F52–76. Google Scholar Crossref\nDeakin S, Morris G (2012) Labour Law, 6th Edition. Oxford: Hart. Google Scholar\nDelsen L (1998) When do men work part-time. In: O’Reilly J, Fagan C (eds) Part-Time Prospects: An International Comparison of Part-Time Work in Europe. London: Routledge, 57–76. Google Scholar\nDolan P, Peasgood T, White M (2008) Do we really know what makes us happy? A review of the economic literature on the factors associated with subjective wellbeing. Journal of Economic Psychology 29(1): 94–122. Google Scholar Crossref\nDurbin S, Tomlinson J (2010) Female part-time managers: networks and career mobility. Work, Employment and Society 24(4): 621–40. Google Scholar Link\nErgeneli A, Ilsev A, Karapınar PB (2010) Work–family conflict and job satisfaction relationship: the roles of gender and interpretive habits. Gender, Work and Organization 17(6): 679–95. Google Scholar Crossref\nFagan C, Walthery P (2011) Individual working-time adjustments between full-time and part-time working in European firms. Social Politics 18(2): 269–99. Google Scholar Crossref\nFagan C, Lyonette C, Smith M, Saldaña-Tejeda A (2012) The influence of working time arrangements on work-life integration or ‘balance’: a review of the international evidence. Conditions of Work and Employment No. 32. Geneva: ILO. Google Scholar\nFleetwood S (2007) Why work-life balance now? International Journal of Human Resource Management 18(3): 387–400. Google Scholar Crossref\nFoster D (2007) Legal obligation or personal lottery? Employee experiences of disability and the negotiation of adjustments in the public sector workplace. Work, Employment and Society 21(1): 67–84. Google Scholar Link\nGambles R, Lewis S, Rapoport R (2006) The Myth of Work-Life Balance: The Challenge of Our Time for Men, Women and Societies. Hoboken, NJ: Wiley. Google Scholar Crossref\nGarcia I, Molina A, Navarro M (2007) How satisfied are spouses with their leisure time? Evidence from Europe. Journal of Family and Economic Issues 28: 546–65. Google Scholar Crossref\nGolden L (2009) Flexible daily work schedules in U.S. jobs: formal introductions needed? Industrial Relations: A Journal of Economy and Society 48(1): 27–54. Google Scholar Crossref\nGreen A, Livanos I (2015) Involuntary non-standard employment and the economic crisis: regional insights from the UK. Regional Studies 49(7): 1223–35. Google Scholar Crossref\nGregory A, Milner S (2009) Trade unions and work-life balance: changing times in France and the UK? British Journal of Industrial Relations 47(1): 122–46. Google Scholar Crossref\nGregory M, Connolly S (2008) Feature: the price of reconciliation: part-time work, families and women’s satisfaction. The Economic Journal 118(526): F1–7. Google Scholar Crossref\nGuillaume C, Pochic S (2009) What would you sacrifice? Access to top management and the work–life balance. Gender, Work and Organization 16(1): 14–36. Google Scholar Crossref\nHall L, Atkinson C (2006) Improving working lives: flexible working and the role of employee control. Employee Relations 28(4): 374–86. Google Scholar Crossref\nKelliher C, Anderson D (2008) For better or for worse? An analysis of how flexible working practices influence employees’ perceptions of job quality. The International Journal of Human Resource Management 19(3): 419–31. Google Scholar Crossref\nKhattab N, Fenton S (2009) What makes young adults happy? Employment and non-work as determinants of life satisfaction. Sociology 43(1): 11–26. Google Scholar Link\nLee B, DeVoe S (2012) Flextime and profitability. Industrial Relations: A Journal of Economy and Society 51(2): 298–316. Google Scholar Crossref\nLewis J, Campbell M (2008) What’s in a name? ‘Work and family’ or ‘work and life’ balance policies in the UK since 1997 and the implications for the pursuit of gender equality. Social Policy and Administration 42(5): 524–41. Google Scholar Crossref\nLewis J, Plomien A (2009) ‘Flexicurity’ as a policy strategy: the implications for gender equality. Economy and Society 38(3): 433–59. Google Scholar Crossref\nLewis S, Humbert L (2010) Discourse or reality? Work life balance, flexible working policies and the gendered organization. Equality, Diversity and Inclusion: An International Journal 29(3): 239–54. Google Scholar Crossref\nLim C, Putnam R (2010) Religion, social networks and life satisfaction. American Sociological Review 75(6): 914–33. Google Scholar Link\nLucas R (2007) Long-term disability is associated with lasting changes in subjective well-being: Evidence from two nationally representative longitudinal studies. Journal of Personality and Social Psychology 92(4): 717–30. Google Scholar Crossref , Medline\nMcDonald P, Bradley L, Brown K (2009) ‘Full-time is a given here’: part-time versus full-time job quality. British Journal of Management 20(2): 143–57. Google Scholar Crossref\nMcNamara T, Pitt-Catsouphes M, Brown M, Matz-Costa C (2012) Access to and utilization of flexible work options. Industrial Relations: A Journal of Economy and Society 51(4): 936–65. Google Scholar Crossref\nMorganson VJ, Major DA, Oborn KL, Verive JM, Heelan MP (2010) Comparing telework locations and traditional work arrangements. Journal of Managerial Psychology 25(6): 578–95. Google Scholar Crossref\nPhilp B, Wheatley D (2011) Time scarcity and the dual career household: competing perspectives. American Journal of Economics and Sociology 70(3): 587–614. Google Scholar Crossref\nPlantenga J, Remery C (2010) Flexible Working Time Arrangements and Gender Equality: A Comparative Review of Thirty European Countries. Luxembourg: Official Publications of the European Communities. Google Scholar\nPoelmans S, Beham B (2008) The moment of truth: conceptualizing managerial work–life policy allowance decisions. Journal of Occupational and Organizational Psychology 81(3): 393–410. Google Scholar Crossref\nRaess D, Burgoon B (2015) Flexible work and immigration in Europe. British Journal of Industrial Relations 53(1): 94–111. Google Scholar Crossref\nRussell H, O’Connell P, McGinnity F (2009) The impact of flexible working arrangements on work–life conflict and work pressure in Ireland. Gender, Work and Organization 16(1): 73–97. Google Scholar Crossref\nSmith A, Elliott F (2012) The demands and challenges of being a retail store manager: ‘handcuffed to the front doors’. Work, Employment and Society 26(4): 676–84. Google Scholar Link\nSpencer DA (2009) The Political Economy of Work. London: Routledge. Google Scholar\nStavrou E (2005) Flexible work bundles and organizational competitiveness: a cross-national study of the European work context. Journal of Organizational Behavior 26: 923–47. Google Scholar Crossref\nSullivan C, Smithson J (2007) Perspectives of homeworkers and their partners on working flexibility and gender equity. International Journal of Human Resource Management 18(3): 448–61. Google Scholar Crossref\nTeasdale N (2013) Fragmented sisters? The implications of flexible working policies for professional women’s workplace relationships. Gender, Work and Organization 20(4): 397–412. Google Scholar Crossref\nTietze S, Musson G, Scurry T (2009) Homebased work: a review of research into themes, directions and implications. Personnel Review 38(6): 585–604. Google Scholar Crossref\nTomlinson J (2006) Routes to part-time management in UK service sector organizations: implications for women’s skills, flexibility and progression. Gender, Work and Organization 13(6): 585–605. Google Scholar Crossref\nUniversity of Essex. Institute for Social and Economic Research and National Centre for Social Research, Understanding Society: Wave 1, 2009-2010 and Wave 2, Year 1 (Interim Release), 2010 [computer file]. 3rd Edition. Colchester, Essex: UK Data Archive [distributor], February 2012. SN: 6614, http://dx.doi.org/10.5255/UKDA-SN-6614-3 (accessed April 2012). Google Scholar\nVan Wanrooy B, Bewley H, Bryson A, Forth J, Freeth S, Stokes L, , et al. (2013) The 2011 Workplace Employment Relations Study: First Findings. Available at: https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/336651/bis-14-1008-WERS-first-findings-report-fourth-edition-july-2014.pdf (accessed August 2014). Google Scholar\nWheatley D (2012a) Good to be home? Time-use and satisfaction levels among home-based tele-workers. New Technology, Work and Employment 27(3): 224–41. Google Scholar Crossref\nWheatley D (2012b) Work-life balance, travel-to-work and the dual career household. Personnel Review 41(6): 813–31. Google Scholar Crossref\nWight V, Raley S (2009) When home becomes work: work and family time among workers at home. Social Indicators Research 93(1): 197–202. Google Scholar Crossref\nVol 31, Issue 4, 2017\nWork–life balance and flexible working\nFlexible working: evidence from the extant literature\nEmpirical analysis: FWAs and employee satisfaction\nDiscussion and conclusion\n""","0.2645809","""http://journals.sagepub.com/doi/10.1177/0950017016631447""","[-1.933663,52.454008]"
"""University_of_Aberdeen""","""Development of a Map-Matching Algorithm for Rural Passenger Information Systems through Mobile Phones and Crowd Sourcing | Journal of Computing in Civil Engineering | Vol 27, No 6""","""Journal of Computing in Civil Engineering\nDownloaded 384 times\nTechnical Papers\nDevelopment of a Map-Matching Algorithm for Rural Passenger Information Systems through Mobile Phones and Crowd Sourcing\nShare\nAbstract\nIn this research, a passenger-centric passenger information system is proposed, which uses crowd sourcing and mobile phones so that passengers are not only information consumers but are also providers of information to the system. Passengers can allow the system to track their location through their smart phones when they are traveling on public transportation; this will compensate for the lack of a vehicle tracking system in public transportation, particularly in nonurban areas. Map-matching (MM) algorithms integrate location data (i.e., latitude and longitude) obtained from positioning sensors (in this case, mobile GPS) with a digital geographic information system (GIS) road map. Any map-matching algorithm first identifies the road link on which a vehicle is traveling and then determines the vehicle’s location on that road segment. In the proposed information system, at a given point of time, a number of vehicle locations (latitude and longitude) are received from passengers traveling on a bus. In order to provide a precise vehicle location at a given point of time, a novel map-matching algorithm using fuzzy logic, which integrates multiple vehicle locations (obtained from passenger’s smart phones) with a GIS road map, has been developed. The developed map-matching algorithm was tested using real-world data collected on four different bus routes in Aberdeenshire, Scotland, and also GPS data collected in and around Nottingham, United Kingdom. It was identified that the developed MM algorithm is efficient and capable of supporting the proposed passenger information system.\n""","1.2099136","""http://ascelibrary.org/doi/10.1061/%28ASCE%29CP.1943-5487.0000238""","[-2.099122,57.165019]"
"""Cranfield_University""","""ICT Tools to Support Public Participation in Water Resources Governance & Planning: Experiences from the Design and Testing of a Multi-Media Platform | Journal of Environmental Assessment Policy and Management , Vol 05, No 03 | World Scientific""","""Umair ul Hassan , Souleiman Hasan , Wassim Derguech , Louise Hannon , Eoghan Clifford , Christos Kouroupetroglou , Sander Smit , Edward Curry . 2017. Water Analytics and Management with Real-Time Linked Dataspaces. Government 3.0 – Next Generation Government Technology Infrastructure and Services, 173-196. [Crossref]\nYeray Hernández González , Serafín Corral Quintana .  (2016) An integrated assessment of alternative land-based passenger transport policies: A case study in Tenerife. Transportation Research Part A: Policy and Practice 89, 201-214.  Online publication date: 1-Jul-2016. [Crossref]\nAziza Akhmouch , Delphine Clavreul .  (2016) Stakeholder Engagement for Inclusive Water Governance: “Practicing What We Preach” with the OECD Water Governance Initiative. Water 8:5, 204.  Online publication date: 1-May-2016. [Crossref]\nAmanda E. Cravens .  (2016) Negotiation and Decision Making with Collaborative Software: How MarineMap ‘Changed the Game’ in California’s Marine Life Protected Act Initiative. Environmental Management 57:2, 474-497.  Online publication date: 1-Feb-2016. [Crossref]\nGiuseppe Munda . 2016. Multiple Criteria Decision Analysis and Sustainable Development. Multiple Criteria Decision Analysis, 1235-1267. [Crossref]\nFiona Ssozi-Mugarura , Edwin Blake , Ulrike Rivett .  (2015) Designing for sustainability: Involving communities in developing ICT interventions to support water resource management. 2015 IST-Africa Conference, 1-8. [Crossref]\nWAI MING TO , ANDY W. L. CHUNG .  (2014) PUBLIC ENGAGEMENT IN ENVIRONMENTAL IMPACT ASSESSMENT IN HONG KONG SAR, CHINA USING WEB 2.0: PAST, PRESENT AND FUTURE. Journal of Environmental Assessment Policy and Management 16:01.  Online publication date: 1-Mar-2014. [ Abstract | PDF (255 KB) | PDF Plus (258 KB) ]\nA. Zarli , Y. Rezgui , D. Belziti , E. Duce .  (2014) Water Analytics and Intelligent Sensing for Demand Optimised Management: The WISDOM Vision and Approach. Procedia Engineering 89, 1050-1057.  Online publication date: 1-Jan-2014. [Crossref]\nSamuel de Oliveira Apolonio , Adriana S. Vivacqua , Marcos R. S. Borges .  (2012) Scenario-based collaboration: An approach to refinement of plans through Public Engagement. Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD), 135-142. [Crossref]\nRobert Shipley , Stephen Utz .  (2012) Making it Count. CPL bibliography 27:1, 22-42.  Online publication date: 1-Feb-2012. [Crossref]\nMaria Manta conroy .  (2011) Influences on Public Participation in Watershed Planning: Why is it still a Struggle?. Planning Practice and Research 26:4, 467-479.  Online publication date: 1-Aug-2011. [Crossref]\nEsther Diez , Brian S. McIntosh .  (2011) Organisational drivers for, constraints on and impacts of decision and information support tool use in desertification policy and management. Environmental Modelling & Software 26:3, 317-327.  Online publication date: 1-Mar-2011. [Crossref]\nP. Paneque Salgado , S. Corral Quintana , Â. Guimarães Pereira , L. del Moral Ituarte , B. Pedregal Mateos .  (2009) Participative multi-criteria analysis for the evaluation of water governance alternatives. A case in the Costa del Sol (Málaga). Ecological Economics 68:4, 990-1005.  Online publication date: 1-Feb-2009. [Crossref]\nAlexandros Gasparatos , Mohamed El-Haram , Malcolm Horner .  (2008) A critical review of reductionist approaches for assessing the progress towards sustainability. Environmental Impact Assessment Review 28:4-5, 286-311.  Online publication date: 1-May-2008. [Crossref]\nBob Frame , Judy Brown .  (2008) Developing post-normal technologies for sustainability. Ecological Economics 65:2, 225-241.  Online publication date: 1-Apr-2008. [Crossref]\nB.S. McIntosh , C. Giupponi , A.A. Voinov , C. Smith , K.B. Matthews , M. Monticino , M.J. Kolkman , N. Crossman , M. van Ittersum , D. Haase , A. Haase , J. Mysiak , J.C.J. Groot , S. Sieber , P. Verweij , N. Quinn , P. Waeger , N. Gaber , D. Hepting , H. Scholten , A. Sulis , H. van Delden , E. Gaddis , H. Assaf . 2008. Chapter Three Bridging the Gaps Between Design and Use: Developing Tools to Support Environmental Management and Policy. Environmental Modelling, Software and Decision Support, 33-48. [Crossref]\nL. Failing , R. Gregory , M. Harstone .  (2007) Integrating science and local knowledge in environmental risk management: A decision-focused approach. Ecological Economics 64:1, 47-60.  Online publication date: 1-Oct-2007. [Crossref]\nB.S. McIntosh , R.A.F. Seaton , P. Jeffrey .  (2007) Tools to think with? Towards understanding the use of computer-based support tools in policy relevant research. Environmental Modelling & Software 22:5, 640-648.  Online publication date: 1-May-2007. [Crossref]\nGonzalo Gamboa , Giuseppe Munda .  (2007) The problem of windfarm location: A social multi-criteria evaluation framework. Energy Policy 35:3, 1564-1583.  Online publication date: 1-Mar-2007. [Crossref]\nMaja Schlüter , Nadja Rüger .  (2007) Application of a GIS-based simulation tool to illustrate implications of uncertainties for water management in the Amudarya river delta. Environmental Modelling & Software 22:2, 158-166.  Online publication date: 1-Feb-2007. [Crossref]\nMaja Schlüter , Nadja Rüger , Andre G. Savitsky , Nina M. Novikova , Michael Matthies , Helmut Lieth .  (2006) TUGAI: An Integrated Simulation Tool for Ecological Assessment of Alternative Water Management Strategies in a Degraded River Delta. Environmental Management 38:4, 638-653.  Online publication date: 1-Oct-2006. [Crossref]\nB.K. Koo , P.E. O'Connell .  (2006) An integrated modelling and multicriteria analysis approach to managing nitrate diffuse pollution: 1. Framework and methodology. Science of The Total Environment 359:1-3, 1-16.  Online publication date: 1-Apr-2006. [Crossref]\n""","0.35333952","""http://www.worldscientific.com/doi/abs/10.1142/S1464333203001383""","[-0.629225,52.074389]"
"""Brunel_University_London""","""Perceptual impact of environmental factors in sighted and visually impaired individualsBritish Journal of Visual Impairment - Olinkha Gustafson-Pearce, Eric Billett, Franjo Cecelja, 2005""","""PDF\nAbstract\nTo a visually impaired individual the physical world presents many challenges.                     For a person with impaired sight, way finding through a complex environment is                     fraught with dangers, both actual and imagined. The current generation of                     mobility aids have the possibility of addressing a broad range of physical                     issues through technological solutions. The perception of difficulty, however,                     can mean that many visually impaired individuals are fearful or uncomfortable                     about independent mobility or travel. In this context it becomes necessary to                     discover exactly what environments, environmental factors or items constitute a                     ‘perception of difficulty’ in the individual’s                     mental landscape and may trigger a negative responsebeforethey interact with the physical environment. This article reports on research                     that sought to ascertain what levels of perceptual difficulties specific                     environments and factors presented to individuals. The research was conducted                     with both visually impaired and sighted groups and compared differences and                     similarities in perceptual difficulty between these two groups.\nBarfield, W.                 ,                      Mann, S.                 ,                      Baird, K.                 ,                      Gemperle, F.                 ,                      Kasabach, C.                 ,                      Stivoric, J.                 ,                      Bauer, M.                 ,                      Martin, R. &.                      Cho, G.                  (2001) Computational Clothing and Accessories in Fundamentals                     of Wearable Computers and Augmented Reality. Hillsdale, NJ: Lawrence Erlbaum Associates . Google Scholar\nFermuller, C. & Aloimonos, Y.                  (1995) ‘Vision and Action’ , Image and Vision Computing 13(10). Google Scholar , Crossref\nGaraj, V.                  (2001) ‘The Brunel Navigation System for the                     Blind: Determination of the Appropriate Position to Mount the External GPS                     Antenna on the User’s Body’, ION GPS                 2000, Salt Lake City, Utah, USA . Google Scholar\nGaraj, V.                 ,                      Jirawimut, R.                 ,                      Ptasinski, P.                 ,                      Cecelja, F. & Balachandran, W.                  (2003) ‘A System for Remote Sighted Guidance of Visually Impaired Pedestrian’ , British Journal of Visual Impairment 21(2). Google Scholar , Link\nJirawimut, R.                 ,                      Prakoonwit, S.                 ,                      Cecelja, F. & Balachandran, W.                  (2003a) ‘Visual Odometer for Pedestrian Navigation’ , IEEE Transactions on Instrumentation and Measurement 52(4): 1166-1173 . Google Scholar , Crossref\nJirawimut, R.                 ,                      Ptasinski, P.                 ,                      Garaj, V.                 ,                      Cecelja, F. & Balachandran, W.                  (2003b) ‘A Method for Dead Reckoning Parameter Correction in Pedestrian                         Navigation System’ , IEEE Transactions on Instrumentation and Measurement 52(1). Google Scholar , Crossref\nJohnson, V.                  and                      Petrie, H.                  (1998) ‘Travelling Safely: The Problems and Concerns of Blind Pedestrians’ , British Journal of Visual Impairment 16(1): 27-31 . Google Scholar , Link\nKitchen, R. M.                 ,                      Blades, M.                  and                      Golledge, R. G.                  (1997) ‘Understanding Spatial Concepts at the Geographic Scale without                         the Use of Vision’ , Progress in Human Geography 21(2): 225-242 . Google Scholar , Link\nPtasinski, P.                 ,                      Cecelja, F.                  and                      Balachandran, W.                  (2002a) ‘Altitude Aiding for GPS Systems using Elevation Map Datasets’ , The Journal of Navigation 55: 451-462 . Google Scholar , Crossref\nPtasinski, P.                 ,                      Jirawimut, R.                 ,                      Cecelja, F. & Balachandran, W.                  (2002b) ‘Brunel Inverse DGPS Positioning System’ , Journal of the Institute of Navigation 48(4). Google Scholar\nPylyshyn, Z.                  (1999) ‘Is Vision Continuous with Cognition? The Case for Cognitive                         Impenetrability of Visual Perception’ , Behavioural and Brain Sciences 22: 341-423 . Google Scholar , Crossref , Medline\nSarter, N. B.                 ,                      Woods, D. D. & Billings, E. E.                  (1997) ‘Automation                 Surprises’, in                      G. Salvendy                  (ed.) Handbook of Human Factor and Ergonomics (2nd edn). New York: Wiley-Interscience . Google Scholar\nSommers, D.                  (1990) ‘Route Imaging for the Blind: The Use of Guided Imagery to                         Enhance the Mobility Performance of the Congenitally Blind’ , British Journal of Visual Impairment 7(1): 18-20 . Google Scholar , Link\nWoods, D. D.                  (1998) ‘Designs are Hypotheses about how Artifacts Shape Cognition and Collaboration’ , Ergonomics 41(2): 168-173 . Google Scholar , Crossref\n""","0.48294455","""http://journals.sagepub.com/doi/10.1177/0264619605051720""","[-0.472855,51.532848]"
"""Brunel_University_London""","""Wallpaper* City Guides and Gendering the Urban AestheticTourist Studies - Monica Degen, Emma Wainwright, 2010""","""aesthetics , gender , lifestyle , tourist guides\nAshworth, G.J. and Voogd, H. ( 1990) Selling the City: marketing approaches in public sector urban planning. London: Belhaven Press. Google Scholar\nBhattarcharya, D. ( 1997) ‘Mediating India: An analysis of a guidebook’ , Annals of Tourism Research 24(2): 371-89. Google Scholar\nBianchini, F. and Parkinson, M. ( 1993) Cultural Policy and Urban Regeneration. Manchester: Manchester University Press. Google Scholar\nBondi, L. ( 1990) ‘Gender divisions and gentrification: a critique’, Transactions of the Institute of British Geographers, Vol 16, 190-198. Google Scholar , Crossref\nBondi, L. and Rose, E. ( 2003) ‘Constructing gender, constructing the urban: A review of Anglo-American feminist urban geography’, Gender, Place and Culture 10(3): 229-245. Google Scholar , Crossref\nBrownhill, S. ( 2000) ‘Regen(d)eration: Women and urban policy in Britain’ , in J. Darke, S. Ledwith and R. Woods (eds) Women and the City: Visibility and Voice in Urban Space. Basingstoke: Palgrave. Google Scholar\nCosgrove. D. ( 1984) Social formation and symbolic landscape. London: Croon Helm. Google Scholar\nCronin, A. and Hetherington, K. ( 2008) Consuming the Entrepreneurial City: Image, Memory, Spectacle. London. New York: Routledge. Google Scholar\nDepartment of Environment, Transport and Regions ( 1999) Towards an Urban Renaissance. London : ODPM. Google Scholar\nDegen, M. ( 2008) Sensing Cities: Regenerating Public Life in Barcelona and Manchester. London: Routledge . Google Scholar\nDel Casino, V. and Hanna, S.P. ( 2000) ‘Representations and identities in tourism map spaces’ , Progress in Human Geography 24(1): 23-46. Google Scholar , Link\nDel Casino, V. and Hanna, S.P. ( 2006) ‘Beyond The ‘Binaries’: A Methodological Intervention for Interrogating Maps as Representational Practices’, ACME: An International E-Journal for Critical Geographies 4(1): 34-56. Google Scholar\nEade, J. ( 2000) Placing London: From Imperial Capital to Global Capital . London: Berghahn Books. Google Scholar\nEdensor, T. ( 2001) ‘Performing tourism, staging tourism’, Tourist Studies 1(1): 59-81. Google Scholar , Link\nEllis, C. ( 1998) ‘What counts as scholarship in communication? An autoethnographic response’, American Communication Journal 1: 1-5. Google Scholar\nEllis, C. and Bochner, A. ( 2000) ‘Autoethnography, personal narrative, reflexivity: researcher as subject’, pp. 733-768 in N. K. Denzin and Y. S. Lincoln (eds) Handbook of Qualitative Research (second edition). Thousand Oaks: Sage Publications. Google Scholar\nEvans, G. ( 2001) Cultural Planning: An Urban Renaissance? London: Routledge. Google Scholar , Crossref\nFairclough, N. ( 2003) Analysing Discourse: Textual Analysis for Social Research . London: Routledge. Google Scholar\nFeatherstone, M. ( 1991) Consumer Culture and Postmodernism. London: Sage Publications. Google Scholar\nFelski, R. ( 2000) ‘The invention of everyday life’, New Formations 39: 13-32. Google Scholar\nGilbert, D. ( 1999) \""London in all its glory - or how to enjoy London’: Guidebook representations of imperial London’, Journal of Historical Geography 25(3): 279-297. Google Scholar , Crossref\nGolinski, J. ( 1998) Making natural knowledge: constructivism and the history of science. Cambridge: Cambridge University Press. Google Scholar\nGreenberg, M. ( 2000) ‘Branding Cities: A Social History of the Urban Lifestyle Magazine’, Urban Affairs Review 36(2): 228-263. Google Scholar , Link\nGrosz, E. ( 1998) \""Bodies-cities\"", in Pile, S. & Nast, H. (eds) Places through the body. London: Routledge, 42-51. Google Scholar\nHarvey, D. ( 1990) The Condition of Postmodernity. Oxford : Blackwell. Google Scholar\nHarvey, D. ( 2003) Paris: Capital of Modernity. New York : Routledge Google Scholar\nHubbard, P. ( 2004) ‘Revenge and Injustice in the Neoliberal City: Uncovering Masculinist Agendas’, Antipode, 36 (4): 665-686. Google Scholar , Crossref\nHumphreys, M. ( 2005) ‘Getting personal: reflexivity and autoethnographic vignettes’, Qualitative Inquiry 11: 840-860. Google Scholar , Link\nImrie, R. and Raco, M. ( 2003) Urban Renaissance? New Labour, Community and Urban Policy . Bristol: Policy Press. Google Scholar , Crossref\nJacobs, J.M. and Fincher R. ( 1998) ‘Introduction’, pp. 000-000 in R. Fincher and J. M. Jacobs (eds) Cities of Difference. New York: Guilford Press: 1-26. Google Scholar , Crossref\nJohnston, L. ( 2001) ‘(Other) bodies and tourism studies’, Annals of Tourism Research 28: 180-201. Google Scholar , Crossref\nJudd, D. and Fainstein, S. ( 1999) The Tourist City. Yale: Yale University Press. Google Scholar\nJudd, D. ( 2003) ‘Visitors and the Spatial Ecology of the City’ , pp. 000-000 in L. Hoffman et al. (eds) Cities and Visitors: Regulating Cities, Markets and City Space. Oxford: Blackwell Publisher: 23-38. Google Scholar , Crossref\nKearns, G. and Philo, C. ( 1993) Selling Places. Oxford: Pergamon Press. Google Scholar\nKehily, M.J. and Nayak, A. ( 2008) ‘Global femininities: consumption, culture and the significance of place’, Discourse: Studies in the Cultural Politics of Education 29: 325-342. Google Scholar , Crossref\nKing, A. ( 1996) ‘Worlds in the city: Manhattan transfer and the ascendance of spectacular space’, Planning Perspectives 11: 97-114. Google Scholar , Crossref\nLash, S. and Urry, J. ( 1994) Economies of Signs and Spaces. London : Sage Publications. Google Scholar\nLeslie, D. and Reimer, S. ( 2003) ‘Gender, modern design and home consumption’ , Environment and Planning D 21: 293-316. Google Scholar , Link\nMcDowell, L. ( 1997) Capital Culture: Gender at Work in the City Cover Capital Culture: Gender at Work in the City. Oxford: Blackwell Google Scholar\nMcDowell, L. ( 1991) ‘Life without father and Ford: The new gender order of post-Fordism’, Transactions of the Institute of British Geographers 16: 400-419. Google Scholar , Crossref\nMiles, S. and Miles, M. ( 2004) Consuming Cities. Basingstoke: Palgrave. Google Scholar , Crossref\nNead, L. ( 2000) Victorian Babylon: People, Streets and Images in Nineteenth-Century London. Yale: Yale University Press. Google Scholar\nNegrin, L. ( 2006) ‘Ornament and the feminine’, Feminist Theory 7(2): 219-235. Google Scholar , Link\nPaterson, M. ( 2006) Consumption and Everyday Life. London : Routledge. Google Scholar\nPink, S. ( 2009) Doing Sensory Ethnography. London: Sage Publications. Google Scholar , Crossref\nPrice, D. ( 1997) ‘Surveyors and surveyed: photography out and about’ , in L. Wells (ed) Photography: a critical introduction. London: Routledge. pp 65-116. Google Scholar\nPritchard, A. ( 2001) ‘Tourism and representation: A scale for measuring gendered portrayals’, Leisure Studies 20: 79-94. Google Scholar , Crossref\nPritchard, A. and Morgan, N.J. ( 2000a) ‘Privileging the male gaze. Marketing gendered landscapes’, Annals of Tourism Research 27(3): 884-905. Google Scholar , Crossref\nPritchard A., and Morgan N.J. ( 2000b) ‘Constructing tourism landscapes - gender, sexuality and space’, Tourism Geographies 2(2): 115-139. Google Scholar , Crossref\nPostrel, V. ( 2004) The Substance of Style. Harper Perennial : New York. Google Scholar\nRendell, J. ( 2000) ‘Introduction: ‘Gender, space\""’ pp. 000-000 in J. Rendell, B. Penner and I. Borden (eds) Gender Space Architecture . London: Routledge: 101-111. Google Scholar\nRose, G. ( 1993) Feminism and Geography: The Limits to Geographical Knowledge . Cambridge: Polity Press. Google Scholar\nSassen, S. and Roost, F. ( 1999) ‘The City: Strategic Site for the Global Entertainment Industry’, pp. 000-000 in D. Judd and S. Fainstein (eds) The Tourist City. London: Yale University Press: 143-154. Google Scholar\nSmith, N. ( 1987) ‘Of yuppies and housing: Gentrification, social restructuring, and the urban dream’, Environment and Planning D 5: 151-172. Google Scholar , Link\nSmith, M. ( 2006) Tourism, Culture and Regeneration. London: CABI Press. Google Scholar , Crossref\nSpry, T. ( 2001) ‘Performing autoethnography: An embodied methodological praxis’, Qualitative Inquiry 7: 706-732. Google Scholar , Link\nUrry, J. ( 2002) The Tourist Gaze (second edition). London: Sage Publications. Google Scholar\nVisitLondon.com (2009) Key Visitor Statistics. Available at www.visitlondonmediacentre.com/facts_figures/media_facts_sheets/ (accessed 14 April 2009). Google Scholar\nWainwright, E. (2009) ‘Manufacturing space: gendered cityscapes and industrial images in Dundee’, Environment and Planning A 4: 336-352. Google Scholar\nWallpaper* Guide London (2008) London: Phaidon. Google Scholar\nWard, K. ( 1998) Selling Places: The Marketing and Promotion of Towns and Cities 1850-2000. London: Spon. Google Scholar\nWeisman, L.K. ( 2000) ‘Prologue’, in J. Rendell, B. Penner & I. Borden (eds) Gender, space and architecture, an interdisciplinary approach. London: Routledge: 1-5. Google Scholar\nWestwood, S., Pritchard, A. and Morgan, N.J. ( 2000) ‘Gender-blind marketing: businesswomen’s perceptions of airline services’, Tourism Management 21: 353-362. Google Scholar , Crossref\nWilson, E. ( 1991) The Sphinx in the City: Urban Life, the Control of Disorder, and Women. Berkeley: University of California Press. Google Scholar\nWilson, E. ( 2000) ‘The Future of Women’, in J. Darke, S. Ledwith and R. Woods (eds) Women and the City: Visibility and Voice in Urban Space. Basingstoke: Palgrave. Google Scholar\nWolff, J. ( 1985) ‘The invisible flâneuse: Women and the literature of modernity’, Theory, Culture and Society 2(3): 37-46. Google Scholar , Link\nZukin, S. ( 1995) The Culture of Cities. Oxford, Blackwell. Google Scholar\nZukin, S. ( 1998) ‘Urban Lifestyles: Diversity and Standardisation in Spaces of Consumption’, Urban Studies 36 (5/6): 825-839. Google Scholar , Link\nZukin, S. ( 2001) ‘How to create a culture capital: reflections on urban markets and places’, pp. 000-000 in I. Blazwick (ed.) Century City: Art and Culture in the Modern Metropolis. London: Tate Publishing : 258-265. Google Scholar\nZukin, S. ( 2005) Point of Purchase: How Shopping Changed American Culture . New York: Routledge. Google Scholar\n""","0.23630902","""http://journals.sagepub.com/doi/abs/10.1177/1468797611403056""","[-0.472855,51.532848]"
"""Aston_University""","""The challenges of developing an online tool to measure the quality of the passenger experience in a PanEuropean context - Research Explorer : Aston University""","""The challenges of developing an online tool to measure the quality of the passenger experience in a PanEuropean context\nResearch output: Chapter in Book/Report/Conference proceeding › Conference contribution\nAston Business School\nAbstract\nMETPEX is a 3 year, FP7 project which aims to develop a PanEuropean tool to measure the quality of the passenger's experience of multimodal transport. Initial work has led to the development of a comprehensive set of variables relating to different passenger groups, forms of transport and journey stages. This paper addresses the main challenges in transforming the variables into usable, accessible computer based tools allowing for the real time collection of information, across multiple journey stages in different EU countries. Non-computer based measurement instruments will be used to gather information from those who may not have or be familiar with mobile technology. Smartphone-based measurement instruments will also be used, hosted in two applications. The mobile applications need to be easy to use, configurable and adaptable according to the context of use. They should also be inherently interesting and rewarding for the participant, whilst allowing for the collection of high quality, valid and reliable data from all journey types and stages (from planning, through to entry into and egress from different transport modes, travel on public and personal vehicles and support of active forms of transport (e.g. cycling and walking). During all phases of the data collection and processing, the privacy of the participant is highly regarded and is ensured.\n""","1.0038806","""https://research.aston.ac.uk/portal/en/researchoutput/the-challenges-of-developing-an-online-tool-to-measure-the-quality-of-the-passenger-experience-in-a-paneuropean-context(f8ef99ad-b413-4703-9667-6e45062273cd).html""","[-1.888803,52.487018]"
"""University_of_Glasgow""","""The Application of Information and Other Technologies to Improve the Mobility of Blind, Visually Impaired and Deafblind People, Travel Health Informatics And Telehealth  - Enlighten: Publications""","""Enlighten: Publications\nLogin\nIn this section\nThe Application of Information and Other Technologies to Improve the Mobility of Blind, Visually Impaired and Deafblind People, Travel Health Informatics And Telehealth\nHersh, M.A. (2009)     The Application of Information and Other Technologies to Improve the Mobility of Blind, Visually Impaired and Deafblind People, Travel Health Informatics And Telehealth.            In: EFMI Special Topic Conference, Antalya, Turkey, 12-15 Nov 2009,\nFull text not currently available from Enlighten.\n""","1.1325935","""http://eprints.gla.ac.uk/102482/""","[-4.28836,55.871751]"
"""Imperial_College_London""","""The future for in-vehicle information systems: The technology and its impacts - Bell - 2002 - Journal of Advanced Transportation - Wiley Online Library""","""Journal of Advanced Transportation\nThe future for in-vehicle information systems: The technology and its impacts\nAuthors\nMichael G.H. Bell is in the Department of Civil and Environmental Engineering, Imperial College of Science, Technology and Engineering, London, U.K.\nFirst published:\nCited by (CrossRef): 2 articles Check for updates\nCitation tools\nFunding Information\nAbstract\nIn-vehicle information has an important social role to play in improving the efficiency and safety of travel by all modes. In this review, three generations of system are identified. The first generation consists of simple in-vehicle units relying heavily on external data. The second generation has more sophisticated in-vehicle units with colour TFT screens and DVD players for maps and entertainment. The third generation again makes use of external data, using the mobile phone network to download map sections and other data as and when required, thereby obviating the need for beacons and map CDs. For locationing, GPS (and/or Galileo, the European version of GPS) remains the favoured technology. Portable devices offering multi-modal information could improve inter-modal transport efficiency.\n""","0.6073563","""http://onlinelibrary.wiley.com/doi/10.1002/atr.5670360303/abstract""","[-0.178219,51.500505]"
"""StaffOxfordUniversityPhysics""","""How to decarbonize? Look to Sweden: Bulletin of the Atomic Scientists: Vol 72, No 2""","""How to decarbonize? Look to Sweden\nAbstract\nABSTRACT\nBringing global warming to a halt requires that worldwide net emissions of carbon dioxide be brought to essentially zero; the sooner this occurs, the less warming our descendants for the next 1000 years and more will need to adapt to. The widespread fear that the actions needed to bring this about conflict with economic growth is a major impediment to efforts to protect the climate. But much of this fear is pointless, and the magnitude of the task – while great – is no greater than the challenges that human ingenuity has surmounted in the past. To light the way forward, we need to examine success stories where nations have greatly reduced their carbon dioxide emissions while simultaneously maintaining vigorous growth in their standard of living; a prime example is Sweden. Through a combination of sensible government infrastructure policies and free-market incentives, Sweden has managed to successfully decarbonize, cutting its per capita emissions by a factor of 3 since the 1970s, while doubling its per capita income and providing a wide range of social benefits. This has all been accomplished within a vigorous capitalistic framework that in many ways better embodies free-market principles than the US economy. Consequently, the Swedish experience shows that solving global warming does not require us to “tear down capitalism.” The world just needs to be a bit more like Sweden.\nKEYWORDS: Global warming ,  climate change ,  decarbonization ,  Sweden ,  carbon budgets ,  Naomi Klein ,  capitalism ,  energy policy\nIt is surprising and frustrating that, at a time of increasingly overwhelming evidence that human-caused global warming poses a serious problem, public support in the United States for actions of the magnitude required has stubbornly remained tepid for decades. Despite wholehearted denial that there is even a problem – a stance that makes it unique even among the world’s conservative political parties – the Republican Party is able to garner the support of about half of all US voters. While the United States is not alone in failing to meet the challenge of global warming, its status as one of the world’s top emitters of carbon dioxide (chief cause of global warming) makes its recalcitrance especially problematic.\nIn my experience, inaction on restraining carbon dioxide emissions does not stem from insufficient understanding of the science or insufficient fear of the consequences of warming. Instead, it is more due to excessive fear of the nature of the solutions. On the political right, this takes the form of a fear that it is all a thinly disguised leftist plot to impose socialism. Naomi Klein’s recent book, This Changes Everything: Capitalism vs the Climate, plays directly into this fear. Klein’s thesis is that capitalism is structurally unable to meet the challenge of getting fossil fuels out of the economy (Klein 2014 Klein, N. 2014. This Changes Everything: Capitalism vs. the Climate. New York: Simon and Schuster.  [Google Scholar] ; see also the November 2015 interview with Klein on the Bulletin’s website: http://thebulletin.org/naomi-klein-climate-change-makes-hotter-and-meaner-world8910 ). But while Klein is right about many things, including the problem’s urgency and the need for most of the world to take a different course from what is happening right now, she is quite wrong about the root cause of the current inadequate response to global warming.\nThe problem is not too much capitalism, but rather too little, and even a lack of faith in the power of the ingenuity unleashed by capitalism to solve big problems. As currently practiced, US capitalism, far from being the archetype of a free-market economy, is riddled with fossil-fuel subsidies and hobbled by politically powerful corporate stakeholders who have used their influence to protect the value of their fossil-fuel assets, regardless of how bad this may be for the rest of the economy. And even free markets cannot function properly if burdened by “externalities,” such as polluters not having to pay for the damage they cause. It is a case of privatizing the profits, while shunting the expenses off to the public.\nTo break the current impasse, the United States needs to look to success stories for reassurance that decarbonization is not mutually exclusive with prosperity and economic growth. We need to look at Sweden.\nSweden is not socialist\nIf Sweden is to be held up as a model for successful action against global warming in progressive capitalist democracies everywhere, then we must first dispel the notion that Swedes live a grim joyless existence, subsisting on the handouts of an intrusive welfare state. In fact, the very term “welfare state” distorts the Swedes’ conception about the role of their government. The term traditionally used in Sweden is Folkhemmet – the people’s home. The term was introduced in 1928 by Social Democrat leader Per Albin Hansson and meant to encapsulate the Swedish vision of a cooperative, capitalist society in which all work together to provide a good life for all.\nThe principles embodied in Folkhemmet amounted to a rejection of the Marxist approach based on class struggle, revolution, and nationalization of the means of production.\nThe building of Folkhemmet is a dynamic process, with the vision evolving over the years. Still, from the standpoint of an outside observer such as myself, having lived in both the United States and Sweden, it looks like the core principles of Folkhemmet remain intact. It involves a high level of public services delivered efficiently either directly by the government or via private intermediaries in cases where collective action can deliver essential services better than unguided private enterprise. It recognizes that when it comes to providing essential services such as higher education, public transit, or health care, sometimes the “invisible hand” of the market is not just invisible, but entirely absent. Major Swedish social benefits include universal access to health care under a single-payer system, high-quality day care and preschools provided at affordable rates, tuition-free university education, and a full year-and-a-half of paid parental leave. On top of that, every working Swede is entitled to 38 days of paid vacation, plus 18 national holidays – and none of that needs to be used up to provide sick leave or time off to care for sick children, because that is all covered under a separate benefit program.\nIf this is to be called “socialism,” all I can say is that a lot of countries would benefit from a lot more of it. As economist Paul Krugman puts it:\nEvery time I read someone talking about the “collapsing welfare states of Europe,” I have this urge to take that person on a forced walking tour of Stockholm. If you believed what the Right says, a country with Sweden’s level of both taxes and social benefits should be a wasteland. Strange to say, that’s not what it looks like, to say the least. (Krugman 2011 Krugman, P. 2011. “Socialist Hellhole Blogging.” International New York Times, August 19. http://krugman.blogs.nytimes.com/2011/08/19/socialist-hellhole-blogging/   [Google Scholar] )\nIndeed, a long summer evening spent among Swedish friends winding down after work at Medborgarplatsen in Stockholm’s now-fashionable Södermalm district is sufficient to dispel any notions of a gloomy Swedish character one might have imagined if one knew Sweden only through Bergman flicks. However, Sweden is emphatically not socialist. In fact, all this has been carried out within a vigorous free-market economy that fosters innovation and entrepreneurship. Sweden consistently ranks in the top 10 in the World Economic Forum competitiveness rankings and has even beaten the United States in many years. The success of Swedish free enterprise is attested by major multinational corporations such as Volvo, Alfa Laval (which provides a variety of solutions for heavy industry), the engineering firm of ASEA Brown Boveri, and the ready-to-assemble furniture maker IKEA, all of which have their roots in Sweden.\nMore recent examples of Swedish tech innovation include video chat and conference call innovator Skype and the digital music service Spotify. Sweden has become a high-tech start-up paradise not only because of its business-friendly policies, but also because the country’s comprehensive social safety net makes being an entrepreneur less personally risky; financial failure does not put the basic health and well-being of one’s family in jeopardy. And at the same time, Sweden has implemented such typically conservative shibboleths as low corporate income taxes and complete elimination of the inheritance tax. All the traditional post office functions are fulfilled by private providers who compete to provide the best service; the operation of railways and public transit has been largely subcontracted to competitive private enterprise; and there is even substantial involvement by private enterprise in delivering medical care and schooling.\nIronically, the conservative punditry has begun to awaken to the notion that if anything, Sweden represents the opposite of socialism; commentators such as Tim Worstall of Forbes have lately been using this realization to counter arguments that socialism can be successful. Worstall’s premise is that countries like Sweden are not really welfare states at all but “economically free, even classically liberal economies” (Worstall 2011 Worstall, T. 2011. “Paul Krugman and the Socialist Hellhole That Is Sweden.” Forbes, August 20. http://www.forbes.com/sites/timworstall/2011/08/20/paul-krugman-and-the-socialist-hellhole-that-is-sweden/ .  [Google Scholar] ). This is in fact a helpful development, since the major impediment to using Sweden as an exemplar of the way forward, particularly in the politics of the United States, is the misperception that Sweden is a socialist welfare state.\nHere are the numbers: Sweden’s national budget ran a substantial surplus up to 2008, and was essentially balanced through 2011. Since that time the budget deficit has been allowed to grow to accommodate stimulus spending in response to the global financial crisis, but in 2014, the deficit was still under 2 percent of gross domestic product (GDP). Despite the recent world economic crisis, Swedish national debt is under 50 percent of its GDP, in contrast to US figures which come in at over 100 percent of GDP. Sweden recognized early that the demographic crisis in its national pension system threatened fiscal stability, and managed to implement pension reforms (largely by turning the national pension system into a defined-contribution plan) that put pensions on a sound financial footing while still guaranteeing a decent standard of living in retirement. This is something that the United States has yet to accomplish. In Sweden, conservative and liberal parties have been able to face reality and work together to solve problems that political leaders in the United States only dream of solving. And so, too, has been the case with the problem of decarbonization.\nSweden’s success at decarbonization\nThe Swedish success story in decarbonization is told in Figure 1 . Here, Swedish carbon dioxide emissions are given in terms of metric tons of carbon in the carbon dioxide emitted each year, expressed in per capita terms. One metric ton (called a “tonne” for short) is 1000 kilograms, or about 2200 pounds. Carbon is often used in emissions accounting, because it is the carbon in fossil fuels that is moved to the atmosphere when fuel is burned – the oxygen in carbon dioxide comes from the atmosphere. Carbon dioxide is about 27 percent carbon by weight.\nHow to decarbonize? Look to Sweden\nAll authors\nPublished online:\n02 March 2016\nFigure 1. Swedish per-capita gross national income (dashed red line) and annual per-capita CO2 emissions (expressed as metric tons of carbon) over time. The per-capita income is based on constant United States dollars as of the year 2011, computed on the basis of purchasing power parity. Swedish emissions data is courtesy of the Carbon Dioxide Information Analysis Center ( http://cdiac.ornl.gov ) and the Emission Database for Global Atmospheric Research ( http://edgar.jrc.ec.europa.eu/ ). Swedish national income data courtesy of World Macroeconomic Research ( http://macroeconomics.kushnirs.org/ ).\nDisplay full size\nFigure 1. Swedish per-capita gross national income (dashed red line) and annual per-capita CO2 emissions (expressed as metric tons of carbon) over time. The per-capita income is based on constant United States dollars as of the year 2011, computed on the basis of purchasing power parity. Swedish emissions data is courtesy of the Carbon Dioxide Information Analysis Center ( http://cdiac.ornl.gov ) and the Emission Database for Global Atmospheric Research ( http://edgar.jrc.ec.europa.eu/ ). Swedish national income data courtesy of World Macroeconomic Research ( http://macroeconomics.kushnirs.org/ ).\nBefore 1970, carbon emissions were growing exponentially, as is typical for a rapidly industrializing country and is still the case for China and India. Emissions peaked abruptly in the early 1970s, however, when Sweden put into place a number of energy policies in response to the oil shocks of that time. This was initially seen as a matter of national security, and not done primarily for environmental reasons, although environmental considerations could not have been far off the radar screen in the country of Svante Arrhenius – one of the first chemists to win the Nobel Prize, who first clearly drew the connection between fossil fuels and global warming in the early 1900s (Graham 2000 Graham, S. 2000. “Svante Arrhenius. NASA Earth Observatory.” January 18. http://earthobservatory.nasa.gov/Features/Arrhenius/ .  [Google Scholar] ). Sweden’s emissions have been declining ever since, with precipitous declines in the 1980s when a wave of environmentally motivated decarbonization incentives began to be put into place. For example, the Swedish pulp and paper industry increased its output by 18 percent while reducing its carbon emissions by 80 percent between 1970 and 1990. Currently, Swedish per capita emissions are about as low as they were in 1950. At about 1.25 tonnes annually per person, they are about the same as the global average, and far below the US figure of 4.5 tonnes per capita.\nOne of the ingredients in the Swedish success story is the early introduction of carbon taxes (Andersson and Lövin 2015 Andersson, M., and I. Lövin 2015. “Sweden: Decoupling GDP Growth from CO2 Emissions Is Possible.” The World Bank Blog, May 22. http://blogs.worldbank.org/climatechange/sweden-decoupling-gdp-growth-co2-emissions-possible .  [Google Scholar] ). The strong scientific consensus underpinning concern over human-caused global warming is well known, but economists of all political persuasions are almost equally united behind the principle that carbon taxes are the most economically efficient way to bring down carbon dioxide emissions. Sweden introduced one of the world’s first carbon taxes in 1991, at a rate of about $110 per tonne of carbon. The tax was introduced as part of a package of tax reforms that eliminated most other forms of energy tax. These reforms partly offset the carbon tax and provided economic incentives to shift to low-carbon energy sources, without mandating what form that shift would take.\nIndustrial energy use was taxed at half the standard rate to prevent “leakage,” whereby industrial production would simply move to countries where the cost of emitting carbon is lower. (Later, after the European Union emissions trading scheme was put into place, industries covered by that scheme were exempt from domestic carbon taxes.) In recent years, the Swedish carbon tax increased to over $500 per tonne of carbon, and some of the special tax breaks for industry have been reduced. To put this in perspective, the price of carbon under the European Union’s emission trading scheme has collapsed due to an oversupply of permits, and has recently traded at under $30 per tonne, while the price in California’s own emissions trading scheme is around $50 per tonne, and British Columbia’s revenue-neutral carbon tax is about $30 per tonne.\nIn Sweden, the carbon tax had its biggest effect on the heating of buildings, where the tax has been applied at its full rate. Residential and commercial energy use is now almost entirely carbon-neutral in Sweden, a remarkable achievement for a country with a harsh Northern climate. A key ingredient to this success is the extensive use of district heating – basically like an electricity grid, but for heat – in high-density regions. The district heating grid allows energy to be fed in from a number of different sources. Biofuels make up about 70 percent of the energy in district heating now, but heat is also fed into the grid from non-recyclable garbage combustion, the recovery of waste heat from server farms, and even from the excess body heat of passengers in the central railway station. Where district heating is not practical, there is extensive use of high-efficiency heat pumps powered by renewable electricity. Heat pumps work through a kind of thermodynamic magic whereby it takes less than one watt of power to move heat energy at a rate of one watt from a colder place to a warmer place. But this magic works best when the “uphill” temperature difference is not too large, and so it plays well in conjunction with geothermal heat storage systems (“bergvärme” in Swedish), which tap into the reservoir of nearly constant temperatures some tens of feet down in the bedrock. This technique has been applied extensively in Sweden, and can be used wherever there is rock to drill into, which is basically everywhere.\nPublic transit is another area of substantial progress in decarbonization. The Arlanda Express is powered by renewable electricity, and whisks you from downtown Stockholm to distant Arlanda Airport in a mere 20 minutes (an amenity not available between Manhattan and JFK). Streets are populated by various kinds of biofuel and plug-in hybrid buses, and there is an excellent high-speed rail network, largely powered by renewable electricity, ready to take you farther afield. Hydropower is still a major source of renewable electricity in Sweden, but wind power has increased from 0.3 percent of the mix to 8.4 percent in 2014, and that is even before Sweden brings Europe’s largest wind farm online.\nBut carbon taxes are not the whole story. Making buildings carbon-neutral required extensive investments in building energy efficiency, such as low-emissivity triple glazing, insulation, and heat-exchanging ventilation, so as to reduce energy needs to the point where they could be fully supplied by carbon-neutral sources. These efforts have been part and parcel of the retrofitting of much of Sweden’s existing housing stock, not just mandated for new construction. An important collateral benefit is that Swedish homes are much more cozy in winter than the drafty, damp Victorian piles many of us inhabit in Oxford (where I currently ply my profession as a professor of physics).\nEven with carbon taxes, efficient buildings, and district heating, the necessary infrastructure did not miraculously come into being as a result of the invisible hand of the market, but required encouragement through building codes, judiciously applied regulation, and a certain amount of direct government investment. Likewise, it was important that an increasingly renewable-powered public transit and high-speed rail infrastructure stood ready to take over from automobiles. Even though most of this network is operated by private enterprise, substantial forward-looking public investments were required to bring them into being.\nNuclear power also provides low-carbon electricity, and Sweden’s nine nuclear power plants produce 40 percent of the electricity used or exported. Nuclear power is not loved by the Green Party in Sweden, but nonetheless the 1980 decision to phase out nuclear power was repealed in 2010, in recognition of the fact that low-carbon replacements would not be available in time. Four of the older plants are scheduled to be closed in 2020, which will take 2.7 gigawatts of base load power off the grid, but this will be easily replaced by the expansion of wind energy in connection with abundant pumped hydropower storage.\nSweden’s cautious approach to nuclear power stands in contrast to Germany’s foolish decision to abruptly shutter their nuclear power plants, which has resulted in a significant setback to their progress on decarbonization. The retention of nuclear power in Sweden also reflects recognition of the fact that large-scale hydropower, while benign to the climate, is far from environmentally benign generally (Leslie 2006 Leslie, J. 2006. Deep Water: The Epic Struggle over Dams, Displaced People, and the Environment. London: Picador.  [Google Scholar] ). For example, when the Suorva dam was built in the far north in the 1920s, it shut off Europe’s largest waterfall, and subsequent expansions of the reservoir destroyed the diverse aquatic ecosystem that used to exist in the valley flooded by the artificial Akkajärvi lake today, and displaced thousands of indigenous Sami people from their traditional way of life. There is little stomach in Sweden for further expansion of large-scale hydropower, and compared with the environmental disaster of large-scale hydropower (to say nothing of coal), nuclear really does not look too bad.\nSo, has all this emphasis on decarbonization trashed the economy? Figure 1 tells that story as well. Swedish per capita gross national income (GNI) has doubled in real terms since 1970, with steady growth interrupted only by the same periods of global recession that have plagued other countries. And GNI, like its close cousin GDP, does not provide a full measure of quality of life, as many of the social amenities that make Sweden such a good place to live are not reflected in such economic statistics. In the absence of a “control Sweden” run without climate-smart policies, it is difficult to say how rapidly the GNI would have grown without such policies, but there is no credible evidence that climate and energy-smart policies have been a significant drag on the economy. On the contrary, it is likely that these policies benefited GNI growth, by reducing the cost of energy imports, providing high-quality employment, and fostering the development of exportable technologies. But regardless of the question of what the Swedish economy would have done in a hypothetical world, the real world demonstrates beyond doubt that a country can grow its economy and provide a very high standard of living for its people while simultaneously reducing its carbon emissions substantially.\nToward net-zero emissions\nAs remarkable as Sweden’s achievement at decarbonization is, it is not enough. The reason is that the geological processes that ultimately remove excess carbon dioxide from the atmosphere operate exceedingly slowly, over a timescale of tens or even hundreds of thousands of years. That means that as long as there is any net emission of fossil-fuel carbon dioxide remaining, the gas will inexorably continue to accumulate in the atmosphere and the Earth will continue to warm. Reducing the carbon emission rate is an unambiguously good thing, because it reduces the rate of warming, but to halt warming, the world needs to eventually achieve net-zero emissions. The longer it takes to reach net zero, the higher the temperature our descendants will be stuck with into the indefinite future. Sweden is one of very few countries to have undertaken a definite commitment to achieve net-zero emissions, having pledged to reach that goal by 2050. As virtually all remaining Swedish carbon dioxide emissions come from liquid fuels used in transportation (especially private automobiles), getting to net zero will primarily require retooling the passenger car fleet to run on some combination of electricity and biofuels.\nCarbon budgets provide the key measuring stick that tells us how fast we need to get to net-zero emissions. The amount by which the world warms is very nearly linearly proportional to the total amount of carbon burned into the atmosphere from the beginning of the Industrial Revolution to the time that net-zero emissions are achieved, at a rate of about 2 degrees C for each trillion tonnes of carbon burned (see Trillionthtonne.org).\nSome commentators who have positioned themselves as opponents of efforts to reduce carbon dioxide emissions – such as former US Undersecretary of Energy Steve Koonin – have tried to use the need to get to net-zero emissions to justify their position. Their argument goes that reducing emissions will not cool the Earth, so there is no point in doing anything other than adapting to the inevitable warming. This argument was put forth in a recent New York Times op-ed (Koonin 2015 Koonin, S. 2015. “The Tough Realities of the Paris Climate Talks.” International New York Times. November 4. http://www.nytimes.com/2015/11/04/opinion/the-tough-realities-of-the-paris-climate-talks.html?_r=1 .  [Google Scholar] ), compounding displays of ignorance about climate in a lengthy Wall Street Journal essay by Koonin published last year (Koonin 2014 Koonin, S. 2014. “Climate Science Is Not Settled.” The Wall Street Journal, September 19. http://www.wsj.com/articles/climate-science-is-not-settled-1411143565 .  [Google Scholar] ; see response in Pierrehumbert ( 2014 Pierrehumbert, R. 2014. “Climate Science Is Settled Enough.” Slate. October 1. http://www.slate.com/articles/health_and_science/science/2014/10/the_wall_street_journal_and_steve_koonin_the_new_face_of_climate_change.html .  [Google Scholar] ).\nThis fallacy rests on a fundamental misunderstanding of the way carbon budgets affect climate. First, although reducing emissions will not significantly reduce temperature below its current level, it will make the future a lot less warm than it otherwise would have been – although current temperatures are already irreversibly baked in, there is still a chance to avoid additional warming. Further, if we never achieve net-zero emissions, the Earth will continue to warm indefinitely, until unquestionably fatal conditions prevail for large parts of the Earth’s human population and natural ecosystems. The question is not whether we need to achieve net-zero emissions, but how fast we can do it. Reducing the rate of emissions growth is merely the first step toward net zero, and the sooner we make a start the better.\nAnother fundamental misconception in Koonin’s ( 2015 Koonin, S. 2015. “The Tough Realities of the Paris Climate Talks.” International New York Times. November 4. http://www.nytimes.com/2015/11/04/opinion/the-tough-realities-of-the-paris-climate-talks.html?_r=1 .  [Google Scholar] ) New York Times’ op-ed is the claim that keeping a gigatonne of carbon out of the carbon budget has fewer climate benefits when done at a time (such as now) when the cumulative carbon emissions are already large. Here, Koonin is confusing atmospheric concentrations with emissions, in a classic “stock-vs-flow” fallacy – analogous with the confusion between the amount of water in a bucket (the “stock”) and the rate at which the bucket is being filled (the “flow”). It is true enough that increasing the atmospheric concentration of carbon dioxide by one part per million has less warming effect now that concentrations are 400 parts per million than it would have in preindustrial times when concentrations were only 280 parts per million. However, because of chemical limitations on carbon uptake by the oceans, as one adds more and more carbon dioxide into the atmosphere, the proportion that stays in the atmosphere (rather than disappearing into the ocean) increases, and this causes the atmospheric concentration to increase faster than a straight-line prediction as one adds more carbon to the system. This by now well-known effect cancels out the diminishing-returns effect of carbon dioxide concentration on warming, leading to a linear relation between cumulative carbon emissions and warming. A gigatonne of carbon emitted now causes just as much warming as it would have if emitted in preindustrial times.\nIf the world is to keep warming below 2 degrees C, then net carbon emissions must be kept below a trillion tonnes by the time net-zero emissions are achieved. As we have already emitted 600 gigatonnes, that leaves only 400 gigatonnes in the carbon budget (subject to the usual uncertainties relating to climate sensitivity). Divided equally among a present world population of 7.3 billion souls, that works out to a net emission allocation of 55 tonnes per person, which provides a benchmark against which to measure each country’s progress toward net zero. (Note that this 55 tonnes is not an annual or renewable allocation – it is the total net amount of carbon that can be emitted per person alive today over all future time, without breaking the 2-degree limit. That is all there is, and once that allocation is used up, the only choices are to accept greater warming or pray for low climate sensitivity).\nDividing up the remaining carbon budget provides one way of looking at each nation’s fair share of emissions, though an argument could be made in favor of taking into account past emissions as well; by that measure, the United States and Europe already used up their fair share of emissions long ago, and by rights owe the developing world compensation for past overuse of the atmosphere as a dumping ground for carbon. But even if we write off past emissions entirely, and just look at the equal allocation of the remaining carbon budget, Sweden could only continue emitting at its current rate for 44 more years before using up its remaining fair share, so it would need to reach net zero by then. If emissions were ramped down linearly to zero, then Sweden would have 88 years to achieve that goal. That makes Sweden’s commitment to reach net zero by mid-century seem entirely in line with national responsibility, and indeed generous.\nIn contrast, at the current rate that the United States is emitting carbon, our fair share would be used up in about a decade, and it would be necessary to attain net-zero emissions in two decades if a linear ramp-down were to begin today. While the United States squandered 40 years dithering and denying the problem (and much of the rest of the developed world at least dithered), Sweden just got down to the nitty-gritty work of decarbonization and did not wait to see what the rest of the world was doing. If more of the rest of the world had followed Sweden’s example, we would have an easy time keeping warming below 2 degrees C. As it is, even if all nations fulfill the commitments they made before and during the COP21 Paris climate negotiations, we will be hard-pressed to keep warming in 2100 under 3 degrees C, without even factoring in the additional time beyond then needed to get to net zero. It is a tragedy that most of the world wasted the last 40 years, but there is no making up for that. The key thing now is to not waste another 40 years, as the consequences for the climate would be so dire as to pose a truly existential threat to human society and natural ecosystems.\nBut what of China, and indeed the rest of the developing world? Don’t they need vast amounts of cheap fossil-fuel energy to lift their people out of poverty? The telling point here is that Chinese per capita carbon emissions, at nearly 2 tonnes per person, are well in excess of Sweden’s 1.25 tonnes and even in excess of the European Union average, yet China has a GDP per capita of only $6807 – not quite a sixth of Sweden’s. That means that China could halve its per capita emissions while at the same time becoming as rich as Sweden.\nThe Chinese economy contains a vast amount of inefficiency waiting to be mined out. Transitioning to a cleaner energy economy would at the same time go far to address the deadly pollution problems that currently plague the Chinese people. Perhaps this is why China has proved willing to make commitments to emission reduction that, in terms of carbon saved, dwarf what the United States has been willing to do – despite the fact that the moral burden of decarbonization by any measures should fall far more on the United States than on China (Pierrehumbert 2013 Pierrehumbert, R. T. 2013. “Cumulative Carbon and Just Allocation of the Global Carbon Commons.” Chicago Journal of International Law. Vol. 13. http://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1379&context=cjil .  [Google Scholar] ). The lesson for the poorer countries of the developing world, such as India and the African countries, is even clearer: rather than first catching up to China in terms of pollution and then tearing it all down to replace with cleaner technology, wouldn’t it be better to skip the intermediate stage and leapfrog directly to a low-carbon clean energy future?\nThe way forward\nThe Swedish example demonstrates that a combination of well-crafted public policies and private ingenuity can lead to a future that is both low-carbon and prosperous. This is not to say that every country will be able to duplicate that feat by following the exact mix that was successful in Sweden. Given Sweden’s abundant hydropower and biomass relative to its population, the exact mix of policies and energy sources needed to solve the problem will differ from country to country. Some will have more wind, solar, and nuclear in the mix than Sweden, and some nations rich in fossil fuels may find it advantageous to employ more natural gas in conjunction with carbon capture and storage.\nThe point is that this is nothing to be afraid of. It is sometimes made out that revamping global energy systems is a task of monumental proportions, but in fact global energy expenditures are only 10 percent of world GDP and falling, as more and more of the economy becomes based on information rather than energy. This is not a small task, but it is not intrinsically worse than the other major challenges that society has faced and surmounted. And meeting this challenge does not require doing without, or suffering in the cold and dark. It does not require any terrifying revamping of the world economic order. What it does require is a whole lot of boring, unfrightening, and largely routine engineering and construction. It is time to stop quivering in our boots in pointless fear of the future and just roll up our sleeves and build it.\nDisclosure statement\nNo potential conflict of interest was reported by the author.\nReferences\nAndersson, M., and I. Lövin 2015. “Sweden: Decoupling GDP Growth from CO2 Emissions Is Possible.” The World Bank Blog, May 22. http://blogs.worldbank.org/climatechange/sweden-decoupling-gdp-growth-co2-emissions-possible .\n \nGraham, S. 2000. “Svante Arrhenius. NASA Earth Observatory.” January 18. http://earthobservatory.nasa.gov/Features/Arrhenius/ .\n \nKlein, N. 2014. This Changes Everything: Capitalism vs. the Climate. New York: Simon and Schuster.\n \nKoonin, S. 2014. “Climate Science Is Not Settled.” The Wall Street Journal, September 19. http://www.wsj.com/articles/climate-science-is-not-settled-1411143565 .\n \nKoonin, S. 2015. “The Tough Realities of the Paris Climate Talks.” International New York Times. November 4. http://www.nytimes.com/2015/11/04/opinion/the-tough-realities-of-the-paris-climate-talks.html?_r=1 .\n \nKrugman, P. 2011. “Socialist Hellhole Blogging.” International New York Times, August 19. http://krugman.blogs.nytimes.com/2011/08/19/socialist-hellhole-blogging/\n \nLeslie, J. 2006. Deep Water: The Epic Struggle over Dams, Displaced People, and the Environment. London: Picador.\n \nPierrehumbert, R. 2014. “Climate Science Is Settled Enough.” Slate. October 1. http://www.slate.com/articles/health_and_science/science/2014/10/the_wall_street_journal_and_steve_koonin_the_new_face_of_climate_change.html .\n \nPierrehumbert, R. T. 2013. “Cumulative Carbon and Just Allocation of the Global Carbon Commons.” Chicago Journal of International Law. Vol. 13. http://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1379&context=cjil .\n \nTrillionthtonne.org. “Explaining the Need to Limit Cumulative Emissions of Carbon Dioxide.” http://www.trillionthtonne.org/\n \nWorstall, T. 2011. “Paul Krugman and the Socialist Hellhole That Is Sweden.” Forbes, August 20. http://www.forbes.com/sites/timworstall/2011/08/20/paul-krugman-and-the-socialist-hellhole-that-is-sweden/ .\n \n""","0.13960722","""http://www.tandfonline.com/doi/full/10.1080/00963402.2016.1145908""",
"""Imperial_College_London""","""Spinal Cord - Guidelines for the conduct of clinical trials for spinal cord injury (SCI) as developed by the ICCP panel: clinical trial outcome measures""","""Review\nSpinal Cord (2007) 45, 206–221. doi:10.1038/sj.sc.3102008; published online 19 December 2006\nGuidelines for the conduct of clinical trials for spinal cord injury (SCI) as developed by the ICCP panel: clinical trial outcome measures\nJ D Steeves 1 , D Lammertse 2 , A Curt 1 , J W Fawcett 3 , M H Tuszynski 4 , J F Ditunno 5 , P H Ellaway 6 , M G Fehlings 7 , J D Guest 8 , N Kleitman 9 , P F Bartlett 10 , A R Blight 11 , V Dietz 12 , B H Dobkin 13 , R Grossman 14 , D Short 15 , M Nakamura 16 , W P Coleman 17 , M Gaviria 18 and A Privat 18\n1ICORD, University of British Columbia (UBC) and Vancouver Coastal Health (VCH) Research Institute, Vancouver, BC, Canada\n2Craig Hospital, Englewood, CO, USA\n3Cambridge University Centre for Brain Repair, Robinson Way, Cambridge, UK\n4Center for Neural Repair, University of California at San Diego, La Jolla, CA, USA\n5Jefferson Medical College, Thomas Jefferson University, Philadelphia, PA, USA\n6Department of Movement and Balance, Division of Neuroscience and Mental Health, Imperial College London, Charing Cross Campus, London, UK\n7University of Toronto, Krembil Neuroscience Center, Head Spine and Spinal Cord Injury Program, Toronto Western Hospital, Toronto, Ontario, Canada\n8Department of Neurological Surgery and the Miami Project to Cure Paralysis, Lois Pope LIFE Center, Miami, FL, USA\n9National Institute of Neurological Disorders and Stroke, NIH, Bethesda, MD, USA\n10Queensland Brain Institute, University of Queensland, St Lucia, Queensland, Australia\n11Acorda Therapeutics, Hawthorne, NY, USA\n12Spinal Cord Injury Center, Balgrist University Hospital, Zurich, Switzerland\n13Department of Neurology, University of California Los Angeles, Geffen School of Medicine, Neurologic Rehabilitation and Research Program, Los Angeles, CA, USA\n14Baylor College of Medicine, Department of Neurosurgery, One Baylor Plaza, Houston, TX, USA\n15Midlands Centre for Spinal Injuries, Robert Jones and Agnes Hunt Orthopaedic and District Hospital NHS Trust, Oswestry, Shropshire, UK\n16Department of Orthopaedic Surgery, Keio University, School of Medicine, Tokyo, Japan\n17WPCMath, Buffalo, NY, USA\n18Institut des Neurosciences – CHU St Eloi, INSERM U-583, Montpellier, France\nCorrespondence: J Steeves, ICORD, University of British Columbia (UBC) and Vancouver Coastal Health (VCH) Research Institute, c/o 2469-6270 University Boulevard, Vancouver, BC, Canada V6T 1Z4\nTop of page\nAbstract\nAn international panel reviewed the methodology for clinical trials of spinal cord injury (SCI), and provided recommendations for the valid conduct of future trials. This is the second of four papers. It examines clinical trial end points that have been used previously, reviews alternative outcome tools and identifies unmet needs for demonstrating the efficacy of an experimental intervention after SCI. The panel focused on outcome measures that are relevant to clinical trials of experimental cell-based and pharmaceutical drug treatments. Outcome measures are of three main classes: (1) those that provide an anatomical or neurological assessment for the connectivity of the spinal cord, (2) those that categorize a subject's functional ability to engage in activities of daily living, and (3) those that measure an individual's quality of life (QoL). The American Spinal Injury Association impairment scale forms the standard basis for measuring neurologic outcomes. Various electrophysiological measures and imaging tools are in development, which may provide more precise information on functional changes following treatment and/or the therapeutic action of experimental agents. When compared to appropriate controls, an improved functional outcome, in response to an experimental treatment, is the necessary goal of a clinical trial program. Several new functional outcome tools are being developed for measuring an individual's ability to engage in activities of daily living. Such clinical end points will need to be incorporated into Phase 2 and Phase 3 trials. QoL measures often do not correlate tightly with the above outcome tools, but may need to form part of Phase 3 trial measures.\nKeywords:\nTop of page\nIntroduction\nThe second International Campaign for Cures of spinal cord injury Paralysis (ICCP) Clinical Guidelines Panel meeting focused on the outcome measures to be used during spinal cord injury (SCI) clinical trials for the evaluation of a therapeutic intervention. Given the small number of clinical trials that have been undertaken for SCI, it is not surprising that until now there has been little opportunity to develop agreement as to the most appropriate and accurate clinical end points (ie outcome measures) for demonstrating the efficacy of an experimental therapeutic intervention. 1 The various possible outcome measures with their advantages and disadvantages are reviewed in this article.\nChallenges for assessing SCI outcomes or benefits of therapeutic interventions\nIn terms of designing a specific SCI clinical trial with the most accurate assessment of neurological or functional outcome, a consideration of the following issues is suggested:\nPhase of clinical trial, as primary and secondary outcome measures and thresholds are likely to differ or evolve from Phase 1 (safety) to Phase 3 (therapeutic confirmatory trials).\nLevel of spinal injury, including the extent of the zone of partial preservation (ZPP).\nSeverity of spinal injury (varying degrees of incomplete to complete sensorimotor loss).\nTime since injury (early acute to late chronic; ie from unstable to more stable functional capacities after SCI)\nAppropriate nature of outcome measure to the capacity or capability being evaluated (eg sensorimotor impairment, autonomic function, personal functional capacity, performance, or community participation). Different clinical targets normally require distinct outcome assessment tools.\nSensitivity of outcome measure (ie detection threshold).\nAccuracy and validation of outcome assessment tool.\nReliability of measurements between assessments by a single investigator and between investigators (ie intra- and inter-rater reliability).\nFeasibility for using selected outcome measurement tools in a particular center or across multiple centers.\nAdoption of standardized outcome assessment procedures and data sets across multiple trial centers.\nWe will discuss these and other influences as they impact the selection of outcome measures for SCI clinical trials.\nCategories of outcome assessments\nAssessment methodologies for evaluating a clinical end point for an SCI trial fall into three main categories:\nAssessments aimed at describing the neurological connectivity of the spinal cord, irrespective of the ability of the patient to functionally use those connections in everyday activity. The American Spinal Injury Association (ASIA) scale would be an example of such an assessment. This would also include assessments of neurological capacity that are independent of the environment (eg electrophysiological recordings or imaging assessments). If these outcome tools can be shown to accurately predict the long-term functional benefits (clinical endpoints) resulting from a therapeutic intervention, they can also be thought of as surrogate end points.\nAssessments of the abilities of a patient with SCI to perform activities associated with everyday life. Examples would be the Functional Independence Measure (FIM) and the Spinal Cord Independence Measure (SCIM). Functional evaluations may be a more direct measurement of a clinically meaningful change in the functional capacity of a study subject, but the changes in functional outcomes may not always be the result of a demonstrated change in spinal–neurological activity or connectivity. In short, any change in a person's functional capacity after SCI may be due to adaptive changes (or plasticity) within and/or without the central nervous system (CNS), including environmental accommodations and/or alternative compensatory strategies.\nAssessments of an individual's level of participation in societal activities. Quality of life (QoL) can be defined as a person's perception of his position in life, within the context of both his personal and society's values and culture, and relate to his personal concerns, standards and goals. The short form 12- or 36-item medical outcomes health survey (SF-12 and SF-36) are examples of a QoL survey.\nImprovement of functional abilities, reflected in activities of daily living (see above) will be the most meaningful and valued outcomes. However, the early phase clinical trials (Phase 1 and 2) that have been completed to date (using pharmaceutical therapeutics), have focused on assessment of neurological connectivity to provide 'proof of principle' measures. It is likely that neurological assessments will continue to be used as primary outcome measures, indicating the likelihood that a treatment will improve the functional capacities and performance of a subject in later phases of clinical studies. However, no experimental intervention will be considered effective for the treatment of people living with SCI unless it improves their ability to function and engage in everyday life within their society. Outcome assessment tools that accurately and sensitively demonstrate such benefits will need to be incorporated into the more definitive and confirmatory Phase 3 clinical trials.\nClinical trial phases and corresponding categories of outcome measures\nPhase 1\nThe objectives of Phase 1 trials can be quite varied, from the initial exploration of tolerability, through study of human pharmacokinetics and metabolism, to identification of the maximum safely tolerated dose of a candidate therapeutic (see also Lammertse et al 2 ). A Phase 1 trial is specifically designed to evaluate the safety of the intervention and expose any adverse or toxic side effects, usually in small numbers of subjects with a simple open label design. Participant's who choose to take part in a Phase I trial may experience significant risks with a limited probability of receiving individual benefit. Preliminary Phase 2 (proof of concept or evidence of activity) data are sometimes collected during a Phase 1 trial, but only to develop a preliminary sense of potential efficacy and to assist in the identification of appropriate outcome measures to be used in subsequent properly powered Phase 2 or 3 trials. Many of the currently conceived therapeutics for the possible treatment of SCI involve an invasive intervention, such as the direct infusion of a drug or cellular transplant into, or around the injured spinal cord. As a consequence, healthy volunteers (without SCI) are unlikely to be recruited for a Phase 1 SCI clinical trial of this type.\nSCI is a heterogeneous disorder in terms of level of spinal injury, severity of injury and timing of treatments after injury. Some types of SCI (eg central cord syndrome and cauda equina injuries) have higher spontaneous rates of overall sensory and motor recovery. Thus, they may not be the best subjects to be included with other types of traumatic SCI during a Phase 1 or Phase 2 trial, as they could increase the variability of the outcome data. They may also be inappropriate, based on the proposed mechanism of action for the experimental intervention.\nPatients with complete ASIA A thoracic injuries are frequently suggested as being the 'preferred' group of SCI participants for early phase SCI clinical trials. By confining the administration of the experimental therapeutic to the thoracic cord, it is probable that any adverse effects on spinal function would not seriously alter a person's functional capabilities (ie not spread to more rostral cervical levels and compromise arm, hand or respiratory function). Complete ASIA A, thoracic-injured patients are a small proportion of total SCI cases, and there are, as yet, no validated outcome measures for changes in thoracic cord motor function (although some are under development, see below). Sensory function can be evaluated using the ASIA examination or other measures.\nGeneral Phase 1 trial safety outcome measures include: ongoing assessment of standard vital signs, physical examination data (eg temperature, respiration, heart rate, and blood pressure), clinical laboratory tests (eg hematology and urine analysis), as well as the appearance of any systemic adverse event (observed or reported by a trial subject). Depending on the therapeutic drug or cell line being evaluated and the route of administration, other Phase 1 safety outcome measures may include the evaluation of unintended effects on the CNS or other body tissues, including infection, inflammation, or immune reactions.\nA more specific measure of neurological state is the ASIA assessment 3 to determine whether there is any change in neurological level or any sensorimotor deterioration, as well as to subsequently track any changes in the ASIA score. An improvement in ASIA scores is a possibility during a Phase 1 trial, indicating possible efficacy of the treatment, but this is not the primary reason for including an ASIA assessment at this stage of clinical study. An ASIA assessment, just before randomization of a subject to a clinical trial study arm, can be most useful to assure that the candidate meets all inclusion criteria and whether the participating subjects should be stratified (into a sub-category) on the basis of their ASIA score, so only appropriately matched experimental and control subjects are compared thereafter.\nInclusion of ongoing standardized ASIA assessments is warranted on the grounds that this examination: (1) has been widely adopted throughout the world, enabling the comparison of data between centers, (2) can be readily undertaken with a minimum of equipment, and (3) can provide important reference data between different phases of a clinical trial or with previous trial (historical) data. In several previous randomized control trials (RCT), 4 , 5 , 6 , 7 , 8 motor and sensory assessments, comparable with the current ASIA standards have been used as an overall indicator of the general severity of neurological impairment after SCI (especially in terms of segmental motor function, see below).\nLater in this document, we will discuss when and how often an ASIA assessment should be undertaken, the strengths and limitations of the ASIA examination, the separation of upper and lower limb ratings, as well as the intra- and inter-rater reliability of the ASIA assessment (see below).\nPhase 2\nDuring a Phase 2 study (sometimes referred to as the Proof of Concept level), an exploratory evaluation of efficacy becomes more prominent, with the objective of determining potential effect size and variability of an experimental therapy in comparison to a useful control group. Information is gained regarding choice of optimal end points for a larger Phase 3 confirmatory trial of efficacy. During a Phase 2 trial, additional information is also obtained regarding safety. Combined Phase 1/2 trials, where safety and bioactivity of the therapeutic are evaluated together can often occur when the Phase 1 trial does not involve healthy subjects and is restricted to people having the clinical disorder. It is possible for SCI clinical trials to be designed in this manner. Nevertheless, the data from such a combined Phase 1/2 trial must be able to satisfy the essential outcome end points for each respective trial phase.\nThe preferred Phase 2 design would be a RCT where each participant is recruited prospectively and randomly assigned to either the experimental or control arm of the study and where the investigators and, if at all possible the participants, are blinded to which study arm they have been assigned. If available, Phase 2 trials could employ surrogate end points, which are expected to be predictors of functional improvement, to estimate presumed effective doses, and to allow trials of shorter duration and smaller size to be conducted.\nPhase 3\nPhase 3 (therapeutic confirmatory) trials are generally the definitive clinical trial phase and typically undertaken as a RCT. The object is to confirm the preliminary evidence obtained at the Phase 2 stage with a statistically significant clinical benefit of the therapeutic in a wider group of subjects across multiple study centers. For a more detailed discussion of Phase 3 and Phase 4 trial stages, see accompanying article – SCI Guidelines 4 (Lammertse et al 2 ).\nSCI therapies conceived as early interventions or acute stage treatments are likely to be administered within days of spinal injury and it is important that the outcome tools have the ability to accurately and sensitively track meaningful changes across a broad chronological timeframe. Several assessment tools are available or are being developed, each with their individual strengths and limitations. We will discuss each separately.\nASIA impairment scale\nASIA assessments\nAs mentioned above, the ASIA Impairment Scale has become a standardized and routinely adopted classification for most patients suspected of suffering a SCI. 3 It is especially useful for classification of motor-complete and sensory-complete SCI (ASIA A) as well as motor-complete, sensory-incomplete SCI (ASIA B). During the acute stages of SCI, there have been concerns about how soon after injury the ASIA examination can provide useful prognostic information about the eventual degree of impairment. It has been argued that an ASIA assessment within the first 24 h may not provide an accurate prognosis and that a later 72 h examination is a more reliable indicator, as the patient is medically more stable. 9 , 10 , 11 At chronic time points (greater than 12 months after SCI), the ASIA assessment may not capture the most important aspects of functional changes after SCI. Nevertheless, it is still valuable for classifying and stratifying participants for a clinical trial. Functional tests (see below) are perhaps more useful primary outcome tools for chronic studies.\nRegardless of these concerns, it is essential that steps should be taken to standardize and optimize the accuracy of the ASIA assessment. For all patients being considered for entry into a trial, the clinical trial center(s) must conduct an independent and blind ASIA assessment, just before randomization to the therapeutic intervention or relevant control treatment. Subsequent follow-up ASIA assessments should also be undertaken at relevant time points over the course of recovery, as defined for that trial (eg first few weeks, first couple of months, and then at fixed intervals, every few months, throughout the duration of the study) in the same blinded fashion, and preferably by the same examiner. In the absence of a more sensitive and accurate outcome tool, such ASIA assessments enable any initial detriments or benefits to be identified and followed.\nThe Panel strongly recommends that ASIA assessors undergo standardized training with an intra- and inter-rater reliability test being completed at the end of the training session. Follow-up training of the same examiners should be undertaken at reasonable intervals (eg every 6–12 months) by the same qualified trainers. This is especially important when it is necessary to undertake the clinical trial at more than one site. Although the ASIA assessment paradigm seems simple in its description, experience has indicated that rigorous adherence to the definitions, based on training, is necessary to obtain consistent data that can be meaningfully compared both within and across clinical studies or centers.\nPrevious SCI clinical trial experience 4 , 5 , 6 , 7 , 8 suggests that requiring the improvement of one or two ASIA grades over and above spontaneous recovery (eg ASIA B to ASIA C or ASIA D), as a primary outcome end point (to document the benefit of a therapeutic intervention), may be too demanding a threshold (ie is a relatively insensitive measure for a therapeutic effect). A candidate therapeutic with a very large effect size could be addressed with such a challenging clinical point. However, an intervention with a potentially smaller effect size might require a more sensitive outcome measure, such as a statistically significant change in ASIA motor score.\nASIA motor score\nIn many respects, the ASIA motor score is considered more reliable than the ASIA sensory score in predicting functional outcome after SCI. 12 The Panel recommends that upper and lower limb motor scores should be compiled separately as the upper-extremity motor score (UEMS) and lower-extremity motor score (LEMS). This enables a change in motor function to be more clearly tracked and recorded as specific to either the cervical or lumbar levels ( Table 1 ). Separation of the motor scores into UEMS and LEMS also reduces the influence that a large change in the functional strength in one or a few muscles might have on the interpretation of therapeutic benefit.\nFull table\nIn general, establishing a functionally meaningful ASIA motor score threshold to document the benefit of a therapeutic intervention is dependent both on the level and severity of the SCI, 13 as well as the degree of spontaneous recovery after SCI with conventional treatment ( Table 2 and Fawcett et al 11 ). As shown in Table 2 , previous studies have indicated that a low-cervical, ASIA A-injured patient is likely to spontaneously improve about 10 ASIA motor points during the first year after SCI. 7 , 8 , 14 , 15 Thus, to demonstrate the efficacy of a therapeutic intervention, a response to treatment of an additional 10-point improvement in the ASIA motor score (efficacy threshold now being 20 point) might be considered a valid primary outcome end point (cf Fawcett et al 11 ).\nFull table\nDifferent efficacy thresholds would need to be specified for a response at each level and severity of SCI. For example, the spontaneous recovery of ASIA B cervical patients, 1 year after a cervical SCI, has been reported to be about 30 motor points ( Table 2 ), and thus might require an additional 20 point improvement to indicate a clinically meaningful benefit for an intervention. Such a threshold would allow demonstration of benefit with a reasonable number of trial subjects. However, these requirements could be complicated by a 'ceiling' in ASIA motor scores. As no ASIA motor score is collected between T2 and L1, only a physiological assessment of motor connectivity could be reliably undertaken with the thoracic region (see below). It should be noted that the absolute difference in the number of ASIA motor points between an experimental and appropriately matched control group is not as important as whether a statistically valid difference exists and whether that magnitude of difference confers a clinical benefit (ie an improved functional outcome) to the person with SCI.\nFinally, several studies have reported a substantial (25–50) motor point improvement over the first year after SCI for people with ASIA C and D classifications ( Table 2 ), which is on top of their initial ASIA motor score. Thus, an ASIA motor score 'ceiling effect' may make it difficult to discriminate a statistical difference between the ASIA motor scores of SCI participants in the experimental and control arms of a study. In short, the spontaneous ASIA motor score may become so high within the recovery period that a treatment effect will not be detectable. Therefore, a functional test (see below) may be a more appropriate primary outcome tool for ASIA C and ASIA D trial participants.\nStatistically speaking, the use of ASIA motor scores as a primary outcome end point is perhaps most useful for SCI subjects initially enrolled in a clinical trial as either ASIA A or ASIA B. The obvious drawback for ASIA A and ASIA B subjects is that they initially have motor-complete spinal injuries and it may be difficult to produce or discern a clinically meaningful improvement in their ASIA motor score.\nFor reasons arising from the underlying physiology and the natural history of spontaneous recovery, the ASIA motor scores may not always represent a normal, bell-shaped curve and this may make normal-theory statistical procedures like the t-test and analysis of variance incorrect in small samples. As different inclusion and exclusion criteria can affect the representation of these subgroups in the total composition of the study sample (cf Tuszynski et al 16 ), estimates of the standard deviation based on one trial may be inaccurate in predicting the standard deviation in a new trial. In a large sample, the number of patients with low or zero change in the ASIA motor scores, can skew the distribution to the left and leave a large peak. In any case, the changes can still show 'ceiling' effects in people with mild SCI.\nThese technical statistical problems suggest why it may sometimes be attractive to use a binary (success/failure) criterion as a trial's primary outcome measure, rather than an ordinal variable like the ASIA motor score. Although binary variables always have a completely known, parametric probability distribution that can be used by statisticians confidently, they are likely to mask underlying clinical complexities and/or variability.\nSome reports have expressed the ASIA motor score as the 'percent deficit recovered.' Although this strategy has an appealing rationale, it also has a potential danger. It may be that a mild SCI injury, with only a few points in ASIA motor deficit, has a larger chance for spontaneous recovery. Thus, this method would allow mildly injured patients to have disproportionate weight in one direction, whereas patients with severe motor deficit would count heavily in the other direction as they are least likely to improve. The method of presenting the ASIA score as the number of motor points changed from baseline can give more potential weight to the severely spinal injured group (whether you use an individual baseline or the mean of the subgroup), as they have numerically more room to improve. Perhaps the best solution might be to use the number of motor points changed, but to compensate by stratifying the subject population into cohorts or subgroups on the basis of the initial classification of ASIA impairment scale (AIS) severity.\nAdjusting for baseline differences has been used, as in the NASCIS III study. 6 Simply introducing a baseline term in an analysis of covariance may not be sufficient, as the amount of correction required may be different for patients with mild, moderate, and severe SCI in a manner that is not linearly proportional across the range of SCI severity. Also, this introduces a mathematical relation between the outcome variable (the change in ASIA motor score) and the predictor (the initial baseline score) that could make the envelope of data points depart from the commonly assumed model, where the scatter of the data above and below the regression line has a normal distribution with uniform variance.\nThe outcome of a trial can depend strongly on its mixture of population subgroups and clinical covariates (also see Lammertse et al 2 ). In order to design a clinical trial properly, it is important to recognize and distinguish the different questions and problems: (1) as the natural history (ie spontaneous recovery) is different for different SCI severities. For example, if more patients with AIS grade A and fewer patients with grade C are disproportionately assigned to the test treatment, then that treatment will appear artificially of less benefit as ASIA grade A subjects will probably always exhibit the smallest treatment effect, (2) even if the outcome of the trial is positive, any randomization imbalance will provide ammunition for skeptics to find post hoc rationalizations for disbelieving otherwise sound results, (3) even if there is no randomization imbalance at all, there is still the possibility that the test treatment will be less effective in certain groups. For example, it is likely that the target and functional recovery mechanisms available in a subject with an ASIA C injury will differ from those in a patient with an ASIA A injury, (4) even aside from the question of power it may be scientifically and clinically important at the end of the trial to know if there are effect differences among identifiable cohorts or subgroups, and (5) any result is more scientifically credible if hypothesized in advance than if found ad hoc or post hoc. Therefore, the most important covariates should be identified during trial design and included in the primary analysis. Indeed, a major purpose of the current series of papers is to provide designers with historical data that can be used in calculations, sensitivity analyses, and simulations that can help a designer to determine whether a planned trial is likely to succeed (see Lammertse et al 2 ).\nThere are three means available to deal with covariates and subgroups: (1) to include them as strata in a block randomization, (2) to model them as explicit terms in the trial's single, prospectively specified 'primary efficacy analysis', and (3) to include them in prospectively specified secondary analyses. None of these approaches is unrestrictedly useful and trial designers should probably employ all three.\nStratified randomization only protects against randomization imbalance, not differences in effect size. Also, it would be a bad idea to include too many factors as strata, as, if the block size becomes large compared to the recruitment at the individual centers, too many incomplete blocks will be left at the end of the trial and this would precisely defeat the purpose.\nIdentifying and restricting the number of study covariates to a small number normally has the effect of increasing power (in the overall test, rather than in the individual subgroup tests) and therefore decreasing the necessary number of study subjects (ie sample size). In general terms, unexplained variability is reduced when individuals are considered within their own more homogeneous subgroup, and this increases statistical power. However, if relatively unimportant covariates are included 'for completeness,' then statistical tests will exact a penalty and the power will actually be less and not more. Also, as the number of factors rises, it may require very considerable skill in analysis and interpretation to tease out any treatment effect.\nGiven the small number of SCI clinical trials completed to date, identifying important covariates is not yet an exact science (cf Tuszynski et al 16 ). We have reanalyzed some of the GM-1 trial data and found that baseline AIS is a very strong covariate as is the level of injury (eg cervical or thoraco-lumbar). Certain types of spinal injuries (eg a suspected central cord or conus injury or one not involving a fracture dislocation) have a prognostic value (usually for a significant spontaneous functional recovery). Younger patients with incomplete injuries recover better than older ones; but younger patients tend to be more severely injured so that, on the whole, their recovery is no better. Other possibilities (use of spinal surgery or direct admission to tertiary care) did not have a readily detectable effect in the GM-1 study. 7 , 8\nThe ICCP Clinical Guidelines Panel is continuing to examine the raw data from previous SCI trials to determine if a valid therapeutic threshold for ASIA motor scores can be established for different levels and severities of SCI.\nZone of partial preservation\nBelow the most caudal 'functional' ASIA motor level (ASIA motor grade of 3, 4, or 5) the ZPP consists of those myotomes and dermatomes that remain partially innervated ( Table 1 ), but at a level that may not be functionally meaningful (eg ASIA motor grade of 1 or 2). The exact numbers of segments, so affected, make up the ZPP. The term is used only when there is a motor-complete spinal injury. As outlined in the preceding article, 11 it is often difficult to discern the mechanism underlying any neurological or functional improvement when it occurs within the ZPP; it could be due to central repair (plasticity, sprouting, or regeneration) and/or due to similar peripheral modifications, such as peripheral sprouting, as some muscles are innervated from multiple spinal segments. 17\nThere is little doubt that improved recovery of function within the ZPP can provide new and meaningful capabilities for a person with SCI, especially those individuals with a cervical level injury. All the same, the ZPP can also complicate the accurate interpretation of therapeutic action because the extent of recovery within the ZPP can be variable. Spontaneous changes within the ZPP introduce 'background noise' into the determination of therapeutic efficacy. There was general agreement that functional changes within the ZPP need to be interpreted with caution. 18 Any improvement in function ascribed to an experimental intervention that is confined to the first two segments caudal to the last functional ASIA motor level may be due to plastic changes within the ZPP rather than to the formation of new spinal connections across the level of injury. Furthermore, there was recognition that in many previous therapeutic studies clear chronological description of ZPP function has been lacking; future trials should make provision to clearly describe changes in segments adjacent to the level of spinal injury.\nASIA sensory score\nThe lack of sophistication of the ASIA sensory score for accurately describing preserved sensory levels after SCI or as a valid outcome measure has long been recognized. The ordinal 3-point scale for light touch (normal, abnormal, or absent) is highly variable at different assessment times and between ASIA assessors. The ASIA pin-prick score appears to be the more useful clinical measure of preserved spinal sensory function (eg sacral sparing in people with an ASIA B classification), as well as a predictor for future recovery. 19 , 20 The ASIA light touch score does not necessarily correlate with subsequent sensory functions accurately and does not seem to be particularly useful as an SCI clinical trial outcome measure.\nTop of page\nQuantitative sensory testing\nQuantitative sensory testing (QST) is emerging as a potential adjunct to the neurological exam in the evaluation of sensory dysfunction after SCI. 21 , 22 , 23 Commonly, QST has used quantitatively controlled thermal (warm and cool), mechanical (monofilaments/von Frey hairs) and vibratory stimuli (eg 100 Hz) with psychophysical scaling against established normative values, to differentiate the contributions from small and large diameter peripheral sensory afferent projections or distinguish the contributions of ascending spinal sensory pathways (spinothalamic and dorsal columns, respectively). QST measures appear to correlate with somatosensory-evoked potential (SSEP) recordings and with ASIA sensory scores.\nAlthough further validations of QST techniques are required, QST appears to be a more sensitive technique than the ASIA sensory score, but it is a time-consuming evaluation. With repeated measures, QST might be considered as a secondary outcome measure of spinal cord function. Nevertheless, the Panel currently has more confidence in the sensitivity, accuracy, reliability, and reproducibility of motor function tests than in QST, primarily because QST can be a lengthy procedure with a number of highly variable stimulation parameters. A recent simple adjunct for the sensory evaluation of SCI, which overcomes some of the complexities of the QST, is the electrical perceptual threshold (EPT) test. 24 EPT supplies a measure of sensory threshold for each dermatome and provides a more quantitative map of the level and completeness of SCI, including the ZPP. 22 , 25 , 26\nTop of page\nElectrophysiological assessments\nElectrophysiological measurements such as SSEP, electromyographic (EMG), and motor-evoked potential (MEP) recordings provide objective data (latencies and amplitudes) for assessing spinal conductivity that can be analyzed by a blinded investigator in the form of truly quantitative values, in contrast to measures such as the ASIA scores that are a nonlinear ordinal scale. 27 , 28 , 29 , 30 , 31 Furthermore, electrophysiological recordings have the advantage that they can be performed on comatose or otherwise unresponsive subjects. EMG recordings are useful in the assessment of function, both in response to voluntary effort or when combined with electrical or magnetic stimulation of peripheral nerves (reflexes) or motor cortex (ie MEP).\nComplementary to the neurological assessment, a combination of SSEP, MEP and/or EMG measurements provides information about spinal cord function that is not retrievable by other clinical means and may have additional value in predicting functional outcomes. 32 , 33 Changes in conduction velocity and the magnitude of the compound action potentials, as an outcome measure, must be interpreted with caution. An increased conduction velocity may accurately reflect a remyelination of fiber tracts, which could be the targeted aim of a SCI trial, but in itself, may not herald the recovery of function or improvement in neurological condition. 34 , 35 Strong correlations between AIS scores and electrophysiological measurements are not always evident. 36 In general, the Panel felt that electrophysiological measures were most useful when combined with other outcome tools and could be useful in determining the mechanism of therapeutic action. 37 , 38\nAssessment of thoracic cord function\nCurrently, there are no agreed methods for assessing motor levels in the thoracic cord, although sensory levels are assessed during the standard ASIA examination. This is a significant problem for determining the potential efficacy of an intervention, given the expectation that it is safer to perform initial human studies in patients with a thoracic level injury. The electrophysiological studies described in recent papers 22 , 39 provide methods aimed at detecting changes in motor and autonomic function, as well as providing information on the level and completeness of injury to the thoracic cord. Motor assessments have been developed using transcranial magnetic stimulation to elicit MEPs in paraspinal, intercostal, and abdominal muscles. Quantitative measures that appear to be promising include: thresholds, latencies, and recruitment (input/output curves) of MEPs from trunk muscles innervated at different thoracic levels. 40 Mechanically evoked reflexes, recorded as EMGs in paraspinal muscles, also show abnormalities directly related to the level of spinal injury. 40\nIn summary, these tests may be used to indicate functional improvement or deterioration following treatment. However, the innervation of trunk muscles by multiple thoracic spinal levels means that the resolution of these motor techniques is not as precise as might be achieved in the cervical cord. The tests may be used to indicate motor level within two or three levels (plus or minus).\nTop of page\nAutonomic function testing\nThe accurate evaluation of impaired autonomic nervous system (ANS) function after SCI is currently limited. In addition to the motor and sensory deficits associated with SCI, coincident ANS impairments are common (cf Claydon et al 41 ). Individuals with SCI often exhibit autonomic dysreflexia, which results in episodes of uncontrolled hypertension. The recognition and management of cardiovascular dysfunctions following SCI represent challenging clinical issues, as well as important therapeutic targets since cardiovascular disorders in the acute and chronic stages of SCI are the cause of death in individuals with SCI. 42 , 43\nAs sympathetic vasomotor control is disrupted below the level of a complete sensorimotor SCI lesion, reflex vasodilatation owing to local heating of the skin in people with chronic SCI is diminished. 44 Thus, it has been suggested that assessment of reflex vasodilatation may be a useful noninvasive outcome measure to detect the preservation of any central autonomic pathways after SCI and possibly to document any change in spinal autonomic functions after a therapeutic intervention. 23 , 45\nTracking standard vital signs is imperative throughout the entire phase of any clinical trial, especially as the influence of the ANS on any of these measured functions is well established. Interestingly, measurement of the sympathetic skin response (SSR) has been suggested to delineate the level and extent of spinal sympathetic function, as a measure of autonomic dysfunction. 22 , 32 , 41 , 46 It may reveal an incomplete lesion in terms of autonomic function in cases of complete motor and sensory injury. 47 However, SSR remains a controversial measure 22 of overall spinal function and, if adopted as an outcome measure, should be limited to testing the efficacy of an intervention on ANS function and used in conjunction with a number of other outcome measures. Further development of valid outcome tools for the assessment of ANS function after SCI is imperative.\nTop of page\nImaging assessments\nMagnetic resonance imaging (MRI) has become a cornerstone of radiologic technique to detect the location (and to some degree the severity) of an acute SCI, as well as to detect possible complications arising during chronic SCI, such as syringomyelia. At present, MRI along with computerized axial tomography and X-ray images are useful diagnostic tools and potentially helpful for screening participants to be included or excluded from a clinical trial.\nMRI has been useful in determining the extent of cord compression, 48 , 49 , 50 outlining hemorrhages and edema after human spinal injury and in the near future, might be useful in monitoring progressive changes in spinal cord tracts, such as demyelination after spinal injury. Indeed, recent data from the Spine Trauma Study Group, indicates that the extent of cord compression and the presence of hemorrhage and cord swelling are highly predictive of ASIA motor score outcomes at one, 1 year post-SCI. 50\nMRI has also been proposed as a potential SCI assessment tool after a therapeutic intervention, and as a means of tracking implanted cells. In experimental models of SCI, diffusion tensor imaging (DTI) can delineate both disrupted and intact axonal fiber tracts within the spinal cord, as well as the orientation of glial scarring surrounding a spinal lesion. 51 With further development, MR technologies may develop a useful early 'surrogate' end point measure that would accurately predict the long-term functional benefits of an experimental intervention after SCI (cf Schwartz et al 51 ).\nNevertheless, MRI is still largely a qualitative measure and quantitative standards, in relation to functionally measured SCI outcomes, will need to be developed and validated before MRI can be used as an outcome tool (cf Miller 52 ). It is hoped that MRI and Magnetic Resonance Spectroscopy technologies will rapidly mature, with more sophisticated algorithms (including DTI and functional MRI), such that imaging will become a valuable non-invasive assessment tool.\nFunctional tests\nGeneral considerations\nFor chronic SCI studies (greater than 12 months after initial SCI), ASIA assessments may not be a sufficient tool as an outcome measure, especially for studies on incomplete SCI where the ASIA motor score is likely to be substantial and highly variable between individuals. Nevertheless, an ASIA assessment, before randomization, is valuable for classifying and stratifying participants in a clinical trial. At acute and sub-acute stages after SCI, the value of functional outcome tools is less clear, especially for motor-complete SCI (ASIA A and ASIA B), which are likely to be the initial subjects in early Phase trials. If the expected therapeutic benefit is modest, a dramatic improvement in functional performance may not be readily evident. Nevertheless, functional outcome assessments should be undertaken as a secondary outcome measure.\nThere was agreement from the ICCP Clinical Guidelines Panel that an improvement in the measurable performance of meaningful function is necessary for any therapeutic intervention to be universally accepted as beneficial (for a review, see Ditunno et al 53 ). The World Health Organisation (WHO), specifically the International Classification of Functioning, Disability and Health (or ICF), has rigorously defined function and impairment, as well as activities of life and disability (see below). ICF-1 is a health sphere of influence classification system that describes, among other things, body functions and structures, activities and participation. In short, reduced function in a body structure can result in difficulty executing an activity of daily living.\nICF complements the WHO's International Classification of Diseases (ICD; eg latest version is ICD-10) and is currently being reviewed for the next iteration, ICF-2. ICF is useful to understand and measure functional outcomes after SCI and all clinical researchers are encouraged to become familiar with these classifications and definitions ( http://www3.who.int/icf/ ).\nLower limb function\nFor clinical trials involving people with motor-incomplete SCI (ASIA C and ASIA D), at acute, subacute, and chronic SCI stages, several validated tests of ambulatory performance have been developed, including the Walking Index for Spinal Cord Injury (WISCI) and a number of timed walking tests. 54 , 55 WISCI is a 21-level hierarchical scale of walking based on physical assistance, need of braces and devices, with an ordinal range from 0 (unable to walk) to 20 (walking without assistance for at least 10 m). It is an example of a more sensitive and precise scale for rating a specific functional activity in people with incomplete SCI. WISCI is currently a valid outcome measure for strategies directed to improve ambulation by subjects with incomplete SCI. 54\nAlthough the WISCI has been validated as a qualitative outcome measure for the assessment of standing and walking after incomplete SCI, the opinion of the ICCP Clinical Guidelines Panel is that a more accurate assessment may be provided by a combination of WISCI and some of the more quantitative timed walking tests. Such quantitative walking tests include the timed up and go, time taken for a 10-min walk test (10 MWT) or one of the many similar variants (25, 30 ft, 8 m) and the distance traversed during a 6-min walk test. 55 There may be some redundancy between tests like the 10 MWT and the 6-min walk and it may be pragmatically easier to undertake a short timed walk test as the more routine walking assessment, especially in trials that involve centers, which may not have adequate facilities for measurement of longer duration walks.\nUpper limb function\nThe number of people surviving with a cervical level spinal injury has risen dramatically over the past few decades and cervical SCI now accounts for approximately 50% of all people living with a SCI. Thus, validating a functional outcome tool to assess arm and hand capacity after a cervical spinal injury was identified as a top priority by the Panel.\nAt the present time, there is a lack of agreement on what might be the most useful test of arm and hand function after SCI (for a review, see van Tuijl et al 56 ). Many of the scales developed have been deemed too insensitive to track small, but potentially meaningful functional gains. The majority of tests have been developed within the domains of stroke or hand surgery, but less often to describe the impairment and course of hand function recovery after SCI, particularly for acute tetraplegic patients. Many previous studies examined tetraplegics after functional reconstructive surgery of the upper limb or application of a hand neuroprosthesis and did not provide randomized control data.\nIt is generally accepted that the assessment of hand function has to include several components including: (1) proximal arm and trunk stabilization (reaching out), as well as placement of the arm and hand, (2) sensory testing of at least two sensory qualities (touch sensation, vibration, temperature, two-point discrimination, proprioception), (3) manual muscle testing of intrinsic (small hand muscles) and extrinsic muscles (forearm) involved in hand control, (4) description of different grasp forms (like pulp and lateral pinch), and (5) the effect of tenodesis on hand function, specifically for opening and closing of fingers and the fist.\nThe Quadriplegia Index of Function (QIF) was developed in the 1980s 57 as a scale for evaluating 10 areas of self-care and mobility for people living with tetraplegia. The QIF has been noted to be a better indicator of motor recovery than the FIM (when compared with ASIA motor scores) and a more sensitive measure of small gains in arm function. 58 , 59\nOne of the more established hand function assessment tools is the Sollerman test 60 although the test was not developed for SCI. The Sollerman test has limited resolution for hand function in tetraplegics, requires specialized equipment, and is a long duration examination (60–90 min). Another common test is the Manual Muscle Test, which has been used to evaluate handgrip strength, although it has been criticized as not sensitive enough to distinguish small or moderate changes in human subjects. 61\nThe Action Research Arm Test looks at different types of pinches and provides a qualitative scoring, but has been mainly applied in stroke patients. The Jebsen (Taylor test) is most frequently used in stroke and includes writing, lifting cans, simulated feeding, stacking checkers, and picking up paper clips and coins. However, it does not detect changes of intrinsic muscles and allows compensatory trunk and shoulder movements to accomplish any tasks.\nOther upper limb outcome assessment tools have recently been introduced. As an example, there is the motor capacities scale (MCS). 62 This scale was developed and tested in France with the participation of 52 motor-complete C5–C7 tetraplegics, although some had received restorative upper limb surgery. The MCS initially involved 36 items associated with activities of daily living (ADL), including: transfers, repositioning in a prone and seated position, use and control of either a manual or powered wheelchair, bilateral reaching to a predetermined target, and bilateral hand grasping. High inter-rater reliability (correlation coefficient of 0.99) was noted for the MCS, as was a high correlation with the Sollerman test (correlation coefficient of 0.96). Initial correlation with ASIA motor scores was lower (0.74). Because of redundancies, this list has now been reduced to 31 items associated with ADL. The MCS is undergoing further testing and validation.\nA Toronto group recently developed the Tetraplegia Hand Measure, which combines a modified Sollerman test with quantitative assessments of sensory function. A Zurich group developed a hand function test, which also uses certain key elements of the Sollerman test. An initiative is now underway across Canada, the United States, and Europe to develop an integrated hand function test as a valid assessment tool for SCI clinical trials.\nComprehensive functional outcome tools\nThe FIM was first developed in the 1980s (cf Stineman et al 63 ). The FIM is a proprietary global disability outcome assessment tool, which has been used for rating the functional performance of individuals, with a variety of different disorders and disabilities, on a series of ADL. It has been used as the functional outcome measure in many trials, such as the NASCIS III clinical trial. 6 Because of its application to a broad range of disabilities, it has become a standard tool for decisions on support and reimbursement as a person re-integrates back into their home community (ie it has been called a 'burden of care' tool). For the purposes of SCI clinical trials, some of the FIM subsections are not directly relevant to people living with SCI (eg communication and social cognition) and FIM scores, and ASIA motor scores are not tightly correlated. 58 , 59\nA more recently developed functional measure is the Spinal Cord Independence Measure (SCIM) and it appears to be a more sensitive and accurate functional assessment for ADL after SCI. SCIM has now gone through a few iterations 64 , 65 , 66 and is undergoing further refinement in multinational studies. The SCIM is a 100-point disability scale developed specifically for SCI with emphasis on 18 activities associated with:\nself-care (feeding, bathing, dressing, grooming), max.=20 points\nrespiration and sphincter management (ventilation, bladder, bowel, use of toilet), max.=40 points (clinically weighted)\nmobility (in bed, transfers, indoors and outdoors, wheelchair, walking), max.=40 points.\nPreliminary findings suggest that the SCIM may be a more relevant and a useful outcome tool for SCI clinical trials than the FIM. However, the well-established nature of FIM may slow the adoption of SCIM. It may be too much to expect that one comprehensive functional outcome tool will accurately and sensitively track all SCI clinically meaningful benefits after a therapeutic intervention; a number of functional outcome measures may be required initially.\nTop of page\nQoL surveys\nQoL assessments for people with SCI have been intensely debated as clinical trial endpoint tools (cf Dijkers et al 67 ). The inclusion of a QoL assessment is often recommended as one outcome measure to be included in any clinical trial assessment, though often as a secondary outcome. WHO defines QoL as a person's perception of his position in life within the context of the culture and value systems in which he lives and in relation to his goals, standards, and concerns. As outlined above, WHO published the ICF in 2002 with three distinctive dimensions:\nbody structure and function/impairment at organ level\nactivity/activity limitation at personal level\nparticipation/restriction at societal level.\nSeveral QoL surveys have been developed, along two paths, and are illustrated by the two following examples:\nSF-36 (Medical Outcomes Study 36-item Short Form health survey) is a profile where the investigator determines the domains of life that are pertinent and the assumption is that the same domains are important to all people in that group. SF-36 reflects the perspective and choices made by the 'outsider' (investigator) rather than the subjective point of view of the 'insider' (subject).\nSWLS (satisfaction with life survey) is an example of an alternate self-reported appraisal, where statements (eg I am satisfied with my life) are rated on a 7-point Likert-type scale (ranging from 'strongly disagree' to 'strongly agree'). SWLS is an example of a more global QoL where the individual (insider) is allowed to either adjust the weighting of a domain or in some cases self-nominate a domain as to its relative importance on their QoL. This can make comparisons between subjects or between study arms difficult.\nThus, QoL tools are either investigator-determined (eg SF-36), enabling statistical comparisons between an experimental and control group or they are more individualized (eg SWLS), allowing the participating subject to weigh the value (importance) of any individual field in the self-assessment of their own QoL.\nIn terms of SCI clinical trials of pharmaceutical drugs or cell-based transplants, especially during Phase 1 and 2, the former type of QoL survey (eg SF-36) is not suitable as a primary outcome measure, and should only be used in combination with other types of outcome data (eg ASIA motor scores or a functional outcome measure). Which precise QoL survey is best suited to a specific SCI trial has not been determined. It may be advisable to use more than one type of assessment.\nThe concern of the Panel was that any choice made by a subject during a QoL survey might accurately relate to a change in QoL, but be unrelated to an observable change in neurological impairment or functional capacity. Likewise, a small but significant improvement in neurological function might not influence the responses on a QoL survey. The consensus of the Panel was that changes in neurological function or functional outcomes should be used as the primary measure for Phase 1 or 2 SCI clinical trials that evaluate the activity of a pharmaceutical or cell-based transplant intervention.\nTop of page\nSpasticity\nA velocity-dependent, abnormal increase in muscle tone with exaggerated tendon jerks is one definition of spasticity, 68 which is a common complication of SCI and a variety of other CNS disorders. 69 , 70 Spasticity can lead to incoordination of muscle action, reduced functional limb movement and in its more severe forms may result in chronic pain, muscle contracture, and permanent muscle shortening. Several treatments have been developed to minimize spastic symptoms, including systemic or intrathecal Lioresal (Baclofen) and (more recently) the direct intramuscular injections of Botulinum toxin (Botox) into specific affected muscles.\nThe level of spasticity is known to vary over time, thus a single clinical assessment will not necessarily reflect accurately an individual's overall level of spasticity. The principal clinical outcome measure for spasticity has been the long-established Ashworth Scale or the modified Ashworth Scale, even though both scales have less than ideal inter-rater reliability 71 and have a poor correlation with self-rated assessments of spasticity. 70 The scale determines the amount of resistance felt during the passive displacement of a limb, but it does not accurately account for the dependence of the resistance to the velocity of the stretch, which can be highly variable from examiner to examiner.\nTop of page\nPain\nIt has been suggested that over 50% of people living with SCI reported experiences of chronic neuropathic pain. Agreement on classifying pain (as musculoskeletal, neuropathic, or visceral forms) after SCI has been elusive, but the classification of Siddall et al 72 has been widely quoted. Sharp, stabbing, or burning pain within the dermatomes at or just above the level of SCI is often termed at-level neuropathic pain, whereas similar types of pain below the level of the lesion have been called below-level neuropathic pain.\nThere are a few RCTs that have evaluated the benefits of gabapentin 73 and lidocaine 74 for the treatment of neuropathic pain after SCI (for a review, see Finnerup and Jensen 75 ). Nevertheless, causing pain as a result of an experimental treatment is also a major concern, especially as some of the emerging therapeutics have the potential to stimulate axonal fiber outgrowth or functional plasticity within central pain pathways. Thus, the Panel felt that inclusion of specific pain measures would be an important component of SCI therapeutics' outcome testing. The most straightforward assessment would rely on patient's self-reports of any increased pain during treatment. Several tools have been developed, including the visual analogue scale 76 and the neuropathic pain scale 77 Nevertheless, these may not always provide an accurate reflection of neuropathic pain, especially as an individual's emotional health and/or social interactions can modify pain perception.\nIn an acute or subacute situation, the source of an individual's pain may be difficult to locate or originate outside the CNS pain sphere (eg result from concomitant injury to another body tissue or due to a preceding condition). Clinical trials may want to consider a more direct measure for a change in central pain threshold. For example, components of the QST and/or EPT may be useful evaluations (cf Savic et al 25 , Savic et al 26 ).\nThere are many pain perception surveys available, including the well-known McGill pain questionnaire. 78 However, which pain assessment is the most accurate and easiest to use is a matter of debate. In more chronic SCI situations, pain management is an important clinical goal. One approach to mapping whether a pain management strategy is having a meaningful benefit is to assess how pain intensity interferes with ADL. Two common measurement scales of pain interference, the graded chronic pain (GCP) disability scale and three versions of the brief pain inventory (BPI), have recently been examined for their reliability and validity as pain assessment tools in a survey of 127 people living with chronic pain after SCI. 79 The self-report data asked questions on how pain interfered with ADL. Needless to say, increasing pain intensity caused increased interference with ADL. Both GCP and the three different length versions of the BPI were found to be internally consistent and related to the reported level of pain experienced.\nAnother issue is to carefully distinguish between neuropathic and normal musculoskeletal pain. A therapy that restores some normal pain sensation may make a patient aware of conditions that were previously unfamiliar to the spinal injured individual, such as lower-back pain or other forms of normal, internally referenced visceral pain.\nTop of page\nSummary and recommendations for the future\nObjective outcome measures are critical in designing useful SCI therapeutic clinical trials. Different clinical targets (eg sensorimotor tasks, autonomic function, personal functional capacity, performance, or community participation) normally require distinct and appropriate outcome assessment tools, which have been validated as both sensitive and accurate.\nThe most common outcome assessment tools currently being employed are the ASIA impairment grades and ASIA motor scores. The accuracy of initial and subsequent ASIA examinations is essential to ascribing a therapeutic benefit in neurological recovery. For example, a candidate drug or cell transplant with a very large effect size might rely on statistically significant differences in ASIA grades between the experimental and control arms of an SCI study. However, an intervention with a potentially smaller effect size might target a more specific and sensitive neurological outcome measure, such as a statistically significant difference between experimental and control groups for the ASIA motor score.\nEstablishing valid treatment effect thresholds for ASIA motor scores requires calculation of the spontaneous improvement of ASIA motor scores for each severity and level of SCI within 'untreated' control populations. Such an initial evaluation is now being undertaken by the ICCP Clinical Guidelines Panel. Nevertheless, any first table of ASIA motor score thresholds will require ongoing monitoring and updating to maintain relevance.\nValid and clinically meaningful sensory assessment tools for SCI remain a challenge where current assessment tools are either inadequate or insufficiently validated. Electrophysiological assessment tools exist and would benefit from broader application and standardization. Such evaluations are currently underway. Likewise, there is a need to develop a number of clinically valid autonomic function tests.\nAn improvement in the measurable performance of a meaningful function or behavior is necessary for any therapeutic intervention to be universally accepted as clinically beneficial. Thus, accurate and sensitive functional outcome measures are critical to SCI clinical trials and this will be especially true for any Phase 3 studies. The FIM scale is not specific to SCI and not suitable, although the recently developed SCIM assessment may be a more specific and accurate outcome tool for detecting clinical end points in SCI. The continued development and validation of tests that quantify highly relevant behaviors such as walking or hand function are most important; such tools may have greater utility for documenting the subtle benefit of a therapeutic than a more global scale of disability.\nThe inclusion of QoL measures in SCI trials is important, but which precise QoL survey is best suited to a specific SCI trial and their importance in the overall assessment of an intervention has not been determined. It may be best to use more than one type of assessment. The concern of the Panel was that any choice made by a subject during a QoL survey might be unrelated to an observable change in neurological or functional outcome. Likewise, a small, but significant, improvement in neurological function might not influence the responses on a QoL survey, which are often governed by attitude and social integration and not by physical disability.\nGiven the paucity of Phase 3 SCI clinical trial experiences and thus the emerging nature of SCI clinical studies, the current opinion of the Panel was that changes in neurological function or functional outcomes should be used as the primary measure for Phase 1 or 2 SCI clinical trials designed to evaluate the safety and/or provide evidence of activity of a pharmaceutical or cell-based transplant intervention. Neurological function tests should remain an element of the outcome assessment in Phase 3 trials.\nGlossary of definitions\n(Additional glossaries are included in the three accompanying papers)\nNeurological level of spinal injury is generally the lowest segment of the spinal cord with normal sensory and motor function on both sides of the body. However, the spinal level at which normal function is found often differs on each side of the body, as well as in terms of preserved sensory and motor function. Thus, up to four different segments may be identified in determining the neurological level and each of these segments is recorded separately and a single-level descriptor is not used. Note that the level of spinal column injury may not correlate with the neurological level of SCI.\nASIA (American Spinal Injury Association) Impairment Scale (or AIS) describes the completeness of a spinal injury (see Marino et al 3 ). An individual with an ASIA A grade has no motor or sensory function at the level of S4–S5 sacral segments. ASIA B has some sensory function below the neurological level, including S4–S5, but not motor function. ASIA C has some motor function below the neurological level, but more than half of the key muscles involved have a muscle strength score that is less than 3 ( Table 1 ). ASIA D has motor function below the neurological level but more than half of the key muscles have a muscle grade of 3 or more. ASIA E indicates normal motor and sensory function.\nTetraplegia (quadriplegia) is the term used to refer to loss of motor and/or sensory function owing to damage to the spinal cord, with impairment of the upper extremities as well as trunk, legs, and pelvic organs. This implies damage to the spinal cord at or above the C8 level.\nParaplegia is the equivalent term used to refer to functional loss below the level of the upper extremities, which may involve loss of motor and/or sensory function within the trunk, and/or the lower extremities. This implies damage to the spinal cord below the level of C8 and may include damage to conus medullaris or cauda equine (ie neural tissue within the spinal canal).\nComplete and incomplete SCI are other terms used to describe the overall severity of SCI. Technically, SCI is classified as complete if there is no motor or sensory function preservation in the sacral (most caudal) spinal segments. Thus, incomplete SCI is when there is some preserved motor or sensory function at the lowest sacral spinal level (S4–5). There can be extensive variability in the degree of preserved function after incomplete SCI.\nASIA Sensory and Motor Assessments form the basis for the International Standards for Neurological and Functional Classification of Spinal Cord Injury (the ASIA International Standards) and are conducted in the supine position and involve a qualitative grading of sensory responses to touch and pin-prick at each of 28 dermatomes along each side of the body and a qualitative grading of the strength of contraction within 10 representative (key) muscles, primarily identified with a specific spinal level, 5 for the upper extremity (C5–T1) and 5 for the lower extremity (L2–S1) on each side of the body ( Table 1 )\nASIA Motor Score is calculated by assigning to one muscle group, innervated and primarily identified with a specific spinal level, a score between 0 (no detectable contraction) and 5 (active movement and a full range of movement against maximum resistance). C5–T1 and L2–S1 are tested, giving 10 levels on each side of the body for a possible maximum score of 100.\nLEMS is the lower extremity motor score which is a maximal 50-point subset of the ASIA motor score for the representative leg and foot muscles.\nUEMS is the upper extremity motor score which is a maximal 50-point subset of the ASIA motor score for the representative arm and hand muscles.\nMotor level is defined as the most caudal spinal level as indexed by the key muscle group for that level having a muscle strength of 3 or above while the key muscle for the spinal segment above is normal (=5).\nASIA sensory score is calculated by testing a point on the dermatome for each spinal level from C2 to S4–5 for both light touch and pin-prick sensation. Each point is assigned a score from 0 (absent sensation) through 1 (abnormal sensation) to 2 (normal sensation). This gives a possible maximum score of 56 on each side for a maximum total of 112 each for light touch and pin-prick.\nSensory level is defined as the spinal segment corresponding with the most caudal dermatome having a normal score of 2/2 for both pin-prick and light touch.\nZone of partial preservation (ZPP) is only used when SCI is complete and refers to those segments below the neurological level of injury where there is some preservation of impaired motor or sensory function (usually, but not always, within a few segments of the neurological level).\nTop of page\nReferences\nSteeves J, Fawcett J, Tuszynski M. Report of International Clinical Trials Workshop on spinal cord injury February 20–21, 2004, Vancouver, Canada. Spinal Cord 2004; 42: 591–597. |  Article  |  PubMed  |  ChemPort  |\nLammertse D et al. Guidelines for the conduct of clinical trials for spinal cord injury (SCI) as developed by the International Campaign for Cures of spinal cord Paralysis (ICCP) Panel: Clinical trial design. Spinal Cord 2006 [E-pub ahead of print: 19 December 2006; doi:10.1038/sj.sc.3102010]. |  Article  |\nMarino R et al. International standards for neurological classification of spinal cord injury (6th edn). J Spinal Cord Med 2003; 26(Suppl 1): S49–S56.\nBracken MB et al. A randomized, controlled trial of methylprednisolone or naloxone in the treatment of acute spinal cord injury. Results of the second National Acute Spinal Cord Injury Study. N Engl J Med 1990; 322: 1405–1411. |  PubMed  |  ISI  |  ChemPort  |\nBracken MB et al. Administration of methylprednisolone for 24 or 48 hours or tirilazad mesylate for 48 hours in the treatment of acute spinal cord injury. Results of the Third National Acute Spinal Cord Injury Randomized Controlled Trial. National Acute Spinal Cord Injury Study. JAMA 1997; 277: 1597–1604. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nBracken MB et al. Methylprednisolone or tirilazad mesylate administration after acute spinal cord injury: 1-year follow-up. Results of the third national acute spinal cord injury randomized controlled trial. J Neurosurg 1998; 89: 699–706. |  PubMed  |  ISI  |  ChemPort  |\nGeisler FH, Coleman WP, Grieco G, Poonian D, the Sygen® Study Group. Measurements and recovery patterns in a multicenter study of acute spinal cord injury. Spine 2001a; 26: S68–S86. |  Article  |  PubMed  |  ChemPort  |\nGeisler FH, Coleman WP, Grieco G, Poonian D, the Sygen® Study Group. The Sygen® multicenter acute spinal cord injury study. Spine 2001b; 26: S87–S98. |  Article  |  PubMed  |  ChemPort  |\nBlaustein DM, Zafonte RD, Thomas D, Herbison GJ, Ditunno Jr JF. Predicting recovery of motor complete quadriplegic patients: twenty-four-hour versus 72-h motor index scores. Arch Phys Med Rehabil 1991; 72: 786.\nBurns AS, Lee BS, Ditunno Jr JF, Tessler A. Patient selection for clinical trials: the reliability of the early spinal cord injury examination. J Neurotrauma 2003; 20: 477–482. |  Article  |  PubMed  |\nFawcett JW, Curt A, Steeves JD, Coleman WP, Tuszynski MH. Guidelines for the conduct of clinical trials for spinal cord injury (SCI) as developed by the ICCP Panel: Spontaneous recovery after spinal cord injury and statistical power needed for therapeutic clinical trials. Spinal Cord 2006 [E-pub ahead of print: 19 December 2006; doi:10.1038/sj.sc.3102007]. |  Article  |\nMarino RJ, Graves DE. Metric properties of the ASIA motor score: subscales improve correlation with functional activities. Arch Phys Med Rehabil 2004; 85: 1804–1810. |  Article  |  PubMed  |\nColeman WP, Geisler FH. Injury severity as a primary predictor of outcome in acute spinal cord injury: retrospective results from a large multicenter clinical trial. Spine J 2004; 4: 373–378. |  Article  |  PubMed  |\nWaters RL, Adkins RH, Yakura JS, Sie I. Motor and sensory recovery following complete tetraplegia. Arch Phys Med Rehabil 1993; 74: 242–247. |  PubMed  |  ISI  |  ChemPort  |\nMarino RJ, Ditunno JF, Donovan WH, Maynard F. Neurologic recovery after traumatic spinal cord injury: data from the Model Spinal Cord Injury Systems. Arch Phys Med Rehabil 1999; 80: 1391–1396. |  Article  |  PubMed  |  ChemPort  |\nTuszynski MH, Steeves JD, Fawcett JW, Lammertse D, Kalichman M. Guidelines for the conduct of clinical trials for spinal cord injury (SCI) as developed by the ICCP Panel: Clinical trial inclusion/exclusion criteria and ethics. Spinal Cord 2006 [E-pub ahead of print: 19 December 2006; doi:10.1038/sj.sc.3102009]. |  Article  |\nMarino RJ, Herbison GF, Ditunno JF. Peripheral sprouting as a mechanism for recovery in the zone of injury in acute quadriplegia: a single-fiber EMG study. Muscle Nerve 1994; 17: 1466–1468. |  Article  |  PubMed  |  ChemPort  |\nDietz V, Curt A. Neurological aspects of spinal cord repair: promises and challenges. Lancet Neurol 2006; 5: 688–694. |  Article  |  PubMed  |\nCrozier KS, Graziani V, Ditunno JF, Herbison GJ. Spinal cord injury: prognosis for ambulation based on sensory examination in patients who are initially motor complete. Arch Phys Med Rehabil 1991; 72: 119–121. |  PubMed  |  ISI  |  ChemPort  |\nKatoh S, el Masry WS. Motor recovery of patients with motor paralysis and sensory sparing following cervical spinal injuries. Paraplegia 1995; 30: 506–509.\nHayes KC et al. Clinical and electrophysiological correlates of quantitative sensory testing in patients with incomplete spinal cord injury. Arch Phys Med Rehabil 2002; 83: 1612. |  Article  |  PubMed  |  ISI  |\nEllaway PH et al. Towards improved clinical and physiological assessments of recovery in spinal cord injury: a clinical initiative. Spinal Cord 2004; 42: 325–337. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nNicotra A, Ellaway PH. Thermal perception thresholds: assessing the level of human spinal cord injury. Spinal Cord 2006; 44: 617–624. |  Article  |  PubMed  |  ChemPort  |\nDavey NJ, Nowicky AV, Zaman R. Somatopy of perceptual threshold to cutaneous electrical stimulation in man. Exp Physiol 2001; 86: 127–130. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nSavic G et al. Quantitative sensory tests (perceptual thresholds) in patients with spinal cord injury. J Rehab Res Dev 2006 (in press).\nSavic G, Bergstrom EMK, Frankel HL, Jamous MA, Ellaway PH, Davey NJ. Perceptual thresholds to cutaneous electrical stimulation in patients with spinal cord injury. Spinal Cord 2006; 44: 560–566. |  Article  |  PubMed  |  ChemPort  |\nCurt A, Dietz V. Ambulatory capacity in spinal cord injury: Significance of somatosensory-evoked potentials and ASIA protocols in predicting outcome. Arch Rhys Med Rehabil 1997; 78: 39–43. |  Article  |  ChemPort  |\nCurt A, Keck ME, Dietz V. Functional outcome following spinal cord injury: significance of motor-evoked potentials. Arch Phys Med Rehab 1998; 79: 81–86. |  Article  |  ChemPort  |\nDavey NJ, Smith HC, Wells E, Maskill DW, Savic G, Ellaway P. Frankel HL responses of thenar muscles to transcranial magnetic stimulation of the motor cortex in incomplete spinal cord injury patients. J Neurol Neurosurg Psychiatry 1998; 65: 80–87. |  PubMed  |  ChemPort  |\nDavey NJ, Smith HC, Savic G, Maskill DW, Ellaway PH, Frankel HL. Comparison of input-output patterns in the corticospinal system of normal subjects and incomplete spinal cord injured patients. Exp Brain Res 1999; 127: 382–390. |  Article  |  PubMed  |  ChemPort  |\nKirshblum S, Lim S, Garstang S, Millis S. Electrodiagnostic changes of the lower limbs in subjects with chronic complete cervical spinal cord injury. Arch Phys Med Rehabil 2001; 82: 604–607. |  Article  |  PubMed  |  ChemPort  |\nCurt A, Dietz V. Electrophysiological recordings in patients with spinal cord injury: significance for predicting outcome. Spinal Cord 1999; 37: 157–165. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nMetz GA, Curt A, van de Meent H, Klusman I, Schwab ME, Dietz V. Validation of the weight-drop contusion model in rats: a comparative study of human spinal cord injury. J Neurotrauma 2000; 17: 1–17. |  PubMed  |  ISI  |  ChemPort  |\nDiehl P, Kliesch U, Dietz V, Curt A. Impaired facilitation of motor-evoked potentials in incomplete spinal cord injury. J Neurol 2006; 253: 51–57. |  Article  |  PubMed  |\nWolfe DL, Hayes KC, Hsieh JT, Potter PJ. Effects of 4-aminopyridine on motor-evoked potentials in patients with spinal cord injury: a double-blinded, placebo-controlled crossover trial. J Neurotrauma 2001; 18: 757–771. |  Article  |  PubMed  |  ChemPort  |\nSmith HC et al. Corticospinal function studied over time following incomplete spinal cord injury. Spinal Cord 2000; 38: 292–300. |  Article  |  PubMed  |  ChemPort  |\nLaubis-Herrmann U, Dichgans J, Bilow H, Topka H. Motor reorganization after spinal cord injury: evidence of adaptive changes in remote muscles. Restor Neurol Neurosci 2000; 17: 175–181. |  PubMed  |\nThomas SL, Gorassini MA. Increases in corticospinal tract function by treadmill training after incomplete spinal cord injury. J Neurophysiol 2005; 94: 2844–2855. |  Article  |  PubMed  |\nCurt A, Schwab ME, Dietz V. Providing the clinical basis for new interventional therapies: refined diagnosis and assessment of recovery after spinal cord injury. Spinal Cord 2004; 42: 1–6. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nKuppuswamy A et al. Motoneurone excitability in back muscles assessed using mechanically evoked reflexes in spinal cord injured patients. J Neurol Neurosurg Psychiatry 2005; 76: 1259–1263. |  Article  |  PubMed  |  ChemPort  |\nClaydon VE, Steeves JD, Krassioukov A. Orthostatic hypotension following spinal cord injury: understanding clinical pathophysiology. Spinal Cord 2006; 44: 341–351. |  Article  |  PubMed  |  ChemPort  |\nDevivo MJ, Krause JS, Lammertse DP. Recent trends in mortality and causes of death among persons with spinal cord injury. Arch Phys Med Rehabil 1999; 80: 1411–1419. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nGarshick E et al. A prospective assessment of mortality in chronic spinal cord injury. Spinal Cord 2005; 43: 408–416. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nNicotra A, Asahina M, Mathias CJ. Skin vasodilator response to local heating in human chronic spinal cord injury. Eur J Neurol 2004; 11: 835–837. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nNicotra A, Young TM, ASahina M, Mathias CJ. The effect of different physiological stimuli on skin vasomotor reflexes above and below the lesion in human chronic spinal cord injury. Neurorehabil Neural Repair 2005b; 19: 325–331. |  Article  |\nCariga P, Catley M, Mathias CJ, Savic G, Frankel HL, Ellaway PH. Organisation of the sympathetic skin response in spinal cord injury. J Neurol Neurosurg Psychiatry 2002; 72: 356–360. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nNicotra A, Catley M, Ellaway PH, Mathias CJ. The ability of physiological stimuli to generate the sympathetic skin response in human chronic spinal cord injury. Restor Neurol Neurosci 2005a; 23: 331–339.\nFehlings MG et al. The optimal radiologic method for assessing spinal canal compromise and cord compression in patients with cervical spinal cord injury Part II: results of a multicenter study. Spine 1999; 24: 605–613. |  Article  |  PubMed  |  ChemPort  |\nBono CM et al. Measurement techniques for lower cervical spine injuries: consensus statement of the Spine Trauma Study Group. Spine 2006; 31: 603–609. |  Article  |  PubMed  |\nMiyanji F, Furlan JC, Aarabi B, Arnold PM, Fehlings MG. Correlation of MRI findings with neurological outcome in patients with acute cervical traumatic spinal cord injury: a prospective study in 100 consecutive patients. Radiology 2006 (in press).\nSchwartz ED, Duda J, Shumsky JS, Cooper ET, Gee J. Spinal cord diffusion tensor imaging and fiber tracking can identify white matter tract disruption and glial scar orientation following lateral funiculotomy. J Neurotrauma 2005; 22: 1388–1398. |  Article  |  PubMed  |  ISI  |\nMiller DH. Biomarkers and surrogate outcomes in neurodegenerative disease: lessons from multiple sclerosis. J Am Soc Exp NeuroTherapeutics 2004; 1: 284–294.\nDitunno JF, Burns AS, Marino RJ. 2005. Neurological and functional capacity outcome measures: essential to spinal cord injury clinical trials. J Rehab Res Dev 2005; 42(Suppl 1): 35–41. |  Article  |\nMorganti B, Scivoletto G, Ditunno P, Ditunno JF, Molinari M. Walking index for spinal cord injury (WISCI): criterion validation. Spinal Cord 2005; 43: 43–71. |  Article  |\nvan Hedel HJ, Wirz M, Dietz V. Assessing walking ability in subjects with spinal cord injury: validity and reliability of 3 walking tests. Arch Phys Med Rehabil 2005; 86: 190–196. |  Article  |  PubMed  |  ISI  |\nvan Tuijl JH, Janssen-Potten YJ, Seele HA. Evaluation of upper extremity motor function tests in tetraplegics. Spinal Cord 2002; 40: 51–64. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nGresham GE, Labi ML, Dittmar SS, Hicks JT, Joyce SZ, Stehlik MA. The Quadriplegia Index of Function (QIF): sensitivity and reliability demonstrated in a study of thirty quadriplegic patients. Paraplegia 1986; 24: 3–44.\nMarino RJ et al. 1993 Assessing self-care status in quadriplegia: comparison of the quadriplegia index of function (QIF) and the functional independence measure (FIM). Paraplegia 1991; 31: 225–233.\nYavuz N, Tezyurek M, Akyuz M. A comparison of two functional tests in quadriplegia: The quadriplegia index of function and the functional independence measure. Spinal Cord 1998; 36: 832–837. |  Article  |  PubMed  |  ChemPort  |\nSollerman C, Ejeskar A. Sollerman hand function test. A standardized method and its use in tetraplegic patients. Scan. J Plast Recontr Surg Hand Surg 1995; 29: 167–176. |  ChemPort  |\nNoreau L, Vachon J. Comparison of three methods to assess muscular strength in individuals with spinal cord injury. Spinal Cord 1998; 36: 716–723. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nFattal C. Motor capacities of upper limbs in tetraplegics: a new scale for the assessment of the results of functional surgery on upper limbs. Spinal Cord 2004; 42: 80–90. |  Article  |  PubMed  |  ChemPort  |\nStineman MG et al. A Prototype Classification System for Medical Rehabilitation. American Rehabilitation Association: Washington DC 1994.\nCatz A, Itzkovich M, Agranov E, Ring H, Tamir A. SCIM--spinal cord independence measure: a new disability scale for patients with spinal cord lesions. Spinal Cord 1997; 35: 850–856. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nItzkovich M et al. Reliability of the Catz-Itzkovich Spinal Cord Independence Measure assessment by interview and comparison with observation. Am J Phys Med Rehabil 2003; 82: 267–272. |  Article  |  PubMed  |  ISI  |\nCatz A et al. A multi-center international study on the spinal cord independence measure, version III: Rasch psychometric validation. Spinal Cord 2006 [E-pub ahead of print: 15 August 2006; doi:10.1038/sj.sc.3101960]. |  Article  |\nDijkers MP. Individualization in quality of life measurement: instruments and approaches. Arch Phys Med Rehabil 2003; 84(Suppl 1): S3–S14. |  Article  |  PubMed  |\nYoung RR. Spasticity: a review. Neurology 1994; 44: 12–20.\nHobart JC et al. Getting the measure of spasticity in multiple sclerosis: the multiple sclerosis spasticity scale (MSSS-88). Brain 2006; 129: 224–234. |  Article  |  PubMed  |  ChemPort  |\nLechner HE, Frotzler A, Eser P. Relationship between self- and clinically rated spasticity in spinal cord injury. Arch Phys Med Rehabil 2006; 87: 15–19. |  Article  |  PubMed  |\nPandyan AD, Johnson GR, Price CI, Curless RH, Barnes MP, Rodgers H. A review of the properties and limitations of the Ashworth and Modified Ashworth Scales as measures of spasticity. Clin Rehabil 1999; 13: 373–383. |  Article  |  PubMed  |  ChemPort  |\nSiddall PJ, Taylor DA, McClelland JM, Rutkowski SB, Cousins MJ. Pain report and the relationship of pain to physical factors in the first six months following spinal cord injury. Pain 1999; 81: 187–197. |  Article  |  PubMed  |  ISI  |  ChemPort  |\nLevendoglu F, Ogun CO, Ozerbil O, Ogun TC, Ugurlu H. Gabapentin is the first ine drug for the treatment of neuropathic pain in spinal cord injury. Spine 2004; 29: 743–751. |  Article  |  PubMed  |  ISI  |\nFinnerup NB et al. Intraveneous lidocaine relieves spinal cord injury pain. Anesthesiol 2005; 102: 1023–1030. |  Article  |  ChemPort  |\nFinnerup NB, Jensen TS. Spinal cord injury pain – mechanisms and treatment. Eur J Neurol 2004; 11: 73–82. |  Article  |  PubMed  |  ChemPort  |\nJensen MP, Karoly P. Self-report scales and procedures for assessing pain in adults. In: Turk DC, Melzack R (eds). Handbook of Pain Assessment. Guilford Press: New York, NY 1992, pp 152–168.\nBradley S, Galer BS, Jensen MP. Development and preliminary validation of a pain measure spedific to neuropathic pain: the neuropathic pain scale. Rehabil Med 1997; 48: 332–337.\nMelzack R. The McGill pain questionnaire: from description to measurement. Anesthesiol 2005; 103: 199–202. |  Article  |\nRaichle KA, Osborne TL, Jensen MP, Cardenas D. The reliability and validity of pain interference measures in persons with spinal cord injury. J Pain 2006; 7: 179–186. |  Article  |  PubMed  |\nTop of page\nAcknowledgements\nWe are grateful for the support of The International Campaign for Cures of spinal cord injury Paralysis (ICCP), which provided the funding for the authors' travel and accommodation expenses. The ICCP represents the following member organizations: Christopher Reeve Foundation (USA), Institut pour la Recherche sur la Moëlle Epinière (FRA), International Spinal Research Trust (UK), Japan Spinal Cord Foundation, Miami Project to Cure Paralysis (USA), Paralyzed Veterans of America (USA), Rick Hansen Man In Motion Foundation (CAN), SpinalCure Australia, and Spinal Research Fund of Australia. We thank the European Multicenter study in Spinal Cord Injury (EM-SCI) for sharing their data on spontaneous recovery after spinal cord injury. ICORD (International Collaboration on Repair Discoveries) in Vancouver provided all logistical coordination and support. All panel members (authors) volunteered their time and effort. Finally, we are most grateful for the input and constructive comments from a countless number of SCI investigators over the past 2.5 years.\n""","0.13623166","""http://www.nature.com/sc/journal/v45/n3/full/3102008a.html""","[-0.178219,51.500505]"
"""University_of_Aberdeen""","""Meeting the on-line needs of disabled tourists: an assessment of UK-based hotel websites - Williams - 2006 - International Journal of Tourism Research - Wiley Online Library""","""International Journal of Tourism Research\nPrevious article in issue: The changing profile of caravanners in Australia\nPrevious article in issue: The changing profile of caravanners in Australia\nMeeting the on-line needs of disabled tourists: an assessment of UK-based hotel websites\nAuthors\nUniversity of Aberdeen Business School, Aberdeen, Scotland, UK\nUniversity of Aberdeen Business School, Edward Wright Building, Aberdeen AB24 3QY, Scotland, UK.\nCited by (CrossRef): 12 articles Check for updates\nCitation tools\nCiting literature\nAbstract\nThe easy exchange of rich information between often geographically dispersed parties is an important precursor of successful tourism transactions. Internet-based technologies, in particular the World Wide Web, offer possibilities to both buyers and sellers to exchange information without the constraints of geography and time diminishing its richness. The disabled, representing a significant part of any marketplace, may, however, have difficulties accessing the content of the Web and therefore sharing the benefits of rich information exchange. This is the principal concern of ‘Web content accessibility’. Focusing on the tourism sector, in particular UK-based hotels, this paper examines the accessibility of their websites. However, recognising that it is not just access to information that is important for the disabled, but also the quality of that information, the paper also examines the extent to which the information contained on websites serves their particular needs. Utilising the accessibility testing software ‘Bobby’, disappointingly low levels of Web content accessibility were found amongst the sample of websites. Against a framework of information needs developed from criteria provided by disability organisations, the sample also revealed disappointingly low levels of specific (relevant) information for the disabled. Copyright © 2006 John Wiley & Sons, Ltd.\nArticles related to the one you are viewing\nCiting Literature\nNumber of times cited: 12\n1\nStephanie Morris, Scholah Kazi, Emerging trends regarding accessible accommodation in Dubai luxury hotels, Worldwide Hospitality and Tourism Themes, 2014, 6, 4, 317\nCrossRef\n2\nWei Wang, Shu Cole, Perceived Onboard Service Needs of Passengers with Mobility Limitations: An Investigation among Flight Attendants, Asia Pacific Journal of Tourism Research, 2014, 19, 11, 1239\nCrossRef\n3\nEleni Michopoulou, Dimitrios Buhalis, Information provision for challenging markets: The case of the accessibility requiring market in the context of tourism, Information & Management, 2013, 50, 5, 229\nCrossRef\n4\nJennie Small, Simon Darcy, Tanya Packer, The embodied tourist experiences of people with vision impairment: Management implications beyond the visual gaze, Tourism Management, 2012, 33, 4, 941\nCrossRef\n5\nSimon Darcy, Developing Sustainable Approaches to Accessible Accommodation Information Provision: A Foundation for Strategic Knowledge Management, Tourism Recreation Research, 2011, 36, 2, 141\nCrossRef\n6\nDimitrios Buhalis, Eleni Michopoulou, Information-enabled tourism destination marketing: addressing the accessibility market, Current Issues in Tourism, 2011, 14, 2, 145\nCrossRef\n7\nSimon Darcy, Shane Pegg, Towards Strategic Intent: Perceptions of disability service provision amongst hotel accommodation managers, International Journal of Hospitality Management, 2011, 30, 2, 468\nCrossRef\n8\nVictoria Richards, Annette Pritchard, Nigel Morgan, (Re)Envisioning tourism and visual impairment, Annals of Tourism Research, 2010, 37, 4, 1097\n9\nSimon Darcy, Inherent complexity: Disability, accessible tourism and accommodation information preferences, Tourism Management, 2010, 31, 6, 816\nCrossRef\n10\nSimon Darcy, Tracy Taylor, Disability citizenship: an Australian human rights analysis of the cultural industries, Leisure Studies, 2009, 28, 4, 419\nCrossRef\n11\nAna Isabel Polo Peña, Dolores María Frías Jamilena, The relationship between business characteristics and ICT deployment in the rural tourism sector. The case of Spain, International Journal of Tourism Research, 2009, n/a\n""","0.41596383","""http://onlinelibrary.wiley.com/doi/10.1002/jtr.547/abstract?systemMessage=Wiley+Online+Library+will+be+unavailable+on+Saturday+7th+Oct+from+03.00+EDT+%2F+08%3A00+BST+%2F+12%3A30+IST+%2F+15.00+SGT+to+08.00+EDT+%2F+13.00+BST+%2F+17%3A30+IST+%2F+20.00+SGT+and+Sunday+8th+Oct+from+03.00+EDT+%2F+08%3A00+BST+%2F+12%3A30+IST+%2F+15.00+SGT+to+06.00+EDT+%2F+11.00+BST+%2F+15%3A30+IST+%2F+18.00+SGT+for+essential+maintenance.+Apologies+for+the+inconvenience+caused+.""","[-2.099122,57.165019]"
"""University_College_London""","""Abstracts from the Society for Clinical Trials Annual Meeting, Boston, Massachusetts, USA - May 19-22, 2013Clinical Trials, 2013""","""Monitoring Compliance on Cancer Clinical Trials: A Peer Review Mechanism for Continuous Quality Improvement\n(1) Erica E. Love (2) Yelena Novik (3) Lisa Gaynes & (4) Nick Shuman\n(1) NYU Cancer Institute, NYU School of Medicine (2) NYU Cancer Institute, (3) NYU School of MedicineNYU Cancer Institute, (4) NYU School of MedicineNYU Cancer Institute, NYU School of Medicine\nInternal monitoring and self-auditing on cancer clinical trials is essential in maintaining data integrity used to make therapeutic decisions, in protecting research subjects, and in ensuring proper use of resources. NYU Cancer Institute implemented an Internal Audit Committee (IAC) in 2006 to ensure compliance and improve data quality in Investigator Initiated trials (IITs). Quarterly, 10% of cases enrolled on IITs are randomly selected for auditing. Auditors consist of investigator physicians, regulatory and research coordinators, and research nurses to ensure a peer review process. Elements reviewed during audits include informed consent, screening and eligibility and protocol compliance. According to National Cancer Institute (NCI) guidelines, deficiencies are categorized as major or minor and overall results as acceptable, acceptable, needs follow up or unacceptable. On average the IAC audits 27 cases each year. Since 2006 the total number of cited deficiencies has dropped significantly. In 2007 an average of 16 major deficiencies were cited per audit. By 2009, the number of major deficiencies per audit was down to an average of 8; a 50% decrease in just two years. In 2011, the IAC implemented an additional level of monitoring entailing quality assurance (QA) reviews of the first two cases and the third case on each IIT. In 2012, QA reviews were conducted on 5 cases. Investigators are made aware of any deficiencies from the first two cases and are asked to implement a corrective action plan (CAP). The third case is reviewed to ensure the CAP was successful in eliminating any deficiencies to ensure continuous quality improvement. Implementing these internal auditing systems resulted in consistent standards of monitoring and audits, increased participation amongst investigators and research staff in the audit process, decreased number of deviations, greater feedback from PIs to audit findings, and CAPs addressing common causes of deviations.\nA03\nImplementing Clinical Trials Methodology into the Everyday Care of Abused Children\nChad Shenk\nCincinnati Children’s Hospital Medical Center, University of Cincinnati College of Medicine\nChild maltreatment affects 700,000 children each year in the United States and increases the risk for a number of adverse health outcomes throughout the lifespan. Child advocacy centers (CACs) are often the initial stage of care for this population where multiple health services, including targeted prevention and treatment implementation, are delivered as part of the standard of care. However, bridging the gap between typical service delivery and treatment evaluation is often difficult, as CACs typically do not have the methodological expertise to evaluate clinical services formally. This presentation provides a framework for implementing state-of-the-art clinical trials methodology into the everyday practice of CACs to identify the optimal course of treatment for children exposed to abuse. The use of sequential multiple assignment randomized trials (SMARTs) will demonstrate how multiple levels of care can be evaluated within the context of ongoing healthcare delivery at a CAC. SMARTs are the ideal methodology for evaluating treatment services in clinic settings as they model the treatment decisions made by families and practitioners at various stages of care. For instance, targeted prevention is often the first line treatment for abused children. However, up to 70% of abused children do not respond to targeted prevention and require additional treatment. Additional treatment often involves behavioral or pharmacological interventions or both to achieve a response. Implementing a SMART in such a setting would identify: 1) the most effective initial intervention for abused children, 2) the optimal dose of the initial intervention, and 3) the most effective second-line intervention for those children not responding to targeted prevention. Specific measurement strategies will be reviewed to promote the assessment of predictors of treatment response as well as mediators of long-term treatment response.\nA04\nFrom Late Phase to Early Phase: Experiences of Establishing a Disease-Specific Early Phase Trials Unit\n(1) Sarah Brown (2) Louise Flanagan (3) Fiona Collinson (3) Sarah Flynn (4) Nina Thenagaran (5) Deborah Sherratt (5) Avie-Lee Coney\nClinical Trials Research Unit, University of Leeds\nIn November 2009, the Clinical Trials Research Unit at the University of Leeds successfully became the Myeloma UK (MUK) Clinical Trials Coordinating Office (CTCO) for the design, coordination and analysis of a network of early phase trials in myeloma. In an initiative developed by MUK, a dedicated team of clinicians, trialists and statisticians were brought together to streamline the clinical trial development process, from trial concept to first patient entered. Facilitating knowledge retention and specialization, the ultimate aim of the MUK CTCO is to promote bench to bedside research and allow more patients faster access to novel therapies. With an international track record in late phase trials, the Clinical Trials Research Unit initiated a strategy to transfer these skills to the early phase setting. Key areas associated with the successful development of the MUK CTCO included: Establishing a core team of researchers, Initiating a training program, Producing standard operating procedures, Standardization of case report forms and databases, Statistical design, Identifying independent umbrella committees, Pharmaceutical collaboration Developing trials of novel therapies, as well as providing initial evidence for use of licensed therapies in different settings, the initiation of the MUK CTCO has led to the successful implementation of five phase I and II trials over three years (1 in follow-up, 1 in recruitment and 3 in set-up), with a further two currently in development. We will discuss the key areas associated with the development of the MUK CTCO, from initiation to the successful set-up of five early phase clinical trials, and examine the different challenges associated with setting up and running phase I/II trials compared with large randomized controlled phase III trials and the impact this has on workload, timelines and the establishment of new processes.\nA05\nPulling the Trigger on Risk-Based Monitoring\nRick Morrison\nComprehend Systems\nDue to increasing industry pressures, drug and device makers are continually looking for ways to save money and run more efficiently. On-site monitoring can account for anywhere from 25 to 30 percent of the overall cost of a clinical trial. A shift to risk-based monitoring is a critical strategy to alleviating this strain. Modern analytics tools and technologies are driving the emergence of risk-based monitoring because they enable powerful insights into data that haven’t been possible in the past. Modern alerting systems can incorporate rules from analytics systems specifically looking for known or anticipated anomalies. When alerts are triggered, processes can be implemented to react, even with on-site monitoring if necessary. As more and more anomalies are found, a library of alerts can be developed to categorize and respond to unique risks immediately. Over time, the collection can become sophisticated and hugely effective in identifying problems proactively greatly reducing the need for on-site monitoring. In this presentation, Rick Morrison, CEO and co-founder of Comprehend Systems, will discuss how sponsors can transition to risk-based monitoring practices using advanced technologies and analytics. Attendees will gain an understanding of how to get started with risk-based monitoring, including: Which types of trials are appropriate for centralized monitoring practices. How to determine and apply the effective trigger factors. What processes, procedures and tools need to be put into place. Where risk-based monitoring strategies are heading in the future. With these underlying tools and strategies in place, clinical trial sponsors can automate and streamline complex criteria and processes to enhance the efficiency of the entire site monitoring process leading to significant cost reduction of each trial.\nA06\nThe Role of the Data Manager in Risk-Based Monitoring\nAaron Perlmutter\nData Coordination Unit, Division of Biostatistics and Epidemiology, Medical University of South Carolina\nThe high cost of on-site monitoring is well documented in the literature, but the return on this investment is less clear (Duley et al, 2008; Korieth, 2011). The development of electronic data capture and clinical trial management systems provides instant access to data and real-time quality checks, and a mechanism to perform monitoring centrally and remotely (KAI, Feb 2012). In recent years, a greater emphasis has been placed on a risk-based monitoring approach that incorporates centralized monitoring as a way to monitor data quality, and the FDA proposed a greater reliance on risk-based monitoring in its 2011 Draft Guidance for Industry, Oversight of Clinical Investigations - A Risk-Based Approach to Monitoring. Centralized monitoring can be used to assess adherence to study procedures and to identify problematic data and sites (Baigent et al, 2008). While centralized monitoring, when combined with on-site monitoring, can increase a trial’s data quality, the concept of risk-based monitoring is a relatively recent development in clinical trials, and guidance is limited regarding the role of the data manager in centralized monitoring. Data managers are in a unique position to assist in the process as they likely perform some centralized monitoring already, including checks for range and logic (e.g. correct time sequence, no contradictory data points), as well as completeness and timeliness of data entry. With the appropriate tools, the data manager can identify poorly performing sites and suspicious data patterns. When this information is shared with the study team and acted upon, the integrity of the trial can be improved. Centralized monitoring is as an important component in risk-based monitoring strategies (Brosteanua et al, 2009). However, beyond basic checklists, its methodologies are not clearly defined. This presentation attempts to define the data manager’s role in risk-based monitoring and to demonstrate tools to perform centralized monitoring in a CTMS.\nA07\nIdentifying Atypical Sites with Central Statistical Monitoring\nErik Doffagne\nCluePoints Inc.\nRegulatory agencies are moving forward by encouraging alternative approaches that streamline the clinical trials costs without compromising the quality or the scientific validity. A recent draft guidance from FDA suggests that risk based approaches should be used for clinical trials monitoring. A similar position has been taken by the EMA in a reflection paper. The industry is now considering the solutions for implementing novel monitoring strategies driven by indicators of the investigational sites quality. A common way to achieve the evaluation of the site quality or performance is to look on predefined metrics, often called Key Risk Indicators (KRI). In this talk we introduce a complementary layer to KRI: a Central Statistical Monitoring (CSM) approach based on advanced statistical methods and data mining tools aiming at detecting sites with atypical patterns in the data. Atypical patterns might reflect different kinds of problems from fraud (e.g. invented patients) to quality issues (i.e. underreporting). We demonstrate a concrete CSM implementation and illustrate how the CSM processes can be designed. The requirements and the challenges of the proposed approach are discussed.\nA08\nRisk-Based Monitoring Approach in Practice – Combination of Real-Time Central Monitoring and On-Site Source Document Verification\nWenle Zhao\nMedical University of South Carolina\nAbout two thirds of clinical trials conducted in US are funded by federal agencies and other non-profit organizations. While 100% source document verification for industry sponsored studies has been the standard for decades, it is rarely the case for federal funded trials managed by academic research institutions due to funding limitations. The use of web-based clinical trial management system and the risk-based monitoring strategies proposed by the FDA’s Draft Guidance for Industry, “Oversight of Clinical Investigations – A Risk-Based Approach to Monitoring,” promote a hybrid approach to monitoring. By combining a real-time central monitoring function with risk-based selective on-site source document verification, higher trial operation and clinical data quality can be made more affordable. This presentation covers the system design of the risk-based monitoring approach and its incorporation into a CTMS for the Neurological Emergencies Treatment Trials Network funded by NINDS. The risk-based monitoring module consists of three components, a central monitoring component, an on-site monitoring visit planner, and an on-site monitoring report. A complete monitoring check list is developed by the study team based upon regulatory requirements and the study protocol. All answers to check points that can be derived based upon the data in the study database is propagated into the central monitoring component, which is available to authorized users in real-time. The monitoring visit planner provides site trial performance risk assessment information for monitors to better prioritize on-site monitoring tasks. The monitoring report records all site performance based findings, identified by both on-site and central monitoring.\nA09\nInteractive Statistical Software for Estimating the Operating Characteristics of Cancer Phase I Clinical Trial Designed with Standard 3+3 Algorithm\n(1,2) Zhibo Wang, (3) Taofeek K Owonikoko, (1,2) Jeanne Kowalski, (3) Fadlo R Khuri, (1,2) Zhengjia Chen\n(1) Department of Biostatistics and Bioinformatics, Emory University (2) Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute, Emory University. (3) Department of Hematology and Medical Oncology, Emory University\nStandard 3+3 with or without dose de-escalation designs are still most widely used in cancer Phase I clinical trials for their simplicity and robustness. In clinical practice, it is necessary to provide the operating characteristics of clinical trials at the planning stage. Lin and Shih (2001) developed the formula for estimating the operating characteristics of the Phase I clinical trials with Standard 3+3 designs. Unfortunately, the formula might be difficult for clinical practitioners without advanced statistics knowledge to understand and implement. Therefore, we develop an interactive and user friendly software called Standard_3+3_design©TM which can estimate the trial’s operating characteristics, including the probability of a dose being chosen as the maximum tolerated dose (MTD); the expected number of patients treated at each dose level; target toxicity level (i.e. the expected dose limiting toxicity (DLT) incidences at the MTD); expected DLT incidences at each dose level; and expected overall DLT incidences in the trial. The software has the standalone version for free download and the web-based version for direct use online at Emory University website: http://sisyphus.emory.edu/Software_clinical.html .\nA10\nEscalation with Overdose Control Using all Toxicities and Time to Toxicity Data for Cancer Phase I Clinical Trials\n(1) Ye Cui (2) Taofeek K Owonikoko (3) Zhibo Wang (4) Fadlo R Khuri (5) Jeanne Kowalski (6) Zhengjia Chen\n(1) Georgia State University (2) Department of Hematology and Medical Oncology, Emory University (3) Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute. Department of Biostatistics and Bioinformatics, Emory University (4) Department of Hematology and Medical Oncology, Emory University (5) Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute. Department of Biostatistics and Bioinformatics, Emory University (6) Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute. Department of Biostatistics and Bioinformatics, Emory University\nThe primary purpose of cancer Phase I clinical trial which is a critical step in development of new drug against cancer is to determine the maximum tolerated dose (MTD) and schedule of new drug. It is usually a small study with limited data so that fully utilizations of all toxicities and time to toxicity data are essential to improve the trial efficiency and accuracy of MTD estimation. Chen et al. (2010) proposed a novel normalized the equivalent toxicity score (NETS) system which can fully utilize multiple toxicities per patient instead of a binary indicator of dose limiting toxicity (DLT). Cheung et al. (2000) developed the time of event (TITE) approach to incorporate time to toxicity data. Escalation with Overdose Control (EWOC) is an adaptive Bayesian Phase I design which can allow rapid dose escalation while controlling the probability of overdosing patients. In this study, we use EWOC as a framework and integrate it with the NETS system and TITE approach to develop an advanced Phase I design entitled EWOC-NETS-TITE. Simulation studies have been conducted to compare its operating characteristics with those of EWOC and standard 3+3 design. Simulation results demonstrate that EWOC-NETS-TITE can not only substantially improve the trial efficiency and MTD accuracy, but also allow patients to be entered in a staggered fashion and shorten trial length. A user-friendly software of EWOC-NETS-TITE is under development and will be available in the future.\nA11\nContinuous Tumor Size Change Percentage and Progression Free Survival as Endpoint of the First and Second Stage Respectively in a Novel Double Screening Phase II Design\n(1) Zhengjia Chen, (2) Ye Cui, (3) Taofeek K. Owonikoko, (4) Zhibo Wang, (5) Dong M. Shin, (6) Fadlo Khuri (7) Jeanne Kowalski\n(1) Department of Biostatistics and Bioinformatics, Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute, Emory University. (2) Department of Mathematics and Statistics, Georgia State University. (3) Department of Hematology and Medical Oncology, Emory University. (4) Department of Biostatistics and Bioinformatics, Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute, Emory University. (5) Department of Hematology and Medical Oncology, Emory University. (6) Department of Hematology and Medical Oncology, Emory University. (7) Department of Biostatistics and Bioinformatics, Biostatistics and Bioinformatics Shared Resource at Winship Cancer Institute, Emory University\nA phase II trial is an expedite and low cost trial to screen potentially effective agents for the following phase III trial. Unfortunately, the positive rate of Phase III trials is still low although agents have been determined to be effective in proceeding Phase II trials mainly because the different endpoints are used in Phase II (tumor response) and III (survival) trials. Good disease response often leads to, but can NOT guarantee, better survival. From statistical consideration, transformation of continuous tumor size change into a categorical tumor response (complete response, partial response, stable disease, or progressive disease) according to World Health Organization (WHO) or Response Evaluation Criteria In Solid Tumors (RECIST) will result in a loss of study power. Tumor size change can be obtained rapidly, but survival estimation requires a long time follow up. We propose a novel double screening phase II design in which tumor size change percentage is used in the first stage to select potentially effective agents rapidly for second stage in which progression free or overall survival is estimated to confirm the efficacy of agents. The first screening can fully utilize all tumor size change data and minimize cost and length of trial by stopping it when agents are determined to be ineffective based on low standard and the second screening can substantially increase the success rate of following Phase III trial by using similar or same outcomes and a high standard. Simulation studies are performed to optimize the significant levels of the two screening stages in the design and compare its operating characteristics with Simon’s two stage design. A real phase II clinical trial is provided to demonstrate the substantial improvement in trial efficiency and success rate of Phase III clinical trial as well as significant reduction in the cost and length of trial.\nA12\nIdentifying Continuous Phase II Endpoints Using Recist 1.1 Data Warehouse Measurements\n(1) Ming-Wen An, (2) Sumithra J. Mandrekar, (3) Xinxin Dong, (4) Axel Grothey, (5) Jan Bogaerts, (6) Daniel J. Sargent\n(1) Vassar College, (2) Mayo Clinic, (3) University of Pittsburgh, (4) Mayo Clinic, (5) European Organization for Research and Treatment of Cancer Headquarters, (6) Mayo Clinic\nBackground: The high failure rate (e.g. 50-60%) of Phase III oncology trials represents a major obstacle to therapeutic development. We previously reported that alternative cut-points to RECIST as well as alternate categorical metrics provided no meaningful improvement in overall survival (OS) prediction. In this work, we examined continuous RECIST measurement based metrics as alternative phase II oncology trial endpoints.\nMethods: We assessed the predictive ability of multiple candidate continuous metrics (first and last slope; first and last % change; longest duration of stability; and indicator of >10% increase from the previous assessment) based on longitudinal cycle-by-cycle tumor measurements from 1039 patients based on the RECIST 1.1 data warehouse. Data from 13 trials were randomly split (60:40) into training and validation sets. For each set of metrics, Cox models of OS were fit separately for breast, lung, and colorectal, adjusting for baseline tumor burden, stratifying by number of lesions (<=3 vs. >3) and study. Data were landmarked at 12-weeks. Predictive ability was assessed via c-index, Hosmer-Lemeshow goodness-of-fit-type statistics, and hazard ratios/p-values.\nResults: Metric predictive ability for OS differed across disease group, with poorest prediction in Breast (c-index range for the various metrics, training set: 0.51-0.61 Breast; 0.56-0.64 Lung; and 0.58-.72 Colorectal). While most metrics demonstrated a statistically significant association with OS, none of the continuous metrics performed better than the current categorical RECIST classification of CR/PR versus SD versus progression (c-indices: 0.58 Breast, 0.60 Lung, 0.65 Colon).\nConclusions: The continuous metrics we considered perform no better than RECIST in predicting patient survival across multiple tumor types. Ongoing work is focusing on issues of missing data and multicollinearity. At the present time, no metrics have been demonstrated to out-perform RECIST, indicating its use continues to be appropriate due to both its simplicity and its relative predictive ability.\nA13\nPower Analysis for Seamless Phase II/III Designs with Subpopulation Selection and a Change of Survival, Binary and Normal Endpoints\n(1) Qian Wu (2) Qiang Zhang\n(1) Department of Biostatistics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA\n(2) Radiation Therapy Oncology Group, American College of Radiology, Philadelphia, PA\nSeamless designs provide a powerful way to demonstrate the effect of a new therapy with less sample size and shorter duration. In a phase II/III design, the decision of stopping or continuing to confirmative phase III is based on the results of efficacy or futility from the learning phase II, such as the subpopulation selection design of Jenkins et al. (Pharmaceutical Statistics, 10:347-356, 2011). In recent oncology trials, progression free survival (PFS) is usually the primary endpoint in phase II and overall survival in phase III study. These two survival endpoints are correlated. Usually we assume exponential distribution for these survival times. However, we often observe non-constant hazard rate, so in this research we consider the more flexible Weibull distribution. In addition, other correlated novel endpoints have been proposed recently for phase II oncology trials, such as tumor size measured by imaging tools as continuous endpoint and/or the typical binary response rate. In order to account for the correlation between survival and binary, normal endpoints, Gaussian Copula distributions are used to simulate phase II/III data of different types. Based on the subpopulation selection methods by Jenkins (2011) for seamless phase II/III design, we conducted the power analysis and compared the results between designs using Weibull and exponential distributions. Similar designs and analyses were also done for normal and binary cases. We demonstrated that the type-I error rate is controlled at design level, such as 5% for each type of phase II endpoint with various possible correlations to overall survival, by Monte Carlo simulations. Multiple comparison issues are also discussed. Results show that consideration of correlation between different early endpoints and overall survival is important to protect type I and type II error rates and designs proposed here can be applied to a wide variety of settings including oncology trials.\nA14\nSeamless Phase II/III Design for Oncology Trials with Treatment Selection and a Change of Survival Endpoints\n(1) Qiang Zhang; (2) Qian Wu\n(1) Radiation Therapy Oncology Group, American College of Radiology, Philadelphia, Pennsylvania, USA (2) Department of Biostatistics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA\nSeamless phase II/III designs have been proposed in recent years and these designs are applied to different disease settings in oncology area. Advantages of these methods include great savings on sample size and trial duration. The goal of the phase II trial is to select the most effective regimen when there are multiple experimental arms. This is done through comparing each of the new treatments to a common control arm using a chosen endpoint, such as progression free survival. The winner will be tested further for overall survival in phase III study. We propose a new seamless design in which there are two phases, at the end of phase II, only one arm will be selected to go on to phase III. Patients accrued during phase II will be included in the definitive testing. We use group sequential method to design each component. Separate interim analyses are built in such that each phase can be stopped early for either efficacy or futility. Once sample sizes are derived, operating characteristics for the seamless II/III design are evaluated through simulations under the null and various alternative hypotheses. For these analyses, we account for possible correlations between two survival times. Issues such as multiplicity and accrual suspension are discussed. Control of the overall error rate can also be achieved based on asymptotic joint distributions of test statistics in addition to simulations. We will present expected sample sizes and stopping times using different design error rates and power. Savings on sample size and time will be compared to typical separate designs. A recently approved RTOG Head and Neck cancer study will be used as an example to illustrate how to design such trials. Results show that the newly proposed design has desired properties, which offer cost effectiveness, operational efficiency and most importantly, scientific innovation.\nA15\nApplication of a Seamless Phase II/III, Group-Sequential Design in the Development of A S. Aureus Vaccine\n(1) Jonathan Hartzel (2) Ivan Chan\n(1) Merck & Co., Inc. (2) Merck & Co., Inc.\nDeep sternal wound infection (DSWI) following cardiac surgery occurs in 0.5-3% of patients, is frequently attributable to Staphylococcus aureus, and is associated with increased morbidity/mortality. A novel vaccine candidate (V710) against S. aureus was shown to be immunogenic and generally well-tolerated in Phase I studies. Due to the low incidence of infection, a large efficacy study was required to assess the efficacy and safety of V710 in preventing S. aureus bacteremia and/or DSWI in patients undergoing cardiac surgery. An innovative event-driven, group-sequential design was employed to conduct a seamless Phase II/III double-blind, placebo-controlled efficacy study. Details of the study design will be discussed including its advantages over conducting separate Phase II and III studies and the logistical complications it can introduce. Additionally, the surprising results of the study, which was stopped early for low efficacy and for a potential safety signal, will be reviewed.\nA16\nPragmatic Seamless Design for an Efficacy Trial of Asthma Management with Reduced Cost\n(1) Mei Lu, (1) Edward Zoratti, (1) Dennis Ownby, (2) Douglas Roblin, (3) Christine Johnson, (1) Christine Joseph\n(1) Department of Public health Sciences, Henry Ford Health System (2) Department of Pediatrics and Internal Medicine, Medical College of Georgia (3) Kaiser Permanente Center for Health Research Southeast\nClinical trials are critical for medical decision-making but are fraught with problems, including low enrollment, poor compliance and costly implementation. Translation of results into practice can take years. Low cost, efficient strategies are needed for conducting trials in order to accelerate implementation. For this project we will use as a prototype a computer-tailored intervention for urban teens with asthma in the clinic that has already shown success as a school-based program. A seamless multi-site, open-label, randomized phase II/III trial has been designed to evaluate the intervention using a pragmatic approach and a comparative effectiveness design. Computerized methods will be implemented for patient identification, consent, enrollment and tracking. A centralized database with remote data capture is planned for case report form completion with a highly integrated electronic medical record (EMR). Routine clinic visits will serve as trial data collection points. Adolescents, aged 15-19 years, diagnosed with asthma (n=354), will be randomized to control or intervention arms consisting of 4 weekly computer sessions a minimum of 1 week apart, followed by a 6 and 12 month follow-up survey. The phase II trial will include the first 250 patients with primary assessment of study feasibility and reliability of EMR data. The primary endpoint is asthma control, as determined by the Asthma Control Test. Secondary outcomes include asthma exacerbations, symptom-days, symptom-nights, days of restricted activity and school/work days missed. Two interim looks are proposed when 40% and 70% (Phase II) of patients are enrolled. Phase II of the trial is funded through NHLBI. We will seek additional funding for a Phase III component if there is an indication of treatment effect at interim analysis. Currently, we are in the planning phase, have received IRB approval, and will begin patient enrollment in March, 2013. Study results will inform strategies for program dissemination.\nA17\nRevisiting the Question: To Adjust or Not to Adjust for Baseline Covariates in Randomization and/or in Analysis of Clinical Trials?\n(1) Yuko Palesch (2) Yanqiu Weng (3) Wenle Zhao\n(all) Medical University of South Carolina\nBaseline covariate adjustment in randomization and/or in analysis has yielded a vast literature in clinical trials. The contentious issues involve statistical operating characteristics (SOC), randomization performance metrics, and clinical interpretation. Through simulation using bootstrapping technique on data from a moderate-sized clinical trial (N-400) with two treatment arms, we evaluate six scenarios that are combinations of each of the three randomization methods (simple (SR), stratified permuted block (SPB), and a recently developed minimal sufficient balance algorithm (MSB)) with each of the two analysis approaches (with and without adjustment of baseline covariates with strong prognostic values). The results show that, as expected, the type I error probability for the adjusted analysis is slightly conservative for both SPB (0.0493) and MSB (0.0456), and more so if the analysis is not adjusted for the covariates (0.0319 and 0.0305, respectively). Power is similar among the three randomization methods for adjusted analyses, but the unadjusted analyses yield substantially reduced power. For randomization metrics, SR and MSB are ideal in the probabilities of deterministic assignments (0) and correct guesses (0.5 and 0.54, respectively), but the MSB best controls the probability of significant imbalances in the covariates between the treatment arms. We conclude that SR with covariate-adjusted analysis may be the best approach due to its SOC and simplicity of implementation, but with the caveat that it may be more vulnerable to challenges in trial result interpretations due to potential covariate imbalances between the treatment groups. MSB, also with covariate-adjusted analysis, may alleviate that concern while maintaining similar SOC. One should weigh the benefit in the SOC and randomization metrics against the effect of potential covariate imbalance on clinical interpretation. Additional scenarios (under different Ns and other SOC parameters) also will be presented.\nA18\nEstimating Heterogeneous Treatment Effects from Data Generated in Randomized Controlled Trials Based on the Parallel Group Design Using Predictive Baseline Covariates\n(1) Ruediger Paul Laubender (2) Ulrich Mansmann\n(1) Institute of Medical Informatics, Biometry, and Epidemiology (IBE), Ludwig-Maximilians-University Munich\n(2) Institute of Medical Informatics, Biometry, and Epidemiology (IBE), Ludwig-Maximilians-University Munich\nOne of the aims of ‘individualized medicine’ is to allocate the optimal therapy to a patient when several treatment options are available and when it is known that patients differently react to these treatments. The presence of heterogeneous treatment effects (also known as subject-treatment interactions) can be estimated by study designs which allow for replication of the treatment effects like the repeated crossover design (Senn 2001). However, randomized controlled trials (RCTs) allowing for replications are not possible when treatments irreversibly change the patients or where carryover is an issue (e.g. treatments used in oncology, psychiatry and infectiology). In such situations only RCTs based on the parallel group designs can be used. From a statistical point of view, we only observe the marginal distributions of the responses to the treatments of interest from RCTs based on the parallel group design but we cannot observe the joint distribution of these responses in contrast to designs allowing for replications. For normally distributed responses, it might be still possible to reconstruct the joint distribution and hence to identify heterogeneous treatment effects from a RCT based on the parallel group design by redefining a predictive baseline covariate as an indicator for the individual treatment effect (ITE) and by assuming a multivariate normal distribution. Given these assumptions we derive estimators for reconstructing the joint distribution, for the ITE and for the probability that an individual will profit from treatment A compared to treatment B given his or her value of the predictive covariate. This is an estimator easily understandable by physicians and patients and cannot be derived from a usual regression model with a treatment-covariate interaction. A simulation study was performed to judge the performance of the newly proposed estimators. Additionally, our methods are applied to a data examples used by Gadbury et al. (2000, 2001).\nA19\nLumping and Splitting Approach to Analyzing High Dimensional Sparse Counts Data\n(1) Zhibao Mi (2) Joseph Collins\n(1) VA Cooperative Studies Program (2) VA Cooperative Studies Program\nIt is not unusual for hundreds or even thousands of clinical endpoints to be collected from individual subjects to assess the safety or efficacy of a drug/device in clinical trials/studies, but events from the majority of clinical endpoints are rare. The challenge to analyzing high dimensional sparse data is how to balance analytical consideration for statistical inference and clinical interpretation with precise meaningful outcome of interest at intra-categorical and inter-categorical levels. Lumping or grouping similar clinical events into a composite category has the statistical advantage of increasing testing power, reducing multiplicity size, and avoiding competing risk problem; however, too much or inappropriate lumping would jeopardize the clinical meaning of interest and external validity. Whereas splitting or keeping each individual event at its basic clinical meaningful category can overcome the drawbacks of lumping, this practice may create analytical issues of increasing type II error, multiplicity size, and competing risks and having a large proportion of endpoints with rare events. It seems that lumping and splitting are diametrically opposite approaches, but in fact, they are complementary. Both are essential for high dimensional data analysis. In this report, a lumping method is proposed based on the correlations between a treatment/exposure and clinical endpoints and biological information to reduce data dimension. It is combined with a splitting method using Poisson and/or negative binomial models to adjust for over-dispersion and using zero inflated Poisson models for those endpoints with substantial number of zeros. The multiplicity is adjusted based on the results from lumping and splitting analyses by controlling the false discovery rate. The clinical relevance of the proposed analytical approach was tested and validated using an example with 372 safety binary endpoints from a VA double-blinded randomized multi-center clinical trial. The safety evaluation was based on the results of lumping and splitting analyses.\nA20\nEvidence Informed Missing Data Imputation for Cognitive Function Tests within RCTs\n(1) Janice Pogue, (2) Martin O’Donnell, (3) Salim Yusuf\n(1) McMaster University, (2) McMaster University and Galway University Hospital, (3) McMaster University\nIn order to avoid bias in trial results, we need to ensure complete data collection of outcomes for all participants. Unfortunately in trials that measure cognitive function, some participants who have already experienced disability may be unable or unwilling to complete follow up assessments. These assessments are not missing at random, yet trials frequently use statistical models that assume they are. Many authors have suggested that we use the reason that the assessment is missing to select the analytic approach for trials (Simes, Stat Med 1998; Little, NEJM 2012; Schott, NEJM 2012), but few examples of this methodology exist within published trials. We suggest that reasons for missing data be systematically collected in a form that can be directly related to each trial participant’s probability of cognitive decline. Reasons for missing data are provided by the site in a format that can be directly related to the probability (confirmed, probable, possible, or unrelated) that each individual has experienced cognitive decline. This information, together with published norms for the scale in disabled populations, is then used within the method of multiple imputation to make evidence informed assumptions for each trial participant. We provide simulation data for a trial measuring cognitive decline using the Digital Substitution Scale and demonstrate biased trial results for the common methods that assume random missingness. The systematic collection of reasons for missing assessments can provide a scientific rationale for handling missing data to directly inform the imputation methods we use to analyze trial results, not as a sensitivity analysis, but within the primary models. This method should decrease the likelihood of biased trial results due to informative missing data.\nA21\nStandardizing Early Phase Trial Documentation and Processes\n(1) Debbie Sherratt (2) Sarah Flynn (3) Louise Flanagan (4) Sarah Brown (5) Nina Thenagaran (6) Avie-Lee Coney\nClinical Trials Research Unit, University of Leeds\nIn 2009 Myeloma UK set out an innovative research model to rapidly and systematically address the critical challenges that are slowing down research, the development of, and access, to new drugs, developing an Early Phase Clinical Trials Network. Subsequently, in November 2009, the Clinical Trials Research Unit (CTRU) at the University of Leeds successfully became the Myeloma UK (MUK) Clinical Trials Coordinating Office (CTCO) for these early phase clinical trials. In the three years the network has been running, we have developed a number of phase I and phase II processes, and associated documents, which may be transferable between early phase trials, To date we have set-up five early phase trials, 1 in follow-up, 1 in recruitment, and 3 in trial set-up - each building on the standardization of the early phase processes developed. Standardized processes and documentation that have been developed include: consenting and registering participants, case report forms, database specifications, and data reports, dose escalation documentation, trial endpoints and safety reporting. The differences in processes between phase I and phase II trials will be discussed as well as the advantages and challenges of standardizing processes in each phase and across the two phases. The standardization of processes within the MUK CTCO has improved the efficiency of the set up of trials, reduced the need for multiple document reviews and allows familiarity with processes and paperwork within the trial team. Furthermore, it is hoped that standardization will facilitate expedited center set up and aid the conduct of trials by allowing centers to be familiar with the processes required and the data collection documentation to be used.\nA22\nSocial Media in Clinical Trial Recruitment and Retention\n(1) Evan Hempel (2) Stephanie Millin\n(1) KAI Research, Inc. an Altarum Company (2) KAI Research, Inc. an Altarum Company\nOne may argue study participants are the most important part of clinical research; without them there would be no clinical trials. Sponsors spend a significant portion of study budgets on participant recruitment: developing flyers, radio/television advertisements and notices in local newspapers/newsletters; yet these costly techniques are no guarantee of success. In the ever changing world of the internet and social media many individuals turn to the Web and virtual networks for answers to medical concerns. KAI-Research, Inc. (KAI) has firsthand experience demonstrating how social media can be an inexpensive and effective tool for quickly meeting recruitment goals as well as improving study retention. In a recent multi-site neonatal clinical trial, one hospital utilized their Facebook page to post information about the study. This site easily enrolled twice as many infants as all the other sites combined and it cost $1,000 less per subject than the second most successful site. The rapid enrollment enabled the trial to conclude six months ahead of schedule, creating noteworthy cost savings. In a longitudinal study of young adults, some sites found participants to be more responsive to a personal Facebook message then phone calls or e-mails in study follow-up. Thus, Facebook greatly improved the trial’s retention rate. Using social media in clinical trials also has its challenges. At present there are no FDA regulations or GCP guidelines to assist with implementing a social media strategy while maintaining privacy and HIPAA compliance. IRBs differ in their opinion of how social media should be used in trials. Being informed about the issues and concerns as well as taking steps to address ethical considerations are essential in the successful implementation of social media in clinical trials.\nA23\nSystematic Review of Strategies to Reduce Attrition in Randomised Trials\n(1) Valerie Brueton, (1) J. Tierney, (1) S. Stenning, (2) I. Nazareth, (1) S. Meredith, (3) S. Harding, (2) G. Rait\n(1) UK Medical Research Council, Clinical Trials Unit (2) University College London, Research Department of Primary Care and Population Health (3) UK Medical Research Council, Social and Public Health Sciences Unit\nBackground: Attrition from randomised controlled trials (RCTs) can introduce bias and reduce study power affecting the generalisability, validity, and reliability of results. Objectives To quantify the effect of strategies to reduce attrition from RCTs.\nMethods: Eligible trials were randomised evaluations of strategies to reduce attrition embedded within RCTs. We searched bibliographic databases, trial registers, Society for Clinical Trials meeting abstracts and trial report and review reference lists. We also surveyed UK clinical trials units. Heterogeneity in strategies was explored with subgroup analyses, and the fixed-effect model used to pool results.\nResults: 39 RCTs were eligible with data available from 37of these. Six broad types of strategies to reduce attrition were evaluated. Most aimed to improve postal or electronic questionnaire response. Response was increased by: adding a monetary incentive (RR 1.18; 95% CI 1.09-1.28), higher value incentives (RR 1.12;1.04-1.22), offering monetary incentives (RR 1.25;1.14-1.38), shorter questionnaires (RR 1.04;1.00-1.08) and disease-relevant questionnaires (RR 1.07;1.01-1.14). Based on results of single trials, recorded delivery of questionnaires (RR 2.08;1.11-3.87); a “package” of postal strategies (RR 1.43;1.22-1.67); open trial design (RR 1.37;1.16 -1.63) increase response/retentionand, but interviews were inferior to postal questionnaires (RR=0.90;0.88-0.92). There is no clear evidence that questionnaire response/participant retention is improved by the following: a charity donation, adding or offering gifts, “enhanced” letters, priority post, extra reminders to participants, questionnaire question order, site reminders, sending questionnaires early, long and clear questionnaires, behavioural or case management strategies, or that monetary incentives are better than an offer of entry to a prize draw, or telephone surveys are better than a questionnaire plus monetary incentive. Conclusion Offering and giving monetary incentives and short and relevant questionnaires improve response. Other promising strategies need further evaluation. Application of these results depends on context and existing trial follow-up procedures.\nA24\nQ-QAT (Quanti-Qualitative Appointment Timing): A Simple Technique to Elucidate Key Recruitment Issues in RCTs\nJenny Donovan, Sangeetha Paramasivan, Sean Strong, Caroline Wilson\nUniversity of Bristol\nBackground: Recruitment to pragmatic randomized controlled trials (RCTs) is acknowledged to be difficult, and very few interventions to improve recruitment have proved to be effective. We present a simple technique employing mixed research methods that can identify key recruitment issues in RCTs that can then be fed-back as part of a complex intervention to improve RCT recruitment.\nMethod: The outline of the technique emerged from a programme of qualitative research undertaken to investigate the process of recruitment in six RCTs with recruitment difficulties. Recruitment appointments were audio-recorded and transcribed, and a simple qualitative content analysis was performed. The Q-QAT technique involved coding recruitment appointment transcripts for time devoted to explaining each of the RCT arms and the RCT itself. The technique was applied to two RCTs with very different clinical contexts, organizational issues and recruitment processes to refine its development. Comparisons of Q-QAT data were made between clinical centres and individual recruiters. Findings were fed-back to recruiters and recruitment rates re-assessed.\nResults: The Q-QAT technique was applied to two RCTs, and coding methods were adapted to take into account the differences in design and organization. In both RCTs there were considerable variations in the quantity of time spent by different recruiters on the RCT arms and RCT itself. In RCT1, Q-QAT data showed that similar proportions of time were devoted to explaining details of each treatment arm (supporting qualitative findings that the arms were presented in a balanced way), but very little time was spent explaining the RCT itself. In RCT2, treatments were presented very differently by specialists in different centres. Findings were presented to recruiters and discussed. Subsequent changes in Q-QAT patterns, qualitative findings and recruitment rates were seen.\nConclusion: The Q-QAT method is relatively simple to apply and appears to offer opportunities to encourage improvement in recruitment practice.\nA25\nThe Use of Site Engagement Technology (SET) to Increase Patient Recruitment and Retention in Clinical Trials: A Case Study\nVivek Murthy\nBrigham and Womens’ Hospital/Harvard Medical School\nSite engagement technologies (SETs) represent a novel approach to solving a critical problem in clinical trials: low recruitment and retention of patients. This problem has implications for cost, study validity, and the speed by which new therapeutics are made available to patients. The technology we have developed enables sponsors and/or CROs to motivate and engage study sites to improve recruitment and retention of patients - and the overall success of the clinical trial. In contrast to traditional trial investigator meetings (IMs), which connect investigators and the sponsor for a limited period of time, site engagement technology generates a sense of community and support from trial activation to closeout. SET tools focus on four key areas of the investigator site-sponsor/CRO interaction: education (on-demand video training and e-learning strategies such as quizzes), motivation (leader boards, peer and sponsor recognition awards, site ranking for milestones), communication (customized email, kudos recognition features, document change notifications), and documentation (repository for memos, FAQ, training forms, protocol, amendments, and more). Additionally, our cloud-based innovation platform allows trials to create specialized tools that can increase the rate of protocol adherence (such as the patient visitation guide and decision tracking features). We intend to highlight multiple case studies in which industry-sponsored trials implemented site engagement technologies. The results demonstrate increased rates of patient enrollment and retention and a >90% rate of technology adoption by study sites.\nA26\nStatistical Modeling of Recruitment for Multi-Site Clinical Trials – Current Status\n(1) Kousick Biswas (2) Tyson Holmes\n(1) Cooperative Studies Program, US Department of Veterans Affairs, (2) Dept. of Psychiatry and Behavioral Sciences, Stanford University School of Medicine\nVA Cooperative Studies Program (CSP) conducts multi-site, randomized clinical trials using the VA medical centers around the country as the participating sites. In general, a typical CSP conducted clinical trial involves 10 to 200 medical centers as sites to recruit patients in the range of 200 – 10000. So, timely recruitment is a very important factor for timely completion of these studies within the allocated budget. This has been a chronic issue for the CSP studies. During the planning stages, as a normal practice, the length of the study and required budget are estimated based on an assumed fixed or temporally invariant recruitment rate throughout the recruitment period. From experience it is evident that recruitment rate in a multi-site clinical trial is not fixed or temporally invariant so that use of this simple model causes errors in prediction of study length. This paper provides a summary of currently used modeling techniques that account for the various uncertainties in rate of recruitment during study conduct to predict realistic recruitment trajectories.\nA27\nData Monitoring Committees in China\n(1) Tingting Li (2) Kent Koprowicz (3) Xiaohui Huang (4) Ping Xu\n(1) Axio Research, LLC. (2) Axio Research, LLC. (3) Axio Research, LLC. (4) Axio Research, LLC\nData monitoring Committees (DMCs), also known as Data and Safety Monitoring Boards (DSMBs) are commonly used in clinical trials where major health outcomes are expected (e.g. death, progression of disease). DMCs periodically review interim data while a trial is ongoing in an effort to ensure the ethical conduct and scientific integrity of the study. DMCs have been an important component of many government and industry sponsored US and European clinical trials for decades. While there has been a recent large increase of clinical research in China and other Asian countries, the use of DMCs remains novel. In the past year we have served as the independent statistical group for several DMCs monitoring oncology trials conducted mainly in China. In this talk we share our experience of working with China-based DMCs. In particular we note the characteristics and challenges for data monitoring in China in regard to constituting and implementing government guidelines, educating local DMC members on responsibilities and process, and meeting planning and logistics.\nA28\nMorbidity Adjudication and Tracking During DCCT/EDIC\nJye-Yu C. Backlund, Stephan Villavicencio, Wanyu Hsu, Lisa Diminick, Patricia A. Cleary and the DCCT/EDIC research group\nThe George Washiington University, The Biostatistics Center\nConcerns about the cardiovascular (CV) events for the treatment of diabetes have served to emphasize the need for accurate assessments of CV primary endpoints in clinical trials. The Epidemiology of Diabetes Interventions and Complications (EDIC), the observational follow-up of the Diabetes Control and Complications Trial (DCCT) cohort, established a surveillance and adjudication process to classify major CV outcomes in 1994. During annual visits, EDIC subjects were asked to report any CV events and the study sites notified the coordinating center of all reported events. Events were adjudicated by the Mortality and Morbidity Review Committee (MMRC), which was masked to DCCT treatment group and glycosylated hemoglobin levels. Events were adjudicated using medical records, ECG findings, and cardiac enzyme levels. Among 670 events reported between 1994-2012, 66 (9.9%) were non-fatal myocardial infarction (MI), 158 (23.6%) revascularization, 22 (3.3%) stroke, 8 (1.2%) congestive heart failure (CHF), 197 (29.4%) angina, 192 (28.7%) arrhythmia, and 27 (4.0%) transient ischemic attack (TIA). Only 280 (41.8%) of the 670 reported events were classified as meeting adjudication criteria, while 125 (18.7%) were not. Another 247 (36.9%) reported events could not be adjudicated due to insufficient documentation. As shown in the Figure, completed adjudication was highest among reported MI, revascularization, and CHF (all > 60%) and lowest among angina, arrhythmia and TIA (all\nA29\nHow to Get the Most Out of Your Courtship with FDA - An FDA Medical Device Reviewer’s Perspective\nPhyllis M. Silverman\nCenter for Devices and Radiological Health, FDA Office of Surveillance and Biometrics Division of Biostatistics\nEfficient and effective communication between FDA and Industry for medical device approval or clearance is crucial at all points along the application timeline. This talk will explore the different options available to Industry in interacting with FDA both before and during the review process, including the different types of meetings and their purpose. It will also present specific elements of study design and statistical analyses that industry sponsors should be prepared to discuss and provide answers to both at meetings and by way of formal written review correspondence. The top 10 communication “crimes” will be revealed.\nA30\nHas the Voluntary Harmonisation Procedure Been a Successful Attempt to Streamline the Approval Process for Multinational Clinical Trials in the EU? Examining Applicants’ Experience to Date with the GTFG’s Initiative\nDavid Smith\nVerius Limited\nContrary to the intentions of the Clinical Trials Directive, divergent practices persist between member states reviewing applications for clinical trials in the EU. A complex network of diverging requirements and practices presents a complicated and unfavourable approval process, particularly for multinational clinical trials (MN-CTs). It is these issues that the Clinical Trials Facilitation Group was targeting in 2009 when it introduced a voluntary harmonisation procedure (VHP) for MN-CTs. Three years on, use of the VHP is far from widespread and published information relating to applicants’ experience with the procedure is negligible. The purpose of this research was to determine the level of success that the VHP has achieved in fulfilling its objectives, by targeting those who have used the procedure. An online questionnaire targeted at MN-CT applicants was distributed to regulatory professionals, and the responses obtained highlight the positive aspects of the VHP, but also the need for improvements. Applicants’ experience with the VHP has been generally positive, with a low occurrence of divergent decisions, and a reduced regulatory burden associated with the harmonised approval process. Timelines for clinical trial approval have not been markedly reduced, however, and significant issues remain that are likely to continue to dissuade prospective applicants from utilising the VHP. The VHP is clearly a step in the right direction, and applicants’ experience with the procedure hints at the likely impact of a Regulation on clinical trials, as proposed by the European Commission in July 2012. The results of this research suggest that the revised legal framework (expected in 2016) will have a positive impact on the clinical research environment in the EU, with applicants first in line to benefit through a faster, more flexible and fully harmonised approval process for clinical trials.\nA31\nMinimizing Reconciliation Between Safety and Clinical Databases\nSean Neal\nMedidata Solutions\nWith patient safety in mind, global regulatory agencies enforce stringent guidelines for timely reporting of safety events by investigator sites and sponsors. This process has been estimated at ~2.15 FTE days/SAE just in sponsor effort and does not even take into account the effort required by site personnel. Estimates from the Tufts Center for the Study of Drug Development suggest study coordinators spend between 10 and 20% of their total work effort on trials just on AE/SAE reporting. Besides consuming valuable safety resources in re-keying data that is already captured in EDC into the safety system, the reconciliation of the data is a huge burden. By way of example, an analysis of just SAE rates from completed studies reporting results over the past 5 years, 2007-2011, from Clinicaltrial.gov (n=4,221) demonstrates the magnitude of the reconciliation effort that would be needed. Overall while the majority of trials, 59%, exhibit SAE rates of 10% or less, 10% of trials have SAE rates between 10-and 20%, 17% of trials have SAE rates between 20 and 50%, and 6% of trials exhibited SAE rates of greater than 50%. As regulatory environment continuous to evolve, the new pharmacovigilance (PV) legislation that takes effect across the European Union (EU) in July 2012 further challenges the manual, paper-based practices. The major overhaul of post-marketing expedited reporting requires Marketing Authorization Holders (MAHs) starts reporting not just SAEs but also suspected non-serious adverse reactions within 90 days of the legislation’s commencement. This presentation will discuss how new technologies in clinical trials help researchers to minimize or fully eliminate such reconciliation between two databases, thus streamlining safety reporting process, improving regulation compliance and enabling safety team focus on what they do best - triage case and detect safety signals as early as possible.\nA32\nSystematic Reviews or Relevant Randomised Control Trials, Which Should I Believe? A Dilemma for Policy Makers\nMartin Tickle\nSchool of Dentistry, University of Manchester\nSystematic reviews sit at the top of the evidence-based hierarchy but their findings can sometimes be at odds to the results of individual trials. This lack of agreement can confuse policy-makers when deciding how best to deploy resources.\nObjectives: To compare the evidence presented by a systematic review and a large pragmatic trial and make suggestions for how policy-makers should approach evidence-based decision-making.\nMethods: A case study that compares the findings of a cluster RCT of fluoride varnish delivered as a public health intervention in schools with those of a Cochrane systematic review of fluoride varnish.\nResults: The systematic review reported that a 46% preventive fraction for dental caries resulted from two or more fluoride varnish applications a year. The cluster RCT randomised 190 schools into test and control groups and 1473 (test) and 1494 (control) 7-8-year-old children participated in the trial. Children in the test group were offered 3 applications of fluoride varnish each year over a 3-year follow-up period; >60% of children received all 9 applications and >90% received >6 applications. The trial could find no statistically or clinically significant difference between test and control groups. Reasons for the differences in the findings between the review and the RCT are identified and discussed, these include fluoride exposure and caries risk in different historical and geographical populations; the setting in which trials are conducted; difference between pragmatic and effectiveness trials; quality of older trials and the potential for publication bias.\nConclusions: Systematic reviews should be frequently updated and where possible sub-group analysis should be undertaken according to population risk and trial design. Policy makers need access to relevant methodological expertise to support their decision-making. A recent, adequately powered, well-designed and well-conducted, pragmatic trial conducted in a relevant population is more useful to policy-makers than a systematic review.\nA33\nSwitching from Non-Inferiority to Superiority in an Adaptive Setting: Some Notes\n(1) Dario Gregori, (2) Giada Morpurgo, (3) Federica Zobec\n(1) University of Padova, (2) ZETA Reserach Srl\nIn the presence of competing primary targets, it is customary to first determine the sample size w.r.t the primary endpoint and then make a powerr analysis of the remaining target, to make sure that enough power analysis is waaranted. EMEA and FDA allow both switching from non-inferiority to superiority in both traditional and adaptive designs. However, the issue of the case highlighted above is not weel addressed in literature, i.e.: how we should re-estimate sample size after each interim analysis when bot primary end non-primary enpoints are evaluated. Ours paper aimed at showing practical consequences in the design of a safety device when either non inferiority and superiority should be verified in order to move further on to next step.\nA34\nUsing Adaptive Designs to Build a Behavioural Change Intervention in a Complex Intervention Randomised Controlled Trial\nJohn Norrie, Jill Francis, Heather Waterman, Jen Burr, Ruth Thomas, Mark Pilling\nUniversity of Aberdeen, City University London, University of St. Andrews, University of Manchester\nIt is routinely challenging picking the optimal intervention when designing complex intervention trials changing behaviours to improve patient reported outcomes, from many different interventions with competing evidence of varying quality and substance, backed by different underlying theories of behaviour change. To separately trial each one isn’t is usually feasible. To select one or two to take forward to trial runs high risk of missing the most effective. And what works for some may not work for others. Although adaptive designs are well developed for drug trials, they are less common in non drug complex intervention settings. We were faced with building the optimal intervention to increase compliance (adherence) with eye drops for managing intra ocular pressure (IOP) in glaucoma. This talk will review the design for an explanatory trial using a mixture of two adaptive type designs to find the best package of measures to change behaviour and improve compliance with the eye drops, which is often poor. The design mixes a standard Adaptive Experimental Design (AED) to build the best combination of 5 generic interventions (including educational information, self efficacy, and barriers to change), delivered either individually or in groups, with real time assessment of effectiveness via a study web portal leading to rapidly evolving stages of randomisation, using a play the winner algorithm, to identify at a population level what intervention works for most people. Alongside this, we integrate a Sequential Multistage Adaptive Randomised Trial (SMART) design to pick up those participants still non compliant and then identify individualised interventions that work for them. This novel application of two adaptive designs promises to provide a useful tool to improve trial design in challenging areas such as behavioural change, and so increase the likelihood of finding the best interventions, uncovering benefit at both population and individual level.\nA35\nDesign of Sequentially Randomized Trials for Testing Adaptive Treatment Strategies\nSemhar Ogbagaber, Abdus Wahed\nUniversity of Pittsburgh Department of Biostatistics\nAn adaptive treatment strategy (ATS) is an outcome-guided treat-ment algorithm approach that allows personalized treatment of complex diseases based on patients’ need, disease and treatment history. Complex diseases such as AIDS, depression, cancer involve several stages of treatment due to multifaceted disease paths. Sequential multiple assignment randomized (SMAR) designs are generally used to draw simultaneous inference about multiple ATS’s, where patients are randomized to available treatment options at dierent stages as they become eligible to receive those treatments. However, design issues such as sample size and power for such designs have not been addressed adequately in the literature. In this article we propose a sample size formula based on Wald-type statistic for comparing multiple ATS’s based on a continuous endpoint through a SMAR trial. We show via simulation that the proposed sample size formula maintains the nominal power and hence provides a useful tool for tpractitioners to design a SMAR trial ensuring adequate power.\nA36\nAdaptive Adjustment of the Randomization Ratio Using Historical Control Data\n(1) Brian P. Hobbs (2) Bradley P. Carlin (3) Daniel J. Sargent\n(1) Department of Biostatistics, The University of Texas MD Anderson Cancer Center (2) Division of Biostatistics, University of Minnesota (3) Department of Health Sciences, Mayo Clinic\nProspective trial design often occurs in the presence of “acceptable” (Pocock, 1976) historical control data. Typically this data is only utilized for treatment comparison in a posteriori, retrospective analysis to estimate population-averaged effects. We propose an adaptive trial design in the context of an actual randomized controlled colorectal cancer trial. The proposed trial implements an adaptive randomization procedure for allocating patients aimed at balancing total information (concurrent and historical) among the study arms. This is accomplished by assigning more patients to receive the novel therapy in the absence of strong evidence for heterogeneity among the concurrent and historical controls. Allocation probabilities adapt as a function of the effective historical sample size (EHSS), which characterizes the relative informativeness defined in the context of a piecewise exponential model for evaluating time to disease progression. A Bayesian hierarchical model is used to assess historical and concurrent heterogeneity at interim analyses and to borrow strength from the historical data in the final analysis. Balancing total information with the adaptive randomization procedure leads to trials that on average assign more new patients to the novel treatment when the historical controls are unbiased or slightly biased compared to the concurrent controls. Large magnitudes of bias lead to approximately equal allocation of patients among the treatment arms. Using the proposed hierarchical model to borrow strength from the historical data, after balancing total information with the adaptive randomization procedure, provides preposterior admissible estimators of the novel treatment effect with desirable bias-variance trade-offs.\nA37\nImplementing Unbalanced Assignment by Minimisation\n(1) Colin Everett (2) Samantha Hinsley (3) Sarah Brown\n(1) Clinical Trials Research Unit, University of Leeds, UK. (2) Clinical Trials Research Unit, University of Leeds, UK. (3) Clinical Trials Research Unit, University of Leeds, UK\nMinimisation, a dynamic method of assigning patients to a study group in a randomised controlled trial, ensuring the treatment groups are similar with respect to a number of specified factors, has often been used for trials where an equal balance between one or more groups is desired. Although equal allocation between two or more groups is common and efficient from a statistical view, there are good reasons for desiring an unequal allocation: one or more interventions may be considerably more expensive, have more significant associated risks, be more inconvenient - leading to larger numbers of patients withdrawing, or may be more limited in its supply or those trained to deliver it. Unbalanced assignment can be easily achieved using simple randomisation and through permuted block randomisation, but the method for achieving unbalanced assignment through minimisation may not be so straightforward. Motivated by the desire to use minimisation to assign patients to interventions with unequal allocation in our trials, we review the methods proposed in the literature for assigning patients to their groups by minimisation which aims to achieve unbalanced assignment. We briefly lay out the underlying methods of the different techniques proposed, and discuss what is involved, along with the challenges faced, when putting these methods into practice, including the additional work undertaken when testing what may be a novel process. Using simulation, we consider whether or not the targeted assignment ratio is reached in both the short and long term, the chances of predicting the next assignment, and impact of using and varying the random assignment probability. We also investigate the effect of changing the number of minimisation factors and their corresponding levels on the reliability of the methods, including looking at the effect of requiring the preservation of the assignment ratio within each centre of a multi-centre trial.\nA38\nA Likelihood-Based Approach to Selecting Doses with Acceptable Toxicity in a Standard Algorithm-Based Phase I Cancer Trial\n(1) Cody Chiuzan (2) Elizabeth Garrett-Mayer\nMedical University of South Carolina Hollings Cancer Center\nIn oncology, the standard ‘3+3’ design has passed the test of time and survived various proposals to adjust the sample size, or other dose-escalation dynamics. The objective of algorithm-based designs is to identify the maximum tolerated dose (MTD, assuming that both efficacy and toxicity increase with dose. While this assumption is reasonable for cytotoxic agents, new cancer treatments such as immunotherapies may not follow this principle. For such treatments, where it is anticipated that lower doses may be as efficacious as higher doses, the objective of the phase I trial might instead be to identify doses with acceptable toxicity, which can be moved forward for further testing. We offer a likelihood-based approach for identifying doses with acceptable toxicity consistent with the decision rules used in the standard algorithm. Without incorporating any prior beliefs, our proposed method is based on the evidential paradigm (Royall 1997) that uses likelihood ratios to measure the strength of statistical evidence for one hypothesis over the other. Once toxicity responses at a certain dose level have been observed, the likelihood-ratio (LR) under the null and alternative hypotheses is calculated and compared to a certain k threshold (level of evidence). Given a true toxicity scenario, the following statistical characteristics are computed for different choices of k: i) probability of weak evidence, ii) probability of favoring the null under the null (analogous to 1-alpha), iii) probability of favoring the alternative under the alternative (analogous to 1-beta), and iv) probability of non-escalation under the null. The k value that displays the best operating characteristics is chosen to be the threshold for the likelihood-ratio. If ln(LR) > ln(k), then the dose is considered acceptably safe; otherwise the dose is thought to be unacceptably toxic. Simulation results are presented for different combinations of testing hypotheses, k thresholds, and various true toxicity rates.\nA39\nDesign and Analysis of Cluster Randomized Crossover Trials with Binary Outcomes with Application to Intensive Care Research\n(1) Andrew Forbes (2) Muhammad Akram (3) Rinaldo Bellomo (4) Michael Bailey\n(1) Monash University, (2) Monash University, (3) Monash University and the Australia and New Zealand Intensive Care Society Centre for Outcome and Resource Evaluation, (4) Monash University and the Australia and New Zealand Intensive Care Society Centre for Outcome and Resource Evaluation\nThere are a number of near-universal interventions applied in the intensive care unit (ICU) setting that may have an effect on mortality, such as intravenous caloric delivery in patients receiving nutrition or oxygen level targeting in mechanically ventilated patients However, assessment of the effects of these interventions, which are likely to be small (eg 2% absolute reduction in mortality), are hindered by the need for the interventions to be applied at intensive care unit level together with the limited number of intensive care units in Australia. As a result, parallel-arm cluster randomized trials to detect small intervention effects cannot be sized appropriately. Efficiency can be gained by incorporating treatment crossover into the design of these trials, leading to so-called cluster randomized crossover trials. However, the development and assessment of these designs in the literature has been limited, and has been primarily focussed on continuous outcomes with associated linear mixed models rather than for binary outcomes. In this presentation we report on our recent analytical and simulation work evaluating estimation methods for cluster randomized crossover trials with binary outcomes. Using an underlying marginal model, we provide modifications of sample size formulae to accommodate binary outcomes and period effects, and variance expressions incorporating unbalanced sample sizes across clusters and across periods within clusters. We report on the adequacy of these expressions using a simulation study based on the cluster size variability and within- and between-period correlations observed in the Australian adult patient intensive care database. Our results indicate that cluster summary and marginal model estimators provide generally appropriate size, power and confidence interval coverage, however marked differences in efficiency occur in certain settings, and we provide rationale for these differences. We also discuss the potential for extension to multi-period designs and their feasibility in the intensive care research setting.\nA40\nA New Paradigm for Large, Simple Trials Based on Electronic Health Records\nKyungMann Kim\nUniversity of Wisconsin-Madison\nIn March 2012, the National Institutes of Health issued a funding opportunity announcement, RFA-RM-12-002, “NIH Health Care Systems Research Collaboratory - Pragmatic Clinical Trials Demonstration Projects (UH2/UH3),” under a Common Fund (Roadmap) initiative and then in September 2012 announced its funding decisions via NIH News with the caption “NIH funds will strengthen national capacity for cost-effective, large-scale clinical studies.” Independently of this initiative, the Institute of Medicine of the National Academies hosted a workshop entitled “Large Simple Trials and Knowledge Generation in a Learning Health System” during November 26-27, 2012. Both these initiatives were made possible thanks to the investment in EHRs by the health care systems and to the commitment made by the US Federal Government through the Health Information Technology for Economic and Clinical Health Act of 2009. Besides improving patient care, EHRs hold much promise of making the conduct of large-scale, pragmatic clinical trials very cost-efficient and even clinical effectiveness research possible in the health care systems. However, it poses regulatory, human subject and information technology challenges to the clinical trials community. In this presentation, I will discuss promise and challenges of conducting pragmatic, large, simple trials and clinical effectiveness research that rely on access to EHRs for obtaining patient follow-up and outcome data, rather than on traditional protocol-specified patient visits and procedures.\nA41\nPatient and Provider Acceptance of a Pragmatic Point-of-Care Pilot Study\n(1) Ryan Ferguson (2) Patricia Woods (3) Katherine Riley (4) John Hermos (5) Thomas Sabin (6) Mary Brophy (7) Robert Lew (8) Leonard D’Avolio (9) Louis Fiore (10) Philip Lavori\n(1) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System, Department of Epidemiology Boston University School of Public Health (2) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System (3) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System(4) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System, Department of Medicine Boston University School of Medicine (5) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System (6) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System, Department of Medicine Boston University School of Medicine (7) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System, Department of Biostatistics Boston University School of Public Health (8) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System (9) VA Cooperative Studies Program Coordinating Center VA Boston Healthcare System, Department of Medicine Boston University School of Medicine (10) Department of Health Research and Policy Stanford University\nThe Point of Care (POC) initiative within the Department of Veterans Affairs (VA) Cooperative Studies Program is designed to conduct randomized comparative effectiveness clinical trials during the delivery of routine clinical care. The objective of the program is to create a learning healthcare system that uses pragmatic trials and the national VA electronic medical record (EMR) infrastructure to answer questions that are vitally important to the health and care of the nation’s veterans. A pilot study comparing inpatient insulin regimens was conducted to assess the feasibility of the initiative at two VA Medical Centers. Medical house staff were offered the option to randomize the insulin regimen at the time of placing insulin orders in the EMR insulin order menu modified for the study. Clinicians declined the randomization option in only 19% of patients eligible for the study. Of the 119 eligible patients approached, 102 (85.7%) agreed to randomization, 8 (6.7%) declined participation, 9 (7.6%) were not enrolled due to use of prohibited medications or because consent was not possible within 24 hours of referral to the study. To date, there have been no safety events and no losses to follow-up. The refusal rate of 6.7% of patients is markedly lower (3 times lower) than the rate experienced in the traditional clinical trials conducted within this Cooperative Studies Program Coordinating Center. Early results of this pilot study demonstrate the feasibility and acceptance by both the patients and the providers in integrating the pragmatic design of the POC trial into clinical care with no added study related procedures for clinicians and patients. The result has created an environment where the research infrastructure and the clinical apparatus of the VA are operating synchronously, thereby reducing barriers to participation in studies that are typically seen in traditional trials.\nA42\nContinuous Safety Monitoring for Randomized Controlled Clinical Trials with Blinded Treatment Information\nGreg Ball\nAstellas Pharma Global Development\nData monitoring committees (DMCs) evaluate accumulating unblinded data and make recommendations about the continuing safe conduct of a trial. However, it is the trial leadership who make the ethical decisions about stopping a trial and they could benefit from objective statistical rules that help them judge the strength of evidence contained in the blinded data. We design safety signals for randomized controlled clinical trials with blinded treatment information, which could be used by anyone, including trial leadership. A Bayesian framework, with emphasis on the likelihood function, is used to allow for continuous monitoring, using all of the available information, without adjusting for multiple comparisons. Close collaboration between Statistics, Medical Science and Pharmacovigilance will be needed in order to design safety signals with good operating characteristics. It is imperative that emerging trends in the data are frequently and carefully evaluated to protect the safety and welfare of patients. While investigators must conscientiously maintain their blind, DMCs must have unblinded access to all of the available information, so that together they can fully protect patients from unsafe treatments, without compromising trial integrity or otherwise interfering with the strength of evidence. Safety signals will provide a full measure of the blinded data that the trial leadership can use, in combination with open information from the DMC, to evaluate the strength of evidence in the available data in order to make fully informed decisions that protect patients from unnecessary harm while allowing the trials to lead to conclusive results. Trial leadership need these safety signals to help them effectively monitor the ongoing safe conduct of clinical trials with blinded data.\nA43\nPrecis-2: A Tool to Improve the Applicability of Randomised Controlled Trials\nShaun Treweek, Kirsty Loudon, Frank Sullivan, Merrick Zwarenstein\nUniversity of Aberdeen; University of Dundee; University of Dundee; Western University\nBackground: RCTs are the best design to evaluate the effect of different interventions but randomisation does not itself promote the applicability of the results to situations other than the one in which the trial was done. PRECIS is a tool that helps trialists consider the effects of their design decisions on applicability.\nAim: To produce an improved and validated version of PRECIS.\nMethods: The study has three phases. Phase 1 involves brainstorming and a two-round Delphi survey of authors who cited PRECIS. The Delphi results will then be discussed and alternative versions of PRECIS-2 developed and user-tested by experienced trialists. Phase 3 will evaluate the validity and reliability of the most promising PRECIS-2 candidate using a sample of 15-20 trials rated by 15 other trialists.\nResults: Brainstorming sessions identified the PRECIS presentation (a wheel), lack of a scoring system and domain weighting as issues for exploration in the Delphi. Ninety individuals were invited to complete round 1 of the Delphi; completed responses were received from 34. Many respondents wanted a scoring system, were unhappy about presentation format and suggested several additional domains for PRECIS-2. Based on these responses, round 2 of the Delphi tested specific scoring alternatives, tabular vs a wheel presentation and new domains. Early responses to round 2 support using both wheel and tabulated forms of PRECIS, several new domains (eg. recruitment setting), 1-7 scoring and show continued uncertainty over weighting. The next stage will use the Delphi suggestions to create alternative versions of PRECIS-2 for user-testing in spring 2013.\nConclusions: We have concrete suggestions for improving PRECIS and a growing list of enthusiastic individuals interested in contributing to this work. By the end of 2013 we expect to have a validated PRECIS-2.\nA44\nDesigning a Series of Hybrid Trials with Continuous Outcomes\n(1) Siew Wan Hee (2) Nigel Stallard\n(1) Warwick Medical School, University of Warwick (2) Warwick Medical School, University of Warwick\nBackground: Clinical trials are commonly designed individually. The number of patients available and other resources required to conduct the trial are essentially finite. A decision concerning the design of one trial will therefore inevitably affect the design and allocation of resources of subsequent trials.\nPurpose: To design a trial in the context of other trials so that the clinical development plan is more cohesive.\nMethod: The development plan is considered to consist of a series of independent trials. The responses observed from each trial are analysed using the classical frequentist analysis. Thus, in the design stage the type I error rate is fixed but the unknown parameter of the treatment efficacy is assumed to be random and follows a prior distribution. A sample size that optimizes the Bayesian expected power (assurance) is identified as the optimal sample size. The design is then extended to include the start-up cost spent on planning, designing and so on.\nResults: If the costs of conducting a trial are ignored, the optimal strategy is to make trials as small as possible. This is unsurprising as it maximises the number of trials that can be conducted and consequently the expected total number of trials that reject the null hypothesis. Taking costs into account, there is a range of optimal solutions for different combinations of cost, parameter of likelihood function and a priori knowledge of the unknown parameter. As expected, if the cost is very low, the optimal sample size is small but if the cost is on the other extreme, that is, very high, the optimal strategy may be not to conduct any trial at all.\nA45\nIncorporating Emerging Therapies into Ongoing Randomised Clinical Trials\n(1) Dena Cohen (2) Walter Gregory (3) Sue Todd (4) Julia Brown\n(1) Clinical Trials Research Unit, University of Leeds (2) Clinical Trials Research Unit, University of Leeds (3) Department of Mathematics and Statistics, University of Reading (4) Clinical Trials Research Unit, University of Leeds\nThe incorporation of an emerging therapy as a new randomisation arm to be included in a clinical trial that is already open to recruitment is a challenging question, both practically and statistically, yet is increasingly a reality in studies. It may take many years to run a clinical trial from concept to reporting within a rapidly changing drug development environment. In order for trials to be most useful to inform policy and practice it is advantageous for them to be able to adapt to the changing environment. Currently there is only scarce literature describing statistical methodology or practical considerations when adapting a trial by adding a new treatment midway through recruitment. Yet, it is desirable to researchers, regulators and patients to allow this adaptation within a clinical trial to reduce overall time and costs for determining optimal therapies compared to running separate trials, facilitate selection of the best of the emerging treatments and keep the outcomes current. It is essential to ensure that the research integrity is not compromised when adapting a trial in this way, because otherwise the outcomes may be uncertain, which would be unethical and an unacceptable waste of resources. This presentation will summarise the findings of a systematic literature review investigating what methodology has already been considered for this scenario, and identifying any trials where this amendment has been implemented. The statistical or design issues that were faced and the validity of the trial outcomes following the amendment will be reported. Considerations for future research as identified from the literature and initial research stages will be discussed. These will include conservation of error, appropriate use of control data from each stage, optimal allocation ratios for randomisation and plausible ranges of key parameters such as time to amendment.\nA46\nRandomized Phase II Trials: Are They Entering Research Practice?\nCatherine Klersy, Luigia Scudeller, Annalisa De Silvestri, Cristina Fiocchi*, Carmine Tinelli\nService of Biometry and Statistics & *Ethical Board Scientific Secretariat, . Research Department. IRCCS Fondazione Policlinico San Matteo\nPhase II trials should be viewed as proof-of-concept trials with the aim of ruling-out/ruling-in novel treatments (drug or medical devices ), for further consideration of their efficacy in phase III studies. Historically, phase II trials have been based on the assessment of treatment efficacy in a one-arm design, as compared to historical controls. More recently, randomized phase II trials have been advocated to reduce the bias of using non concomitant controls. They have been considered for the analysis either as embedded one-sample phase II designs within each treatment arm, or as comparative designs with less stringent criteria for type I error than in phase III studies. Several reviews of the pertinent literature have shown an increase in randomized phase II trials published, particularly in the oncologic literature. It is not clear whether this increase in publication of randomized phase II studies is paralleled by an increase in the number of protocols designed. The aim of our study is to assess the prevalence and characterization of randomized phase II protocols submitted to a large university hospital Ethical Board (EB) over the last 5 years. From June 2008 to November 2012, 1130 protocols were submitted to the EB of our Institution. Six-hundred and fifteen were clinical trials and 205 were classified as phase II studies. The prevalence of one-arm designs decreased over time from 60% in 2008 to 45% in 2012. The association of the choice of a randomized design with the type of study sponsor (profit or no-profit), the clinical setting (oncology, infectivology, or other), the type of treatment (drug/device), the number of centres involved (multicenter/one center) and the type of outcome and follow-up duration will be explored. Among randomized trials, the type of analysis (independent one-arm or comparative design) will be assessed. How to increase methodological knowledge will be discussed.\nA47\nCovariate Effect on Constancy Assumption in Active Controlled Clinical Trials\n(1) Siyan Xu (2) Ralph D’Agostino (3) Kerry Barker (4) Sandeep Menon\n(1) Boston University (2) Boston University (3) Pfizer, Inc. (4) Pfizer, Inc. and Boston University\nBiopharmaceutical companies are heading towards demonstrating a proposed therapeutic protein product is biosimilar to a licensed reference product after the Patient Protection and Affordable Care Act signed into law in 2010. Therefore, instead of answering the question of whether two treatments are different, we are more interested in question like whether a new treatment is equivalent or non-inferior to an existed treatment. In other words, the focus is no longer statistical significance difference, but clinical meaningful comparability. Equivalence or Non-Inferiority (NI) tests are used in late phase of biosimilar studies at efficacy endpoints to show we are equivalent or not inferior to the reference product. If we have a better NI test, it should be easy to apply it to the other direction and get a better equivalence test. In addition to establishment of equivalence/ non-inferiority for efficacy endpoint, if the new treatment also shows other benefits, such as cheaper in price, easy to administration, less toxicity and so on, there will be an improvement in clinical care. To apply NI test, standard statistical methods (Fixed margin method and Synthesis method) requires constancy assumption. However, this assumption may be violated due to covariate effect. My work will focus on comparing four different NI methods: Fixed margin method, Synthesis method, Covariate adjustment method, and Adaptive 2-stage method. The first two are standard methods, but the last two may be useful when constancy assumption is violated due to covariate. We will study the impact of 1) strength of covariate 2) degree of interaction between covariate and treatment arm 3) change in distribution of covariate between historical and current trials on type I error rate and power of each method, and using three different metrics (difference, log relative risk and log odds ratio).\nA48\nThe Consequences of Proportional-Hazards Based Model Selection\n(1) Harlan Campbell, (2) Charmaine Dean\n(1) EMMES Canada, (2) The University of Western Ontario\nFor testing the efficacy of a treatment in a clinical trial, the Cox proportional hazards model is the well- accepted, conventional tool. When using this model, one must confirm that the required proportional hazards (PH) assumption holds true. If the PH assumption fails to hold, it may occur that upon examining a Kaplan-Meier (KM) plot, the survival curves appear to cross, suggesting long-term survival is higher among one group of patients. In this situation - given that the PH assumption does not hold, and given that the KM survival curves are observed to cross - there are options available, proposed as alternatives to the Cox PH model. An important question which arises is whether the potential bias introduced by such a sequential model fitting procedure merits concern and, if so, what are effective mechanisms for correction. We investigate by means of simulation study and draw attention to the considerable drawbacks, with regards to power, of a simple resampling technique, the permutation adjustment, a natural recourse for addressing such challenges. We also consider the recently proposed two-stage testing strategy of Qiu & Sheng (2008) for ameliorating these effects.\nA49\nAssessing the ‘Success’ of the Blind in Sham-Controlled Randomized Clinical Trials and the Impact on Treatment Effect\nValerie Durkalski\nMedical University of South Carolina\nA critical component to randomized-controlled clinical trials is the inclusion of adequate treatment blinding to help ensure unbiased estimates of treatment effects. Although a common design feature, several trials, particularly device and surgical trials, are challenged to develop adequate blinding procedures. When feasible, these trials attempt to preserve the blind by developing a ‘sham’ control that mimics the experimental treatment. In these cases, it is important to capture the quality of blinding throughout the trial period and to assess the impact on the treatment estimates. Options for assessment include questionnaires of ‘best’ treatment guess and confidence in the guess which should be collected at multiple timepoints throughout the trial period. We examine the use of these questionnaires in two recently conducted sham-control trials (one device and one surgical trial) and the relationship between blinding quality, the ‘hunch’ theory and treatment effect.\nA50\nResponder Analysis Outcomes in the Analysis of Acute Stroke Clinical Trials\n(1) Kyra M. Garofolo (2) Sharon D. Yeatts (3) Viswanathan Ramakrishnan (4) Edward C. Jauch (5) Karen C. Johnston (6) Valerie L. Durkalski\n(1,2,3,6) Division of Biostatistics and Epidemiology, Department of Medicine, Medical University of South Carolina (4) Division of Emergency Medicine, Department of Medicine, Medical University of South Carolina (5) Department of Neurology, University of Virginia School of Medicine\nBackground: Traditionally in acute stroke trials, the primary outcome is a dichotomized modified Rankin Scale (mRS). New statistical methods, such as responder analysis, are evolving to address the concern that baseline severity impacts the likelihood of a successful outcome. Responder analysis tailors the definition of success according to baseline severity, producing a more clinically relevant insight into the actual effect of investigational treatments. Whether statistical analyses should adjust for baseline severity when responder analysis is used is unclear, as the outcome already takes into account baseline severity.\nObjective: To investigate the effect of covariate adjustment on statistical operating characteristics in the responder analysis framework.\nMethods: Using a current stroke clinical trial and its pilot studies to guide simulation parameters, 1000 clinical trials were simulated at varying sample sizes under several treatment effects to assess power and type I error. Covariate-adjusted and unadjusted logistic regression were used to estimate the treatment effect.\nResults: Under various settings, the operating characteristics of the unadjusted and adjusted analyses do not substantially differ. Power and type I error are preserved for both the unadjusted and adjusted analyses. Conclusions: Our results suggest that, in the responder analysis framework, the decision to adjust for baseline severity should be guided by the needs of the study, as type I error rates and power do not appear to vary largely between the methods. These findings extend to any trial that uses an ordinal scale as a primary outcome measure and has a baseline prognostic variable.\nTrial Registration: This research is part of the Stroke Hyperglycemia Insulin Network Effort (SHINE) Trial, Identification Number NCT01369069, conducted in conjunction with the Neurological Emergencies Treatment Trial (NETT) Network (U01 NS059041, U01 NS056975).\nA51\nThe Benefits and Challenges of Performing a Central Review of Response in a Chronic Lymphocytic Leukaemia Trial\n(1) Lucy McParland (2) Peter Hillmen (3) Andy Rawstron (4) Alexandra Smith (5) Anna Chalmers (6) Corinne Collett (7) David Phillips (8) Dena Cohen\n(1) Clinical Trials Research Unit (CTRU), University of Leeds (2) St James’s University Hospital (3) St James’s University Hospital (4) Clinical Trials Research Unit (CTRU), University of Leeds (5) Clinical Trials Research Unit (CTRU), University of Leeds (6) Clinical Trials Research Unit (CTRU), University of Leeds (7) Clinical Trials Research Unit (CTRU), University of Leeds (8) Clinical Trials Research Unit (CTRU), University of Leeds\nCentral reviews of response, where responses to therapy are assessed by an independent panel of experts, are beneficial in clinical trials to: increase the accuracy and reliability of the endpoint data; ensure consistency in reporting, particularly in multicenter trials; and reduce potential bias in reporting. Such reviews can be extremely time consuming and expensive. The logistics and appropriateness of conducting a central review requires careful consideration to ensure robust processes and funds are in place prior to any formal analyses. A central assessment on the primary endpoint of our trial in Chronic Lymphocytic Leukaemia (CLL) was recently carried out by a team of independent CLL clinicians prior to an interim analysis. Data required to assess the primary endpoint, rate of complete response (CR) according to iwCLL guidelines, was collated, blinded and sent to two independent assessors, with a third independent assessment requested for discordant reviews. 98 cases were centrally reviewed. In 76(77.6%) cases the initial two assessors gave concordant results, with a third review required in 22(22.4%) cases. The concordance of the local assessments of response compared with the independent central assessments was evaluated for 94 available cases. 48(51.1%) assessments disagreed, with 28(29.8%) differing to the extent of changing the primary endpoint result. In 20(71.4%) of these cases, local reviewers disagreed with the final central assessment and classified participants as having achieved a CR. The central review process proved challenging in this trial, with difficulties including: time taken to attain the required data items and collate the central reviews; impact on the research sites, the central laboratory who process the biological samples and the clinical trial team; and handling of missing data. However, performing a central review of response proved to be vital to ensure the highest levels of accuracy in reporting the primary endpoint.\nA52\nCharacteristics of Participants Agreeing to Long-Term (Up To Ten Years) Follow-Up in a Large Randomized Clinical Trial\n(1) Alice J Sheffet, (2) Susan E Hughes, (3) MeeLee Tom, (4) Ariane Mackey, (5) William Brooks, (6) Wayne M Clark, (7) Michael D Hill, (8) Vito Mantese, (9) Jenifer H Voeks, (10) Mary E Longbottom, (11) Thomas G Brott for the CREST Investigators\n(1) UMDNJ-New Jersey Medical School, (2) UMDNJ-New Jersey Medical School (3) UMDNJ-New Jersey Medical School, (4) CHA Hop de L’Enfant Jesus, (5) Central Baptist Hospital, (6) Oregon Health Science Univ, (7) University of Calgary, (8) Mercy Hospital St. Louis, (9) Medical University of South Carolina, (10) Mayo Clinic Jacksonville, (11) Mayo Clinic Jacksonville\nBackground: Retaining participants in clinical trials is a recognized challenge. Long-term data are needed to assess durability of carotid artery stenting (CAS) and carotid endarterectomy because most patients live a decade or longer post-procedure. Identifying patient and site characteristics of those consenting to continue in long-term follow-up is important for study design and operation.\nMethods: The Carotid Revascularization Endarterectomy vs. Stenting Trial (CREST) randomized 2502 patients at 117 sites. Characteristics of surviving active participants and their enrolling sites were investigated to compare those who consented to participate long-term (5-10 years) with those who refused continuation past 4 years. Chi-square and t-tests were among those used to test for differences between those who did and did not consent to extended follow-up.\nResults: Of 1854 surviving active participants, 1650 (89%) consented to 5-10 years follow-up, and 204 (11%) refused. Sites randomizing >30 patients had a significantly higher proportion consented to long-term follow-up than sites randomizing.\nA53\nLogistical Considerations for Independent Review of Patient Imaging for Quality Assurance or Real Time Reporting in Oncology Clinical Trials\n(1) Catherine Lowe (2) Julie Croft (3) Sarah R Brown\n(1) Clinical Trials Research Unit, University of Leeds (2) Clinical Trials Research Unit, University of Leeds (3) Clinical Trials Research Unit, University of Leeds\nThe Response Evaluation Criteria in Solid Tumours (RECIST) guideline recommends that tumour responses are reviewed by an independent expert in clinical trials utilising response rate as a primary endpoint, therefore oncology trials often employ central collection of patient imaging for quality assurance or real time reporting. Here the authors reflect on their experiences of imaging collection in two large UK multicentre randomised phase III Clinical Trials of Investigational Medicinal Product (CTIMPs). PICCOLO compared Irinotecan alone (Ir) with Irinotecan plus Ciclosporin (IrCs) or Irinotecan plus panitumumab (IrPan) in colorectal cancer. Treatment response was assessed locally and independent confirmation of RECIST response was planned on all baseline and 12 week CT scans for quality assurance purposes only. STAR compares conventional continuation strategy (CCS) with an experimental drug-free interval strategy (DFIS) using sunitinib in metastatic renal cell carcinoma. Baseline and all 12 weekly CT scans are assessed by an independent central radiologist and management of trial treatment is based this rather than the local assessment. Both trials required scans to be copied to disc and forwarded to the data centre. Although logistical issues with this process were experienced by both trials, these problems had greater implications to STAR as patient treatment decisions are reliant on timely assessment of response. Logistical problems experienced included; scans not sent within required timeframe, data centre never receiving discs, anonymisation issues (identifiable information either not obscured or conversely too much detail removed from the disc including scan date, scales and callipers), incompatibility between local and central software, incorrect delivery address used, discs lost in the post and hard copies rather than discs sent to data centre. Further detail on the issues experienced along with possible solutions will be presented, in addition to considerations for future trial design dependant on type of independent review being employed.\nA54\nMeasuring the Impact of Methodological Research at the UK Medical Research Council Clinical Trials Unit\nValerie Brueton, Claire Vale, Rachel Jinks, Babak Oskooei, Jayne Tierney\nMedical Research Council Clinical Trials Unit\nBackground: Evidence of research impact is increasingly requested by research funders. This commonly relies on citation analysis, however other indicators may be more informative. We aimed to measure indicators of impact for our methodological research in the design, conduct and analysis of clinical studies.\nMethods: We used methodological research projects conducted or completed at the UK Medical Research Council Clinical Trials Unit (CTU) from 2009-2012. Publication citation rates and related metrics were obtained using ISI Web of Science. In addition, we recorded email queries generated by these projects retrospectively. Semi-structured interviews were conducted with CTU methodologists to obtain further data on the uptake of methods. The content of the interviews and emails was analysed, along with a descriptive analysis of citations.\nResults: Preliminary results are based on 38 publications from 2009-2010 and 21 research projects. Seven publications had >10 citations/year in both general and specialised medical articles. Approximately 70% of citations came from original research articles. We were unable to obtain email queries for all projects, but based on data available for 6 projects there were 194 queries from 170 individuals across 23 countries, showing widespread use of these methodological developments and associated software. The 13 interviews revealed numerous impacts including application of methods in new research, further development and dissemination of statistical software, establishment of new national and international collaborations and the uptake of research findings in trial governance guidelines. Updated results will be presented.\nConclusions: We have quantified the impacts of our methodological research via citation rates and research queries. Through the interviews we obtained a range of less obvious but important impacts. These results will be used to inform prospective monitoring of impact and strategies to increase awareness and uptake of our methodological research, providing a framework that others might also use.\nA55\nTransitioning Data Management Systems During an Ongoing Study: Challenges, Limitations and Successes\nHolly Battenhouse\nMedical University of South Carolina\nThe transition of data management systems from one data management center (DMC) to a new one is a complex process, particularly when studies within the database are active. Accepting data from a previous DMC requires detailed communication, data cleaning, a full understanding of the process of data collection and entry, and documentation of all variables and transfer steps. Challenges to the data transfer process include potential changes to the case report forms and/or system functions, availability of staff with knowledge of the previous database, and system downtime. In order to facilitate a seamless transfer, all aspects of the process must be understood and planned in advance. This presentation is based on the experience gained from the successful completion of the transfer between two DMCs of over 10 years worth of registry data from the NIDDK-funded Acute Liver Failure Study Group. The transfer was from an access database to a new web-based system with revised case report forms and continuing data collection. The presentation will focus on the process as well as lessons learned and recommendations for improvements to the process.\nA56\nFrom the Lab to the Network: Management of Experimental, Clinical and Specimen Data Using the Open Source Labkey Server Platform\n(1) Elizabeth K. Nelson, (2) Peter Hussey, (3) Matthew Bellew, (4) Mark Igra, (5) Josh Eckels, (6) Britt Piehler, (7) Adam Rauch\n(1-7) LabKey Software\nExisting software systems strain to meet the evolving needs of large-scale clinical research networks. To gain insight into difficult diseases, network researchers must make sense of diverse types of data while working as part of distributed teams and leveraging a range of expertise, methods, and data sources. Few software systems provide robust support for managing, analyzing, and sharing not just clinical and specimen information, but also data from complex, high-throughput assays. Systems that provide significant assay support are often organization-specific, assay-specific, and/or separate from systems that provide access to clinical and specimen information. Ad hoc solutions for integrating assay, clinical, and specimen data are often spreadsheet- and email-based, so they are not as robust, scalable, efficient, reliable, or secure as networks need. LabKey Server stands out as an open source system that bridges the data management needs of network labs performing cutting-edge research assays, central project managers tracking study progress, and network scientists analyzing data from many sources. The system supports managing, integrating, analyzing, visualizing, and securely sharing the diverse data types produced by clinical research networks, from clinical records to specimens to cutting-edge assays. Notable differentiators of LabKey Server include easy extensibility, tools for managing newly invented assay data types, built-in support for complex experimental data types (including flow cytometry, proteomics, various plate-based assays, and DNA sequencing), specimen request tracking tools, basic support for ancillary studies, and a built-in R interface. LabKey Server readily integrates with and extends existing systems that are based on databases and/or spreadsheets. Since its launch in 2005, this open source system has been adopted and customized by organizations across the globe, including the Immune Tolerance Network and consortia within the Global HIV Enterprise. Source code, compiled binaries, documentation and tutorials are professionally maintained and freely available under the Apache 2.0 license at http://www.labkey.org .\nA57\nAn Analysis of Data Changes in Response to Real-Time Protocol Violation (PV) Alerts in an Electronic Data Capture (EDC) System\n(1) Cassidy Conner, (2) Tomoko Goddard, (3) Aaron Perlmutter, (4) Catherine Dillon, (5) Karen Briggs, (6) Keith Pauls, (7) Wenle Zhao, (8) Valerie Durkalski, (9) Yuko Palesch\nMedical University of South Carolina\nReal-time protocol violation (PV) alerts programmed in an Electronic Data Capture (EDC) system are triggered at the time of data entry when data deviates from what is allowed by the protocol. These alerts can serve many purposes, including blocking ineligible subjects from randomization and providing a mechanism for sites to explain the reasons for violations. However, real-time PV alerts may lead to inappropriate data changes in order to bypass a violation. To determine the extent of inappropriate data changes in response to PV alerts, the audit trail of an EDC system for an NIH/NINDS funded phase III multicenter trial was reviewed. We found that the majority of data changes in response to PV alerts were made in order to correct data entry errors due to the complexity of the programmed data checks. When real-time PV alerts are programmed based on the responses to several data points on a form (e.g. the logical progression of a time/date sequence) or based on responses across multiple forms, this complexity resulted in higher rates of data entry errors. Based on the results of this analysis, several key improvements were made to the EDC system and case report forms to help reduce data entry errors.\nA58\nHow Much is Enough: A Risk Based Approach to Analysis Verification\nDeborah Toyota\nBoston Biomedical Associates\nVerification is typically the most time consuming part of producing an analysis. This is especially true for interim analyses where “dirty” data makes verification all the more difficult and all the more necessary. There are different approaches to verification, including manually counting and calculating as well as independent programming with manual or automated compare. All approaches have their share of advantages and their share of disadvantages. And, all approaches typically take more time to verify than they did to produce. One sure way to reduce verification time is to reduce the amount of items checked during verification. The question is, though, how to determine what to check and what not to check. It is also important to consider how to document that decision. A formal risk analysis is the key to scaling analysis verification in a way that provides documented evidence of the decision. FDA has published several guidance documents discussing risk based approaches to different processes used in clinical trials. Using these documents as reference, BBA developed an analysis verification risk assessment. This assessment has been utilized on several projects over the past eighteen months. This presentation will share the risk-based strategies for analysis verification implemented by BBA. The impact on verification processes will be presented and lessons learned will be discussed.\nA59\nPhase II/III Seamless Adaptive Dose Selection Design for Longitudinal Patient Data\n(1) Caitlyn Ellerbe (2) Jordan Elm (3) Viswanathan Ramakrishnan (4) Bruce Turnbull (5) Stacia DeSantis (6) Edward Jauch (7) Valerie Durkalski\n(1) Division of Biostatistics and Epidemiology, Medical University of South Carolina (2) Division of Biostatistics and Epidemiology, Medical University of South Carolina (3) Division of Biostatistics and Epidemiology, Medical University of South Carolina (4) Departments of Statistical Science and of Operations Research, Cornell University (5)Division of Biostatistics and Epidemiology, Medical University of South Carolina (6) Division of Emergency Medicine, Medical University of South Carolina (7)Division of Biostatistics and Epidemiology, Medical University of South Carolina\nIn response to the increase of randomized controlled trials (RCT) that fail during late stages of clinical development, adaptive design trials offer investigators the ability to modify trial parameters to promote safety and trial efficiency. Despite interest in these designs, implementation is rare in the confirmatory setting given uncertainty over whether they maintain RCT standards. Consider a trial comparing three doses of interest and a placebo control, where the primary endpoint is repeatedly measured at several follow-up visits and a clinician preferred dose exists. In a non-adaptive setting, a Phase II study selects the optimal dose based on a short-term endpoint before testing this against a control during a Phase III RCT using a long-term definitive endpoint. Downsides of this approach include the lack of long-term primary endpoint and safety information available for dose selection (Phase II) as well as the downtime associated with pre-study activities between phases. Alternatively, several seamless adaptive dose finding designs exist. When longitudinal data exist, these designs are limited in their ability to use all available information because of correlation between measures. At the interim analysis (dose selection), investigators can either use a short-term surrogate endpoint for all available individuals or a long-term definitive outcome for a subset of the recruited population depending on the selected design. The final analysis (efficacy) must incorporate the remaining data such that the type I error (alpha) is not inflated due to analyzing a single point multiple times. We propose a design using all available information (longitudinal data) at each analysis thus providing an intuitive approach which increases efficiency (decreased N). The design demonstrates the following features: a) ability to control alpha; b) theoretically sound combination function; c) closed testing procedure for correlated hypotheses; d) physician guided dose selection.\nA60\nInternal Pilot Design for Cluster Randomized Trials with Unequal Cluster Sizes\n(1) Ashutosh Ranjan (2) Christopher Coffey (3) Leslie McClure\n(1) Department of Biostatistics, University of Alabama at Birmingham (2) Department of Biostatistics, University of Iowa (3) Department of Biostatistics, University of Alabama at Birmingham\nBackground: Cluster randomized trials are popular in health care research and community based studies. Many of these trials, while planned for equal cluster sizes, in reality have unequal cluster sizes which may lead to an underpowered study. Internal pilot designs can be used to adjust the sample size midway through a study to account for variation in cluster size to help ensure sufficient power to detect planned treatment differences.\nMethods: We compare two methods for re-estimating sample size midway through a cluster randomized trial, for varying levels of imbalance in cluster size: 1) sample size is re-estimated assuming equal cluster sizes followed by a cluster level analysis and 2) sample size is re-estimated considering variability in clusters followed by a cluster level analysis weighted by cluster size. The operating characteristics of the two methods are investigated for a hypothetical but typical community based trial.\nResults: As long as the initial planning variance overestimates the true variance, both the methods maintain desired study characteristics. Method 2 maintains power close to the target level with trivial bias in type I error across all the scenarios of imbalances considered when the initial planning variance underestimates the true variance. In comparison, there is substantial loss in power when Method 1 is used, when the true variance is greater than the initial variance estimate.\nConclusion: Sample size re-estimation using a formula that takes into account the variability in cluster sizes for a trial planned assuming equal cluster sizes, followed by a cluster-level analysis weighting by cluster sizes, is recommended for trials with unequal cluster sizes. This method requires a larger number of clusters be recruited, but researchers employing this procedure will have a better chance of realizing the aims of their study with adequate power.\nA61\nLikelihood-Based Methods for the Prediction of Endpoint Occurrences in Clinical Trials\n(1) Megan Smith (2) Ying Qing Chen\n(1) University of Washington (2) Fred Hutchinson Cancer Research Center\nThe successful conduct of a clinical trial requires careful coordination of the sponsor, clinicians, statisticians, Data Safety Monitoring Board, and many others. Tools to aid in the logistical management of trial operations are essential and help facilitate best adherence to preset interim analysis and monitoring plans. In a clinical trial with censored time-to-event data, the statistical information is governed by the number of observed events. Once the trial is underway, it is useful to have methods to predict the timing of future landmark numbers of events. Unforeseen trends in accrual, health outcomes, or patient dropout may influence the accuracy of design-phase event time predictions. Thus a sensible strategy is to use accumulating data from the trial itself to refine preliminary projections. Here we present flexible, computationally inexpensive likelihood-based approaches for the real-time prediction of endpoint occurrences. We describe two methods to obtain point estimates and prediction intervals for the timing of landmark events. The first is a marginal method that produces an overall estimate of the observed and anticipated study progress. The second method predicts the timing of future events conditional on the exact number and timing of events already observed in the trial. Our data-driven approaches achieve favorable accuracy and precision, are simple to implement, and require neither prior assumptions about model parameter distributions nor the simulation of additional data. Simulation studies demonstrate the accuracy and uncertainty measures for these methods under various distributional assumptions for accrual, event, and dropout times. We also compare our model predictions to actual event times observed in data from the HIV Prevention Trials Network 052 Study to assess the effectiveness of immediate versus delayed antiretroviral therapy strategies on sexual transmission of HIV-1.\nA62\nCharacterization of Two-Stage Likelihood Continual Reassessment Method\n(1) Xiaoyu Jia (2) Shing M Lee (3) Ying Kuen Cheung\n(1) Department of Epidemiology and Biostatistics, Memorial Sloan-Kettering Cancer Center; Department of Biostatistics, Columbia University. (2) Department of Biostatistics, Columbia University. (3) Department of Biostatistics, Columbia University\nThe continual reassessment method (CRM) is an increasingly popular model-based method for dose finding clinical trials among clinicians. A common practice is to use the CRM in a two-stage design, whereby the model-based CRM is activated only after an initial sequence of patients are tested. While there are practical appeals of the two-stage CRM approach, the theoretical framework is lacking in the literature. As a result, it is often unclear how the CRM design components (such as the initial dose sequence and the dose toxicity model) can be properly chosen in practice. This paper studies a theoretical framework that characterizes the design components of a two-stage CRM, and proposes a calibration process. A real trial example is used to demonstrate that the proposed process can be implemented in a timely and reproducible manner, and yet offers competitive operating characteristics when compared to a labor-intensive ad hoc calibration process. We also illustrate using the proposed framework that the performance of the CRM is insensitive to the choice of the dose-toxicity model.\nA63\nFutility Design Incorporating Concurrent Controls: A Variation on the Phase II Trial Design\n(1) Sharon D Yeatts (2) Magdy Selim (3) Yuko Y Palesch\n(1,2) Division of Biostatistics and Epidemiology, Department of Medicine, Medical University of South Carolina (1,2) Data Coordination Unit, Medical University of South Carolina (3) Department of Neurology, Stroke Division, Beth Israel Deaconess Medical Center, Harvard Medical School\nBackground: There is a rising need for more efficient designs to discard ineffective therapies. The objective of the single-arm futility design is to reject treatments without promise. Use of a pre-specified threshold, rather than a concurrent control, as the comparator, reduces the sample size of the traditional concurrently controlled Phase II design. The threshold can be determined based on a clinically relevant treatment effect and historical control data. Potential pitfalls associated with historical control data, including temporal changes and protocol variations across trials, raise concerns over its use. If the historical control data is no longer relevant, results may not accurately reflect the futility of the treatment. Calibration of the threshold via a small control cohort has been suggested. If this cohort is too small, its usefulness is limited; if too large, the trial resembles an underpowered Phase III.\nObjective: To propose an appropriately powered concurrently controlled futility design and describe its application in the phase II trial of deferoxamine in ICH.\nMethods: The futility hypothesis is based on a direct comparison of the arms, in which the treatment effect is compared to a pre-specified futility threshold. Where the primary endpoint is favorable outcome, an absolute treatment effect less than/in favor of the experimental treatment leads to rejection of the treatment as futile. Failure to conclude futility is evidence in favor of a phase III trial.\nConclusions: Despite the increase in sample size over the single arm design, the proposed is not an alternative to Phase III testing. The objective remains to establish futility, rather than to demonstrate efficacy, of the treatment. A concurrent control arm avoids the drawbacks of historical control data and allows for a direct comparison of treatment arms.\nA64\nPersonalized and Adaptive Trials in Phase 2: Comparison of Designs\n(1) Russell Reeve (2) Bradley Ferguson\n(1) Quintiles, Inc. (2) North Carolina State University\nInterest in adaptive trials is increasing, but there is still considerable debate about their value in clinical research. This debate extends from how efficient they are, to how to implement these trials in practice. In this talk, we develop an optimal trial design based on the Hill model, and compare its efficiency compared with standard trials. We present this design in the context of a trial to assess the advantages of a novel rheumatoid arthritis agent in the presence of methotrexate background therapy. In this design, we use ACR20 – a binary endpoint that measures improvement over baseline – as the primary endpoint. In addition to the adaptive design, we also introduce a personalized design for this case. We show that both of these designs yield improvement in the precision to estimate the ED70 versus a fixed design. A discussion of using adaptive designs based on the Hill model versus the more flexible categorical models (e.g., NDLM) will be discussed, and the Hill model design will be shown to be at least as efficient in estimated the ED70 as the NDLM design. And in fact, the personalized design is shown to have superior properties over the adaptive design, which is consistent with efficiency of randomized concentration-controlled trials. Finally, a discussion of some challenges in implementing these highly adaptive designs will be discussed, with solutions based on experience.\nA65\nResponse-Adaptive Dose-Finding for Correlated Bivariate Data with Applications to Complement System Inhibition Studies\nMitchell Thomann\nUniversity of Iowa\nThe complement system is an important part of the innate human immune system. In particular, the C5 complement system plays a role in inflammatory responses throughout the body. Inhibition of this system has been shown to have therapeutic effects in a variety of diseases, including arthritis and sepsis. An important outcome that measures the effectiveness of drugs that work via C5 inhibition is the percent of the complement system that is inhibited. These efficacy outcomes are continuous and bounded. Furthermore, there is little benefit expected when treating patients at higher doses once the maximum level of inhibition has been achieved. Thus, when performing dose-finding trials for drugs that inhibit this system, it is important to consider both drug-related toxicities and complement system inhibition. This paper presents a response-adaptive method for dose-finding that jointly models subjects’ binary toxicity and bounded continuous efficacy outcomes. There are existing dose-finding methods for modeling binary toxicity and normal efficacy outcomes; this method expands their utility to bounded efficacy outcomes. It is based upon the factorization of the joint distribution of the outcomes using the product of the distribution of toxicity outcome and the conditional distribution of the efficacy outcome. Parameter estimates are obtained using Markov Chain Monte Carlo sampling. Groups of subjects are sequentially enrolled to the trial at dose levels based upon updated estimates of efficacy and toxicity. A simulation study is also presented to demonstrate the utility of this method under a variety of conditions. The simulation study’s results are summarized and discussed. The simulations demonstrate that this method performs well when the data are generated from a truncated normal distribution. Similar models could be derived for bounded data from other truncated continuous distribution functions.\nA66\nUnintentional Operation Mistakes in Emergency Treatment Trials and Their Impacts on Trial Results\nWenle Zhao, Yuko Palesch\nMedical University of South Carolina\nUnintentional operation mistakes could occur in clinical trial practices in various ways. Trials treating medical emergencies like strokes and traumatic brain injuries are even more vulnerable. While some operation mistakes may have trivial impacts on the final results of the trial, those mistakes associated with subject eligibility assessments, subject randomization, study drug dispatch, treatment administrate, and primary outcome assessment could affect the trial results seriously. Typically, in clinical trial design, a sample size inflation factor is used to cover the power lost caused by noncompliers. However, the proportion of noncompliers could be under estimated for emergency treatment trials, and different types of operation mistakes could have different impacts. This presentation will quantify the impacts of commonly observed unintentional operation mistakes on the power and type I error under both superiority and on-inferiority trial scenarios.\nA67\nInvestigating Treatment Effect Variability in Randomized Longitudinal Clinical Trials\nJoseph R. Rausch\nCincinnati Children’s Hospital Medical Center University of Cincinnati, College of Medicine, Department of Pediatrics\nThe standard approach to investigating the effects of intervention/prevention regimens on continuous outcomes in randomized clinical trials is via the average (i.e., mean) causal effect. However, there is reason to believe that averages can hide important variation in the treatment effect in many real-world applications and even can potentially mislead researchers and clinicians alike when determining the appropriate treatment for a particular individual. Statistical methods which directly address the possibility of variance in the treatment effect should be employed to better understand treatment effects at the individual level. After reviewing statistical challenges associated with examining treatment effect variability, I discuss one particularly worthwhile approach for investigating treatment effect variability in randomized longitudinal clinical trials using multilevel modeling. Hypothesis testing and estimation, in conjunction with their corresponding assumptions, are discussed to assess scenarios in which these approaches are appropriate in applied research. The practical ramifications of utilizing a statistical approach which directly assesses treatment effect variability are delineated. Finally, an illustrative example is provided to facilitate dissemination of this methodological framework more generally within the context of randomized longitudinal clinical trials.\nA68\nDichotomizing Partial Compliance and Increased Participant Burden in Factorial Designs: The Performance of Four Non-Compliance Methods\n(1) Peter Merrill (2) Leslie McClure\n(1) University of Birmingham at Alabama (2) University of Birmingham at Alabama\nNon-compliance to treatment assignment is an inevitable occurrence in randomized clinical trials (RCTs). Intention to treat (ITT) is generally considered the best method for addressing non-compliance in RCTs. Several alternatives to ITT exist, including: per protocol (PP), as treated (AT), and instrumental variables (IV). These three methods define participant compliance as a binary variable, but partial compliance is a very common occurrence in RCTs. By defining a threshold, above which a participant is called a complier, PP, AT and IV can, but the resulting loss of information may affect their performance. Additionally, trials with factorial designs may experience higher rates of non-compliance due to a heavier burden participants experience by being assigned to multiple experimental treatments. Using simulations, we assessed the performance of ITT, PP, AT, and IV in both the partial compliance setting and in a 2-by-2 factorial study with increased participant burden for those randomized to both active treatments. The bias, mean square error, and type I error rates of the IV method after dichotomizing partial compliance was heavily inflated. PP and AT improved on the bias and power of ITT without inflating type I error beyond acceptable limits. The use of PP and AT cannot readily be recommended because of the possibilities of selection bias inherent in the use of these methods. The performance of all four methods depended heavily on the level non-compliance present, with higher average non-compliance leading to poorer performance. The results suggest the need for a method of estimating treatment effects that can fully utilize partial compliance information.\nA69\nUsing Central Review Data in Analyses of Phase III Trials\nSarah Brown, Helen Marshall, Catherine Lowe, Julie Croft\nClinical Trials Research Unit, University of Leeds\nPICCOLO is a multicentre, phase III, advanced colorectal cancer trial, comparing Irinotecan alone (Ir) with Irinotecan plus Ciclosporin (IrCs) for patients with KRAS mutated or undefined tumours, and with Irinotecan plus panitumumab (IrPan) for patients with KRAS wildtype tumours. The trial was designed as two parallel phase III trials, accounting for patients? KRAS status. For the IrCs comparison, a non-inferiority hypothesis was proposed for the primary endpoint of the proportion of patients alive and progression-free at 12 weeks post-randomisation, based on RECIST criteria. A central, independent review of progression assessments at this time point was planned. During the review process, a number of logistical issues arose. Difficulties were encountered with obtaining both baseline and follow-up scan images from centres; with software issues restricting access to images received; and with ability to read and combine images to enable assessment of progression-free status. These logistical issues resulted in independent review of only a subset of patients being possible. Primary analysis was pre-specified to use local clinical assessment data, with independently reviewed data providing a quality assurance assessment only. Data summaries of the proportion of patients for which there was disagreement in the primary endpoint of progression-free status were performed. Final analysis of the IrCs comparison failed to demonstrate non-inferiority of IrCs compared to Ir on the basis of the locally assessed primary endpoint. Of 241/672 patients for whom a central assessment of progression-free status was performed, discrepancies were apparent for 54 patients (22%). In an extrapolated ad-hoc analysis taking into account these discrepancies, the primary endpoint result was unchanged and non-inferiority was still not demonstrated. We will present our approach to incorporating such review data into analyses, and discuss the purpose of these types of review data.\nA70\nCurrent Practices in Remote Monitoring Visits\nEllen Peskin, Kathleen McWilliams, Maureen Maguire\nUniversity of Pennnsylvania Department of Ophthalmology Clinical Trials Coordinating Center, Center for Preventive Ophthalmology & Biostatistics\nEffective monitoring of study activities in multi-center clinical trials is vital to ensure the protection of human subjects, adherence to study protocols and the collection of accurate data. The gold standard for monitoring has been in-clinic visits, which are effective, and allow the monitor to directly observe implementation of study protocols. Disadvantages of in-person visits are cost and travel time - key issues in this resource-tight environment. The FDA has recently issued draft guidance for risk-based monitoring, including adoption of alternative monitoring methods. Similarly, ICH E6 advises developing monitoring plans that consider several factors in determining the nature of monitoring for a given trial. The development of new technologies allows for monitoring to occur remotely, using the Internet and VOiP services. Currently there are no specific guidelines pertaining to when remote monitoring of clinical trials is appropriate (or not), no guidance on the best methods for conducting them and the resources necessary to implement remote monitoring visits. This leaves Coordinating Centers, sponsors and CROs with wide discretion but less certainty whether a remote monitoring program is compliant with GCP, particularly in trials seeking FDA marketing approval. For this study we will conduct interviews with leaders of 25 Coordinating Centers and CROs conducting both NIH and FDA-regulated trials to ascertain current remote monitoring practices. Interviewees will be queried on the frequency of remote site monitoring, the factors driving the decision to employ them, the equipment required for effective ‘visits’, monitor training, lessons learned and the benefits and drawbacks relative to in-person visits. Interviews will be also conducted with 10 Clinic Coordinators who have experienced remote monitoring visits for their assessment of the relative strength and weaknesses of such visits. The study aims to identify current remote monitoring practices, strategies for successful implementation and guidance for Coordinating Centers/CROs considering adopting them.\nA71\nDesign of an International Cluster-Randomized Trial Comparing Two Data Monitoring Practices\n(1) Katherine Huppler Hullsiek, (2) Nicole Wyman, (3) Jonathan Kagan, (4) Jesper Grarup, (5) Catherine Carey, (6) Fleur Hudson, (7) Elizabeth Finley, (8) Waldo Belloso\n(1) Division of Biostatistics, University of Minnesota (2) Division of Biostatistics, University of Minnesota (3) National Institute of Allergy & Infectious Diseases, National Institutes of Health (4) Copenhagen HIV Programme, University of Copenhagen (5) The Kirby Institute, University of New South Wales (6) Medical Research Council (7) Veterans Affairs Medical Center (8) Hospital Italiano de Buienos Aires (8) Hospital Italiano de Buenos Aires (6)\nMonitoring of clinical research studies protects human subjects and the integrity of trial data. While required by regulatory agencies, specific monitoring activities are at the discretion of the sponsor. Research is needed on the effectiveness and efficiency of risk-based monitoring. As part of the START study the International Network for Strategic Initiatives in Global HIV Trials (INSIGHT) clinical trials network is conducting a cluster randomized Monitoring Substudy. Eligible sites are randomized to one of two strategies: (1) central and local monitoring or (2) central and local and on-site monitoring. The purpose is to evaluate if site performance is improved with on-site monitoring in addition to central and local monitoring. The START study is suitable for evaluation of on-site monitoring because it is a strategic trial on the timing of antiretroviral therapy and not a pivotal or early phase safety study. All sites are centrally (remotely) monitored by the statistical/data center and international coordinating centers for data quality issues and FWA lapses in real-time; local (internal) monitoring is conducted at all sites by the semi-annual completion of forms designed to assess site quality assurance adherence; sites randomized to on-site monitoring also have annual visits conducted by a person independent of the site. A challenge for the Monitoring Substudy design was crafting a primary outcome that could be captured both with and without on-site monitoring. The primary composite outcome includes components related to major eligibility and informed consent violations, use of drugs not permitted by protocol, START primary endpoints and serious events not reported within 6 months of occurrence, and data alteration or fraud. Currently the ongoing Monitoring Substudy has randomized 215 sites (enrolling 3013 people) in 33 different countries. We describe in detail the central, local and on-site monitoring activities along with the design and implementation challenges that have been encountered.\nA72\nCorrelation of a Form-Based Versus Item-Based Metric of Data Accuracy in Case Report form Completion in a Clinical Trial Network\nErin Bengelink, Tess Bonham, Cassidy Connor, Andrace Deyampert, Catherine Riley Dillon, Valerie Durkalski, Shirley Frederiksen, Deneil Harney, Donna Harsh, Samkeliso Mawocha, Keith Pauls, Joy Pinkerton, Arthi Ramakrishnan, Valerie Stevenson, Wenle Zhao, Robert Silbergleit\nNeurological Emergencies Treatment Trials (NETT) Network Clinical Coordinating Center, University of Michigan Neurological Emergencies Treatment Trials (NETT) Statistical Data Management Center, Medical University of South Carolina\nBackground: Performance improvement in a clinical trial network requires metrics that are both valid and practical, but there can be tension between these two qualities. Granular metrics with face validity can be more burdensome than metrics using aggregated data. To measure data collection accuracy, we compared two alternatives. One was an item-based metric (IBM) that required added coding by site monitors but had evident face validity. The other was a form-based metric (FBM) that was readily accessible from typically available information, but with less evident validity.\nObjective: To determine the correlation between a pragmatic FBM of data accuracy in case report form (CRF) completion and an item-based metric IBM with greater face validity, and to compare the characteristics of each.\nMethods: The IBM described the number of data items corrected / number of monitored CRFs after source document verification and data clarification request (DCR) by a site monitor. This required coding every individual DCR as to whether it did or did not result in a correction. The FBM described the number of CRFs with 1 site monitor DCR’s / the number of CRFs monitored. The metrics were calculated for 16 enrollment Hubs participating in an ongoing data-intensive clinical trial of acute traumatic brain injury. Spearman Rank Correlation by Hub was determined. The distributions of the two metrics were characterized.\nResults: There were 10,693 monitored CRFs for 663 subjects containing 2,297 DCRs. 1,344 DCRs led to data corrections. The Spearman rank correlation between the IBM and the FBM was 0.77(p=0.00074). Distributions of the two alternative metrics were similar.\nConclusions: Development of performance metrics may involve tradeoffs between apparent validity and pragmatic implementation. In this application of CRF completion data accuracy metrics we found that a more pragmatic FBM closely approximates but does not completely reproduce the results of the IBM.\nA73\nQuantum Leap - Leveraging Real-Time (Direct) Data-Entry to Increase Speed, Improve Quality, and Dramatically Reduce Costs\n(1) Dean Gittleman, (2) Jules Mitchel, (3) Dario Carrara, (4) Judith Schloss Markowitz\n(1) Target Health Inc. (2) Target Health Inc. (3) Ferring Pharmaceuticals A/S (4) Target Health Inc.\nElectronic Data Capture (EDC) has been in routine use in clinical trials for over a decade. Its promise of better data faster, though, continues to go largely unrealized. In part this is due to layering its use on top of processes developed for paper-based trials. In part this is due to the timidity of sponsor organizations to take advantage of Regulatory gifts (e.g., Risk-based Monitoring, eSource). This resistance to change manifests itself in several ways, but particularly in the continued reliance on site-based source data verification (SDV) by monitors, a practice which we view as both high cost and low value. This paper describes the lessons learned from recently completed trials and one ongoing phase III trial. As such, it presents real-world experience in the use of processes optimized around direct data-entry, using an EDC tool specifically tooled to support real-time data-entry by investigative sites. The authors present data that demonstrate the value inherent in capturing trial data at the time of subject visits – specifically that this enables near real-time monitoring, improved data quality, and improved ability to make evidence-based decisions much faster, while at the same time dramatically reduced monitoring costs, and delivering an improved site experience. Further, the authors describe lessons learned from the implementation of risk-based monitoring plans, centralized (or remote) monitoring, and regular quality reviews, and how these are enabled through the use of technology-enabled, real-time capturing of data at investigative sites. Finally, the paper demonstrates that real-time (direct) data-entry is not only feasible, but preferable to stakeholders, and lays out the challenges and pay-offs for moving in this direction.\nA74\nGraphical Methods for Monitoring Clinical Trial Data\n(1) Elizabeth Sugar, (2) Lea Drye, (3) Thomas Louis, (4) Janet Holbrook\n(1-4) Bloomberg School of Public Health, Johns Hopkins University\nMonitoring the trade off between risks and benefits as well as the integrity of the data are important components of clinical research. One of the roles of a data coordinating center (DCC) is to provide the data safety monitoring board [DSMB]) with the information necessary to adequately evaluate the study. Summary statistics and pre-defined modeling make up one component of DSMB reports, however these summaries usually examine separately adverse events and efficacy outcomes, or are focused on very specific questions (e.g. visual acuity at six months after cataract surgery). Graphical representations of the data often provide greater flexibility to explore the individual components and simultaneously to examine multiple factors. To this end, we describe our experience as a DCC using graphical methods for monitoring data. Understanding the longitudinal patterns in data is one of the most important tasks, but it is also one of the most difficult to accomplish. Standard techniques such as Kaplan-Meier (KM) survival curves, sequential box-plots, scatterplots and spaghetti plots are all regularly used, but do not provide a complete picture. KM curves are useful for identifying the first event but ignore the initial event duration and all subsequent events. Spaghetti plots have the ability to track efficacy outcomes and adverse events simultaneously, but often become unreadable for moderate to large samples. Additional techniques are being developed (e.g., time-lines and Lasagna plots), and these can be adapted to provide information about both risk and benefit over time. Careful selection of sorting variables is especially important when using techniques that display individuals on a single line over time. Development and evaluation of graphical techniques for displaying data is an important component of trial monitoring. We provide examples of their role in monitoring in asthma and ophthalmology trials, demonstrating both the strengths and limitations of several methods.\nA75\nIdentifying Potential Adverse Effects By Patients’ Ratings: A Proof-of-Concept Study of a Novel Approach\n(1) Rajnish Mago, (2) Angelica Kloos, (3) Constantine Daskalakis, (4) Michelle Shwarz, (5) Barry W. Rovner\n(1) Thomas Jefferson University, (2) Department of Child and Adolescent Psychiatry, Children’s National Medical Center, (3) Thomas Jefferson University, (4) Thomas Jefferson University, (5) Thomas Jefferson University\nObjective: Methods to evaluate adverse effects in clinical trials are significantly underdeveloped compared to those for evaluating efficacy. In this pilot proof-of-concept study, we preliminarily compared a novel approach ‘the Symptom Assessment Tool (SAT)’ to a systematic and detailed assessment by a physician for identifying symptoms that were potentially adverse effects (sensitivity) and excluding symptoms that were unlikely to be adverse effects (specificity).\nMethods: The SAT consisted of a) a symptom inventory and rating of symptom severity were completed before starting a psychotropic medication (or increasing its dose) and again 2 weeks later, and b) an algorithm based on questions answered by patients that supported or argued against the symptom being an adverse effect. Each symptom was systematically assessed by both the patient-rated SAT and by a physician and was classified as either a potential or an unlikely adverse effect. The primary analysis compared the classification of symptoms by the SAT to that by the physician. Potential adverse effects were subcategorized as possible or probable adverse effects.\nResults: A sample of 193 symptoms from15 adults was evaluated, only 37.3% of which were considered potential adverse effects by the physician. Approximately half of the “treatment-emergent” symptoms were considered unlikely adverse effects by the physician, most commonly due to a clear alternative explanation. Sensitivity of the SAT compared to physician’s assessment was 90.3% for potential adverse effects and 97.5%for the subgroup of probable adverse effects. The SAT correctly identified 63.6% of the symptoms as unlikely adverse effects (specificity), and its negative predictive value was 91.7%.\nConclusions: The SAT, appropriate for its intended use as an initial patient-rated tool in clinical trials, had high sensitivity and moderate specificity and could present the investigator with a limited number of potential adverse effects for further assessment and intervention. Further evaluation and refinement of this approach is warranted.\nA76\nThe Risks and Benefits Associated with Running Investigator Led Early Phase Trials in Collaboration with Pharmaceutical and Charity Partners\n(1) Nina Thenagaran (2) Louise Flanagan (3) Sarah Brown (4) Sarah Flynn (5) Deborah Sherratt (5) Avie-Lee Coney\nClinical Trials Research Unit, University of Leeds\nThe Clinical Trial Coordinating Office (CTCO) based at the University of Leeds manages a portfolio of early phase multiple myeloma trials in collaboration with the charity Myeloma UK and their Early Phase Clinical Trial Network, and a number of pharmaceutical companies who supply the compounds of interest. These symbiotic collaborations between charity, Clinical Trials Unit (CTU) and the pharmaceutical industry are becoming common practice in clinical research, especially in the wake of recent government reductions to the scientific research budget in the United Kingdom. Now more than ever, these parties have a greater mutual interest in sharing costs and experiences, pooling of risks and ultimately providing high quality scientific research to improve treatment options for patients. The vast experience offered by the pharmaceutical industry, CTCO and Active Trial Centers (ATCs) allows the relatively inexperienced charity to drive forward clinical research in their particular disease area. There are of course, disadvantages to these collaborations, in that each party has different goals and missions which may cause conflict and subsequent time delays. Based on the experiences of the CTCO, we identify the risks and benefits of such collaborations, including the impact of funding on the prioritization of and recruitment in to investigator led trials; the autonomy to test regimens that may not ordinarily be considered and the issues surrounding new collaborations with inexperienced partners.\nA77\nManaging Performance of Field Staff in the Gulf Long-Term Follow-Up Study (Gulf Study)\n(1) Steven K. Ramsey, (1) Edward E. Gaunt, (1) Brian D. Blackmon, (1) Matthew D. Curry, (1) Christopher K. Treseder, (1) David A. Johndrow, (2) Richard K. Kwok, (2,3) Larry S. Engel, (2) Dale P. Sandler\n(1) SRA International, Inc.; (2) National Institute of Environmental Health Sciences; (3) University of North Carolina\nStandardization in data collection activities is critical in assuring the validity of health studies, especially in large-scale, field-based studies examining a wide range of outcomes. This presentation describes our approach to managing performance of field staff to achieve standardization in the GuLF STUDY, a large-scale epidemiological study investigating the potential health effects associated with exposure to oil and dispersants among clean-up workers who responded to the 2010 Deepwater Horizon oil spill. More than 31,000 participants have enrolled to date, including 8,600 who completed a home examination that included anthropometric, clinical, and environmental measurements. Home examinations have been conducted by 57 home-based certified medical assistants (CMAs) who live in the five Gulf states. CMAs are required to complete a four-day, in-person training session provided by the study coordinating center, which includes practice and certification activities. At the time of training, CMAs receive a variety of job-aides to ensure adherence to a standard procedures. Following training, state-based managers oversee the CMAs in collaboration with the coordinating center field operations manager. The operations manager circulates weekly reports to managers that include performance metrics by state and CMA and over time (e.g. procedural completion rates, conditions of specimens) and holds weekly manager teleconferences to review the reports. State managers use these reports to identify areas where additional coaching or re-training is needed. In addition, state managers periodically observe visits and record their observations on standardized quality control forms that are transmitted to the coordinating center. The findings from these visits are also used to identify areas where additional training is needed. Our approach may be of interest to other coordinating centers managing large-scale, field-based investigations.\nA78\nEstimating Site Costs Prior to Conducting Clinical Trials – A Study Site Budgeting Tool\n(1) Dorothee Arenz, (2) Barbara Hero, (3) Barbara F. Eichhorst, (4) Martin Langer, (5) Lars Pester, (6) Julia von Tresckow, (7) Maria J.G.T. Vehreschild, (8) Oliver A. Cornely\n(1) (4) (8) Clinical Trials Center Cologne, ZKS Kˆln, BMBF 01KN1106, University of Cologne; (2) Department of General Pediatrics, University Hospital Cologne; (3) (5) (6) (7) 1st Department of Internal Medicine, University Hospital Cologne\nRationale: Development of a tool for calculating trial fees prior to initiating a clinical trial. Background. Conducting clinical trials is costly and time-consuming. Trial sites usually do not calculate site costs. Underestimating required resources slows enrollment and lowers data quality. It is unclear how to reliably estimate trial site costs.\nMethods: To develop this tool, trial staff from sites at the University of Cologne, Germany formed a task group. Tasks within a clinical trial were itemized into single activities and basic time expenditures were assigned. Hourly rates for different occupation groups involved were derived from total labor costs. No costs invoiced to third parties, e.g. pass through costs for imaging or laboratory tests were included. Results were used to design a cost calculation tool. The resulting tool underwent round robin tests by having study coordinators from various clinical disciplines calculate time expenditures based on the same study protocol and case report forms to validate the tool. In addition, study coordinators of one site calculated time expenditure of all trials initiated and tracked time prospectively over a period of 12 months to assess the predictive value of the tool.\nResults: The study site budgeting tool (STUDGET) is a web-based application determining hours of work on a trial and calculates responding hourly rates of staff, totalizing them to fees required to properly conduct the trial. By improving the accuracy of STUDGET in round robin tests, we achieved a median deviation of 371.59 (range 43.58 - 1,152.08) in calculated case payments (reference 1,671.97). Comparison of predicted and actual hours showed a correlation of 105% median (range 18% - 228%). Outliers were due to unforeseeable changes in trial execution.\nConclusion: We developed the web-based tool STUDGET allowing trial sites to determine the case fee needed to conduct a clinical trial.\nA79\nOnline Request System for Datasets, Writing Groups, and Ancillary Studies for the Search for Diabetes in Youth Study\nJennifer W. Talton, John Hepler, Jeanette S. Andrews, Kenneth Wilson, Donna Kronner, Ronny A. Bell, Ralph B. D’Agostino Jr.\nWake Forest School of Medicine\nWake Forest School of Medicine serves as the Coordinating Center (CoC) for the SEARCH for Diabetes in Youth Study (SEARCH). SEARCH is a multi-center observational study that began conducting population-based ascertainment of cases of non-gestational diabetes in youth less than 20 years of age beginning in 2001 and continuing through the present. In addition to identifying cases, the project has a prospective cohort component that involves follow-up visits. Most of the roughly 60 active investigators who are looking to analyze the large amounts of data SEARCH collects choose to create writing groups, which include a statistician from the CoC to conduct the analyses. For investigators who prefer to analyze the data themselves, a CoC statistician provides them with a dataset. SEARCH also has 6 ancillary studies to the main project and new requests for ancillary studies continue to be made. In order to handle the large volume of requests (writing groups, datasets, ancillary studies), which are all made through the CoC, an online request system was designed which streamlines communication and documentation between the investigators and the CoC. The request system is a step-by-step wizard which guides the investigator through a series of questions and information screens that must be completed prior to sending the request to the CoC. For example, in the dataset request system the investigator must have approval from the Papers and Presentations committee before the request can be submitted and they cannot request a dataset completion deadline that is less than one-month from the submission date. Once completed an e-mail is generated to both the CoC and investigator with the request attached in PDF form. This presentation will elaborate on the specifics of the system by describing additional features available; such as tracking the progress of the request (submitted, completed) which was created for documentation purposes.\nA80\nWhat Makes People Participate in Clinical Trials? Insights from a Meta-Ethnography of Qualitative Studies\nSharon K McCann, Marion K Campbell, Vikki A Entwistle\nHealth Services Research Unit, University of Aberdeen\nIt is well known that recruitment to trials can be difficult. In recent years a number of in-depth qualitative studies have been published that have examined patient experiences of recruitment and participation in trials. To understand the collective insights from these studies, we undertook a meta-ethnography (a formal synthesis method which centres on the construction of interpretations cumulatively across qualitative studies) of qualitative studies published between 1996 and 2010. To be included all studies had to focus on people’s own accounts of their decisions to accept or decline trial participation. Our synthesis highlighted how key aspects of context, recruitment approach and person approached can interact to influence trial recruitment. In particular, the way potential participants were situated in terms of their health states and treatment junctures (and perceptions about these at the time of trial recruitment) was particularly salient. Their sense of their situation at the time of being approached about trial participation influenced their judgements, particularly around the implications of trial participation for the common good (their desire to help others) and their own identity in relation to participating (what their non/participation might say about them). It could also mediate the influence of their communication and relationship with trial recruiters and of the nature of the trial interventions and processes. These insights led to the development of a conceptual model of factors likely to affect trial recruitment. The results of the synthesis and model of factors likely to affect recruitment will be presented together with strategies trialists might adopt using the insights they offer.\nA81\nChallenges of Composite Endpoints for Trial Execution, Statistical Analysis and Interpretation\nJoerg Hasford\nLudwig-Maximilians-Universitaet Muenchen Institut fuer Med. Informationsverarbeitung, Biometrie und Epidemiology\nRecent years have seen an exceptional improvement of overall survival(OS) in patients with chronic myeloid leukemia.Thus classical endpoints like OS would need either huge sample sizes or very long observation time. Composite endpoints, combining OS with events like progression, not achieving complete cytogenetic or molecular remission within a certain treatment period (typically 12 or 18 months) and loss of remission, called Progression-free Survival (PFS) or Failure-free survival(FFS), were introduced to allow for a faster evaluation of new treatments. FFS subsumes ~ 10 singleton events ranging from death to loss of response. FFS data are thus difficult to interprete and provide limited value for informing clinical decisions as the relevance of the singleton endpoints is very heterogeneous. Hence drug authorities ask that CE do not mix disease outcomes with clinical management decisions. An often neglected issue is that intention to treat analysis of CE requires that all singleton events have been properly monitored and assessed at the time points specified in advance in the trial protocol. Thus in a large CML trial out of 1466 evaluable pts only 571 were available for the analysis of FFS (Lauseker M et al. ELN Frontiers Meeting 2012). Including in the statistical analysis patients with missing information on singleton endpoints may seriously bias results. Yet another problem is the common lack of widely accepted uniform definitions of CEs precluding a comparative assessment across trials. The challenging issues of CEs will be exemplarily discussed on the basis of two recently published CML-trials (Saglio G, et al. NEJM 2010;362:2251-2259, and Kantarjian HM, et al. NEJM 2010;362:2260-2270). There is an urgent need for consented CEs and alternatives to CEs.\nA82\nPaint me a Picture: Using Visual Data Representation to Summarize and Explore Data\nKatherine Freier\nBoston Biomedical Associates\nWhile p-values may be the key to ultimate study success, they give very little information about the data collected. Confidence intervals are an improvement because they communicate both value and variability, but there is often much more to be learned from our data if we go beyond numerical summaries. Patterns, trends, and relationships can get lost in numeric summary tables but will jump off the page when shown with the appropriate figure. Much can be done with the basic go-to favorites of histograms, scatterplots, boxplots, and Kaplan-Meier survival curves. This presentation, designed for non-statisticians, takes a look at simple twists on these old favorites with examples of what they can offer to give a broader look at your data through exploratory data analysis. Possibilities beyond histograms, scatterplots, boxplots, and Kaplan-Meier curves are touched on as well.\nA83\nAnne Zielinski\nMedidata Solutions\nMobile computing is becoming ubiquitous, with even toddlers ( http://abcnews.go.com/WNT/video/ipad-effect-toddlers-14813787 ) and seniors ( http://www.lvrj.com/business/more-seniors-get-savvy-with-smartphones-155271575.html ) using and navigating mobile devices. What does the explosion is smartphone and tablet ownership and usage mean for clinical research? Use of handheld devices to collect data from subjects is commonplace, and the use of tablet computers to capture source data, whether from investigators or patients, is becoming more common. As smartphones and tablets grow in popularity, even penetrating areas of the globe where consistent electricity is problematic, researchers have the opportunity communicate directly with patients. A recent direct-to-subject study highlighted somethe opportunities available, but also pointed out some limitations not addressed by technologies. This talk will review the technologies applicable for direct-to-subject communication - traditional ePRO, web-based communications, IVR, SMS text messaging, smartphone apps- and specialized technologies that capture biological measurements directly from patients. The appropriateness and limitations of each for various clinical research scenarios and therapeutic areas will be explored. Finally, some crystal-ball gazing will be indulged, examining opportunities and possibilities generated by mobile technologies that have been heretofore unrealistic.\nA84\nNational Lung Screening Trial (NLST): Post-Trial Data and Image Exploration with an Open Source Query Tool\n(1) Kenneth Clark, (1) Paul Commean, (2) Joshua Rathmell, (2) Thomas Riley, (2) Gordon Jones, (3) Guillermo Marquez, (1) Fred Prior\n(1) Washington University in Saint Louis (2) Information Management Services, Inc. (3) National Cancer Institute/NIH/DHHS\nThe NLST recruited 53,454 participants with histories of heavy smoking and at high risk for developing lung cancer to determine whether image-screening reduces lung-cancer mortality. Participants were randomized to Computed-Tomography (CT) or chest-X-ray arms and received up to 3 screens each, at annual intervals. NLST findings (announced 2011) suggest that CT screening does reduce lung-cancer mortality while chest-X-ray screening does not. NLST non-image data are maintained by Information Management Services (IMS). The CT images (~20 million) are a collection of The Cancer Imaging Archive (TCIA), managed by Washington University (WU). Data and images are available to NLST investigators with approved research plans. In an effort to stimulate and facilitate post-trial secondary research, a WU-IMS partnership created a PostgreSQL-based Query Tool (QT) that allows investigators to quickly search NLST trial data by building their own queries using simple tree menus. QT selection-menus consist of tables (13) and variables (300+) that may be conditioned with yes/no, numerical-range, and pick-list values. Results may be saved to files suitable for statistical analysis. Queries may be saved for later re-use. Hover-over any variable displays its tool-tip data-dictionary-paraphrase. A Help drop-down-menu offers a complete data dictionary, QT User Guide, and QT Tutorial. The QT offers the option to download the CT images associated with query results. If an investigator chooses to download images immediately, the QT launches the image-downloader of the National Biomedical Imaging Archive (NBIA) application, a key component of TCIA. Alternatively, the investigator may choose to save the image pointers as an NBIA shared-list; the investigator and any collaborator may access this shared-list at any time to download the list images. QT allows researchers to explore completed-trial data and download images for, say, retrospective studies or computer-aided-detection algorithm development. QT is open-source and easily configured for other trials, with or without images. Reference: https://biometry.nci.nih.gov/cdas/studies/nlst\nA85\nThe NIH Health Care Systems Research Collaboratory Distributed Research Network\n(1) Jeffrey Brown, (2) Lesley Curtis, (3) Kimberly Lane, (4) Beth Syat, (5) Richard Platt\n(1) Department of Population Medicine of Harvard Medical School and Harvard Pilgrim Health Care Institute (2) Duke University School of Medicine (3) Department of Population Medicine of Harvard Medical School and Harvard Pilgrim Health Care Institute (4) Department of Population Medicine of Harvard Medical School and Harvard Pilgrim Health Care Institute (5) Department of Population Medicine of Harvard Medical School and Harvard Pilgrim Health Care Institute\nBackground: In September 2012, the NIH funded the Health Care Systems Research Collaboratory to engage health care systems as research partners in conducting multi-center pragmatic and cluster randomized clinical studies. The Collaboratory consists of a Coordinating Center and, initially, seven health system-based clinical trial demonstration projects. The Collaboratory is developing broadly applicable tools, methods, and policies to support these activities. One aspect of this work involves developing infrastructure and methods for the use of multi-center electronic clinical, administrative, and research data. Scope The Collaboratory will create the NIH Distributed Research Network to facilitate collaborative clinical research by allowing organizations to quickly and easily share information with research partners without disclosing protected health information or proprietary data. The NIH Distributed Research Network will enable organizations holding clinical or research datasets to 1) make detailed information (metadata) about clinical data resources and existing data sets available and 2) enable secure distributed querying of data that remain in the physical control of the clinical system or investigator that created it. In the parlance of the Office of the National Coordinator’s (ONC) Query Health program, the NIH Distributed Research Network will enable investigators to ‘send the question to the data’ instead of creating large pooled datasets.\nConclusions: The NIH Distributed Research Network will provide a new capability for multi-site clinical investigation by enabling secure distributed querying of clinical and research data without requiring disclosure of confidential or proprietary data.\nA86\nUsing a Webportal to Enhance Network Communication\n(1) Laurie McLeod, (2) Stephanie Wellford, (3) Emily Wilson\n(1) Rho (2) Rho (3) Rho\nIn federally-funded projects, Rho is responsible for managing and coordinating communication for large, multi-center collaborative research efforts. Communication within these large networks can be a challenge due to the number of people involved, the volume of documents and versions distributed, and varying schedules and time zones. Over the last 10 years, communication in clinical research has changed dramatically with technological advances. Direct mail and facsimile have been replaced with more immediate and reliable forms of communication. Rho adapted by creating secure network webportals to disseminate current, readily available information to all network members in an efficient manner. The network webportals house general information, study documents, documents in development, and meeting materials for stakeholders to reference. Managing study and meeting materials real-time and in a central location allows Rho to easily direct stakeholders to information or documents, ensures that all stakeholders are accessing the same version of materials, and reduces the volume and size of documents in email inboxes. Collaborative abstracts, manuscripts, and other working documents are easily edited and re-posted through a virtual check-out system, simplifying version control and streamlining the review process. The webportals are secured through a log-in system, which requires stakeholders to enter a username and password before accessing any content. Rho manages user permissions to guarantee each stakeholder can view the information that relates to their scope of work and interest. These permissions also can be used to allow for distribution of materials to an exclusive group. Our experience and feedback from stakeholders indicate that other multi-center programs could benefit from using network webportals as an information hub to provide network members with current information and documents in a structured, easily accessible form.\nA87\nDevelopment of Data Diamond Reporting System\nSherrie Whisler\nPennsylvania Academy of Family Physicians and Foundation\nThe Pennsylvania Academy of Family Physicians and Foundation developed a quality data reporting website with grant funds from the PA Department of Health (PADOH). Phase I development was fast-tracked, starting January 2012 and in production by June 2012. Primary care practices using the site report data on Diabetes, Asthma, IVD, HTN, and Preventative measures and were trained and using the site by August 2012. The project is called Data Diamond, and its primary design was to establish a website for data reporting in a very dynamic, highly efficient, and scalable way. Using SQL Server 2008, Microsoft .net, and LogiXML, we quickly produce high quality data reports for funders, including the PADOH, and practices. Development of the site allowed us, as an organization, to house multiple projects on one site and establish access to the site for the users based on project permissions. Based on the data model of more than 50 entities, all data entry and reports drive the system’s performance and allow the administrator to define project, practice, and measure in minutes. Subsequent web pages appear instantly or in seconds. User Roles define which areas users can access based on their role in the project. Currently we have more than 4,500 defined users, such as consultant, provider, leader, faculty, and physician champion. The new system easily produces dynamic sets of data reports. Analysis is the core of the Data Diamond website. Stakeholders get an overview of data and can drill down as far as the data allows for more specific information. Phase II development is underway and we plan to develop a patient-specific registry to track data on individuals rather than just total population. With the new Activity/Event tracker, users will be reminded of Learning Sessions, CME Events and register for events online from the system.\nA88\nSimulation-Based Evaluations of Methodological Alternatives for Recycling Randomization Assignments in Randomized Clinical Trials (RCTS) of Acute Conditions (AC) with Brief Follow Up\n(1) Mohamed Mubasher*, (2) Abdullah Alangari, (1) Mohammad Al-Tannir, (1) Imad Tlejeyjeh, (1) Sameeh Ghazal\n(1) Clinical Research Center, King Fahad Medical City (2) Department of Pediatrics, College of Medicine, King Saud University\nBackground: In RCTs of AC (pediatric asthma, flu-like symptoms, etc.) subjects are randomized and kept under brief observation (e.g., ER setting or outpatient clinic) to determine the study outcome(s). If future episodes recur, a subset of these subjects might become eligible. Considerations to re-recruit such subjects constitute a challenging methodological task from randomization schematic perspective and future data analysis.\nObjectives: To present and evaluate alternatives for re-randomization schemes in AC RCTs.\nMethods: Assume in a 2-arm AC RCT, n1 and n2 subjects (n1 + n2 = m) are randomized according to a 1:1 permuted block design (PBD) and then evaluated for the study outcomes. Let n11 and n22 of the n1 and n2, become eligible. We investigated 3 scenarios to re-randomize these subjects: 1) According to the ongoing PBD as independent randomization units, 2) Crossing them over to the other arm according to a preset scheme and 3) Re-enroll them according to their original assignments and treat their outcomes as repeated measures. The distribution of prognostic characteristics was compared between study groups using conditional dichotomous logistic regression to determine the adequacy of the 3 re-randomization scenarios. 100,000 simulations of randomization situations were generated; m = 300 (100) 1000; n ij; i < j =1,2 were selected randomly between n i /4 and n i /2. A non-homogeneous Poisson process was used to generate subjects’ entry. Five dichotomous prognostic factors were simulated with variable proportions; P i = 0.10 (0.20) 0.50 but equal distribution between the two groups.\nResults: For the five prognostic factors: 1. All scenarios produced variable but adequate probabilities of between-groups balance 2. The likelihood of balance was highest under the cross-over scenario, followed by the scheme of retaining the same randomization assignment. Conclusions Cross-over schemes seem to be the most adequate for re-randomization in such RCTs.\nA89\nThe Cameroon Mobile Phone SMS (CAMPS) Trial: A Randomized Trial of Text Messaging Versus Usual Care for Adherence to Antiretroviral Therapy\n(1) Lawrence Mbuagbaw, (2) Lehana Thabane, (3) Pierre Ongolo-Zogo, (4) Richard T Lester, (5) Edward J Mills, (6) Marek Smieja, (7) Lisa Dolovich, (8) Charles Kouanfack\n(1) Centre for the Development of Best Practices in Health (CDBPH), YaoundÈ Central Hospital/ Department of Clinical Epidemiology and Biostatistics, McMaster University (2) Department of Clinical Epidemiology and Biostatistics, McMaster University/ Biostatistics Unit, Father Sean O’Sullivan Research Centre, St Joseph’s Healthcare (3) Centre for the Development of Best Practices in Health (CDBPH), YaoundÈ Central Hospital (4) Department of Medicine, Division of Infectious Diseases, University of British Columbia/ British Columbia Centre for Disease Control (5) Faculty of Health Sciences, University of Ottawa (6) Department of Clinical Epidemiology and Biostatistics, McMaster University/ St. Joseph’s Healthcare (7) Department of Family Medicine, McMaster University, McMaster Innovation Park (8) Accredited Treatment Centre, YaoundÈ Central Hospital\nBackground: Mobile phone technology is a novel way of delivering health care and improving health outcomes. This trial investigates the use of motivational mobile phone text messages (SMS) to improve adherence to antiretroviral therapy (ART) over six months.\nMethodology/Principal findings: CAMPS was a single-site randomized two-arm parallel design trial in YaoundÈ, Cameroon. We enrolled and randomized HIV-positive adults on ART, aged 21 years and above to receive a weekly standardized motivational text message versus usual care alone. The primary outcome was adherence measured using a visual analogue scale (VAS), number of doses missed (in the week preceding the interview) and pharmacy refill data. Outcomes were measured at 3 and 6 months. Service providers and outcome assessors were blinded to allocation. Analysis was by intention-to-treat. Between November and December 2010, 200 participants were randomized, with 101 in the intervention group and 99 in the control group. At 6 months, overall retention was 81.5%. We found no significant effect on adherence by VAS > 95% (risk ratio [RR] 1.06, 95% confidence interval [CI] 0.89, 1.29; p= 0.542; reported missed doses (RR 1.01, 95% CI 0.87, 1.16; p>0.999) or number of pharmacy refills (mean difference [MD] 0.1, 95% CI: 0.23, 0.43; p=0.617. One participant in the intervention arm reported a possible disclosure of status.\nConclusions/Significance: Standardized motivational mobile phone text messages did not significantly improve adherence to ART in this study. This study was different from other studies in terms of its duration, sample size and duration of participants on ART. Other types of messaging or longer term studies are recommended.\nA90\nPractical Considerations on Medical Device Clinical Trials in Georgia, A Former Soviet Union Country\n(1) Giada Morpurgo, (2) Tamar Kemoklidze, (3) Marco Ghidina, (4) Federica Zobec\n(1) Zeta Research Ltd (2) CCRG Ltd (3) Zeta Research Ltd (4) Zeta Research Ltd\nBudget and time restrictions often lead clinical trials to be conducted outside European borders. Two post market clinical trials on a medical device have been conducted in 2012 in Georgia, a former Soviet Union country geographically and culturally very close to Europe. The investigational medical device was a class I ophthalmic solution for allergic conjunctivitis. The trials were structured on different populations (pediatric and adult) and with different design (randomized placebo-controlled, and single arm). Many direct and indirect economic advantages has been registered, basically based on the fact that Georgia offers small and flexible private clinical structures with diffused high speed internet and qualified medical personnel, even if slight differences in ordinary medical practices can be met. Legislation’s principles follow the GCP and international harmonization guidelines but the practical management of a clinical trial is strongly favored by quick authorization procedures (no formal approval by the MoH is required for medical device trials) and flexible session calendars of the ethical review boards, with great time saving compared to European average standards. Contracts and budget management with sites and investigators too seems to be cheaper and strongly simplified. Lack of web-available information about procedures and regulation, investigational products import custom procedures and the need for all the documentation to be translated in local Georgian language makes a local CRO support precious and strongly recommended. Cultural difference mostly with the older generations appear in the patient involvement in the study: as no reimbursement of expensive biotech and other products is provided, Georgian patients are globally looking at clinical trials as an opportunity to receive the latest medical treatments, but patients’ retention in the study seems to be a critical aspect as patients tent to fail in follow up visits after reaching benefits from the treatment.\nA91\nA New Trial Design Fully Integrating Biomarker Information for the Evaluation of Treatment-Effect Mechanisms in Personalised Medicine\n(1) Richard Emsley, (2) Hanhua Liu, (3) Sabine Landau, (4) Graham Dunn\n(1) Centre for Biostatistics, The University of Manchester (2) Centre for Biostatistics, The University of Manchester (3) Institute of Psychiatry, King’s College London (4) Centre for Biostatistics, The University of Manchester\nThe development of personalised (stratified) medicine is intrinsically dependent on an understanding of treatment-effect mechanisms (effects on therapeutic targets that mediate the effect of the treatment on clinical outcomes). There is a need for novel clinical trial designs for the joint evaluation of treatment efficacy, the utility of predictive markers as indicators of treatment efficacy, and the meditational mechanisms proposed as the explanation of these effects. We review the problem of confounding (common causes) for the drawing of valid inferences concerning treatment-effect mechanisms, even when the data has been generated using a randomised controlled trial. We illustrate the potential of the predictive biomarker-stratified design, together with baseline measurement of all known prognostic markers, to enable us to evaluate both the utility of the predictive biomarker in such a stratification and, perhaps more importantly, to estimate how much of the treatment’s effect is actually explained by changes in the putative mediator. We call this a biomarker stratified efficacy and mechanisms evaluation (BS-EME) trial. The analysis strategy involves the use of instrumental variable regression, using the treatment by predictive biomarker interaction as an instrumental variable - a refined, much more powerful and subtle use of Mendelian randomisation. Stratification without corresponding mechanisms evaluation lacks credibility and in the almost certain presence of mediator-outcome confounding, mechanisms evaluation is dependent on stratification for its validity. Using Monte Carlo simulation we show that both stratification and treatment-effect mediation can be evaluated using the BS-EME trial design together with detailed baseline measurement of all known prognostic biomarkers and other prognostic covariates. The direct and indirect (mediated) effects are estimated through the use of instrumental variable methods together with adjustments for all know prognostic markers (confounders) the latter adjustments contributing to increased precision (as in a conventional analysis of treatment effects) rather than bias reduction.\nA92\nWeb-Based Intervention Modelling Experiments: A Way of Exploring Professional Behaviour Change Interventions Before a Full-Scale Trial\nShaun Treweek, Debbie Bonetti, Karen Barnett, Martin Eccles, Jill Francis, Claire Jones, Graeme MacLennan, Nigel Pitts, Ian Ricketts, Frank Sullivan and Mark Weal\n(1) University of Aberdeen; (2) University of Dundee; (3) University of Edinburgh; (4) University of Newcastle; (5) City University; (6) University of Dundee; (7) University of Aberdeen; (8) King’s College; (9) University of Dundee; (10) University of Dundee; (11) University of Southampton\nBackground: An intervention modelling experiment (IME) is a theory-based technique that can guide the choice of professional change interventions by simulating encounters between clinicians and patients prior to entering the intervention into a full-scale trial. To date IMEs have all been paper-based. To test the IME methodology we replicated an earlier, paper-based IME of antibiotic prescribing by family doctors for upper respiratory tract infections using web-delivery. If the IME methodology is robust, the same results should be seen in both IMEs.\nMethod: The study was a two-stage, national trial targeting family doctors. In the larger stage, Stage 1, each participating doctor received a web-based questionnaire and clinical scenarios, which were designed to identify predictors of doctors’ antibiotic prescribing behaviour. Stage 2 delivered a second questionnaire and set of scenarios to evaluate the effect of two, web-delivered interventions on doctors’ prescribing behaviour, one taken from the paper-IME, the other developed from Stage 1, compared to a ‘no information’ comparator.\nResults: 270 doctors took part in Stage 1. Eight of ten predictors of prescribing behaviour were in agreement between the paper and web-based IMEs. From these predictors, a new theory-based intervention based on Action Planning was developed. Stage 2 involved 131 GPs and those receiving the interventions showed increases in the number of scenarios without an antibiotic prescription (Action Plan=0.82 (95% CI=0.26 to 1.37); Persuasive Communication=0.73 (95% CI=0.14 to 1.31). This was in agreement with the paper-based IME.\nConclusions: The IME methodology is robust and can be effectively delivered over the web to develop and explore behaviour change interventions prior to a full scale trial. This is likely to lead to interventions with a greater potential for effecting the desired behaviour change.\nA93\nRandomizing Two Study Eyes from the Same Participant in Ophthalmic Clinical Trials\n(1) Adam R. Glassman, (2) B. Michele Melia\n(1,2) Jaeb Center for Health Research\nThere are many examples of ophthalmology trials enrolling only one eye or requiring enrollment of both eyes, but few allow participants with one or two study eyes depending on the number eligible. The Diabetic Retinopathy Clinical Research Network ( DRCR.net ) has conducted several clinical trials that have allowed but not required both eyes of participants to be randomized. This design increases the potential pool of eligible eyes. When enrollment of both eyes is allowed, eyes may be randomly assigned to the same or different treatments with advantages and disadvantages to each approach. Randomizing one vs. two eyes: Assuming positive but not perfect correlation between the participant’s two eyes, there is greater power for a fixed number of participants when participants can contribute two eyes. The increase in power depends on the correlation and the proportion of patients with two study eyes and is presented in another abstract (Melia and Glassman). This can result in substantial savings on enrollment time and study cost. However, there may be barriers to enrolling both eyes including logistical complexities, safety concerns, or low frequency rate of bilateral disease. Randomization to different or same treatment groups: When eyes are randomized to different treatment groups, potential confounding with patient-level systemic factors is controlled for, decreasing variability and increasing statistical power. This approach may not be recommended if there is expected to be a cross-over effect in the contra-lateral eye, if attributing systemic adverse events to a particular drug is important, if masking is not feasible and the outcome is subjective, or quality of life assessment is important. Careful consideration should be given when designing a study where multiple measurements per participant are possible. If supported by the trial design, including and analyzing both eyes from the same participant can save time and money.\nA94\nPower for the Partially-Paired Randomization Design as Compared with the Unpaired Design in Ophthalmological Clinical Trials\nMichele Melia, Adam Glassman\nDiabetic Retinopathy Clinical Research Network (Jaeb Center for Health Research)\nBackground: When designing an ophthalmological trial, there frequently is a choice of designs: (1) randomize 1 eye, (2) require and randomize 2 eyes, or (3) randomize one eye when one eye is eligible and randomize both eyes when both eyes are eligible. In the latter 2 designs, eyes within a subject may be randomized to the same treatment or to different treatments. Each design has its advantages and disadvantages as presented in another abstract (Glassman and Melia). We explored the impact of proportion of subjects with 2 eyes and correlation between eyes on power for design 3, when eyes are randomized to different treatments, and compared with design 1.\nMethods: We simulated data for change in visual acuity, a common outcome in ophthalmology trials, for 10,000 trials of varying number of treatment arms, sample size, proportion with 2 eyes, correlation between eyes, and treatment effects. Simulated trials were analyzed using a linear mixed model to account for correlation and obtain estimates of power for treatment comparisons.\nResults: As expected, power of the partially-paired design was higher than that of the unpaired design for fixed sample size with positive correlation between eyes. Power of the partially-paired design was lower than the unpaired design when correlation was negative. Power gain for the partially-paired design was modest for the proportion of 2 eyes (0.20) and correlation between eyes (0.25) seen most frequently in our own clinical trials, e.g. 5% for detecting 3 letter difference with 150 subjects per group, but was considerably higher when there was greater correlation between eyes, e.g. 0.5 and higher, and/or when more subjects had 2 eligible eyes.\nConclusions: Our results allowed us to quantify the statistical gains of using the partially-paired design over the unpaired design which can be used to make better-informed decisions when choosing a design.\nA95\nAn Evaluation of Inferential Procedures for Adaptive Clinical Trial Designs with Pre-Specified Rules for Modifying the Sample Size\n(1) Gregory Levin, (2) Sarah Emerson, (3) Scott Emerson\n(1) University of Washington (2) Oregon State University (3) University of Washington\nMany papers have introduced adaptive clinical trial methods that allow modifications to the sample size based on interim estimates of treatment effect. There has been extensive commentary on type I error control and efficiency considerations, but little research on estimation after an adaptive hypothesis test. We evaluate the reliability and precision of different inferential procedures in the presence of an adaptive design with pre-specified rules for modifying the sampling plan. We extend group sequential orderings of the outcome space based on the stage at stopping, likelihood ratio test statistic, and sample mean to the adaptive setting in order to compute median-unbiased point estimates, exact confidence intervals, and P-values uniformly distributed under the null hypothesis. The likelihood ratio ordering is found to average shorter confidence intervals and produce higher probabilities of P-values below important thresholds than alternative approaches. The bias adjusted mean demonstrates the lowest mean squared error among candidate point estimates. A conditional error-based approach in the literature has the benefit of being the only method that accommodates unplanned adaptations. We compare the performance of this and other methods in order to quantify the cost of failing to plan ahead in settings where adaptations could realistically be pre-specified at the design stage. We find the cost to be meaningful for all designs and treatment effects considered, and to be substantial for designs frequently proposed in the literature.\nA96\nEvaluating the Impact of Increasing the Sample Size in the Secondary Prevention of Small Subcortical Strokes (SPS3) Trial: Hindsight is 20/20\n(1) Leslie A. McClure, (2) Jeff Szychowski, (3) Christopher S. Coffey\n(1) University of Alabama at Birmingham, Department of Biostatistics; (2) University of Alabama at Birmingham, Department of Biostatistics; (3) University of Iowa, Department of Biostatistics\nThe NIH-funded SPS3 study assessed whether combination antiplatelet therapy (AC) was superior to aspirin alone (AP), and whether lower blood pressure (BP) control was superior to ‘usual’ BP control in preventing recurrent stroke among patients with a lacunar stroke at baseline. Due to lower than expected overall rates of recurrent stroke, the sample size was increased from 2500 to 3000 to ensure adequate power to detect a 25% reduction in recurrent stroke in each arm. The antiplatelet trial was terminated early due to lack of efficacy coupled with safety concerns. Retrospectively, we assessed the gains achieved by the sample size re-estimation by comparing the difference in stroke recurrence among the first 2500 patients recruited, assuming follow-up ended when the antiplatelet arm terminated to results obtained with the complete 3020 patients. Among the first 2500 patients, we observed 248 events, over an average of 3.9 years follow-up. The recurrent stroke rate for the AC and AP groups was 2.5% and 2.7% per year, respectively. The hazard ratio for AC vs. AP was 0.92 (95% CI: 0.72-1.18). Including the additional 520, only 15 more patients had primary events, and recurrent stroke rates (2.7%, and 2.5% year), and the HR of 0.92 (95% CI: 0.72-1.16) were similar. Despite an increase in sample size in SPS3, there was no appreciable difference in the results obtained for the antiplatelet trial had the study adjustment not been made; however, there is potential for gain to be seen in the BP arm. While sample size re-estimation is an important tool for ensuring adequate power for detecting treatment effects of interest, there is a potential to add unnecessary cost and complexity when no effect exists. More post-hoc examinations of studies using adaptive designs are warranted to fully understand their strengths and weaknesses.\nA97\nA Systematic Review of Methods for Specifying the Target Difference in Randomised Controlled Trials (Delta Review)\n(1) Jonathan Cook, (2) Jenni Hislop, (3) Temitope Adewuyi, (4) Cynthia Fraser, (5) Kirsten Harrild, (6) Douglas Altman, (7) Craig Ramsay, (8) Peter Fayers, (9) Andrew Briggs, (10) John Norrie, (11) Ian Harvey, (12) Brian Buckley, (13) Luke Vale for the DELTA group\n(1) University of Aberdeen (2) University of Aberdeen (3) University of Aberdeen (4) University of Aberdeen (5) University of Aberdeen (6) University of Oxford (7) University of Aberdeen (8) University of Aberdeen (9) University of Glasgow (10) University of Aberdeen (11) University of East Anglia (12) National University of Ireland (13) University of Newcastle\nBackground: Randomised controlled trials (RCTs) are widely accepted as the preferred study design for evaluating healthcare interventions. When determining sample size, a (target) difference must typically be specified. This provides reassurance that the study will be informative i.e. should such a difference exist, it is likely to be detected with the required statistical precision. From both a scientific and ethical standpoint, selecting an appropriate target difference is of crucial importance; too large or small a study is arguable unethical, wasteful and potentially misleading. While a variety of methods have been proposed in the medical field to specify a target difference, their relative merits are unclear as is whether there are methods in the non-medical field which could be used. Aim: To systematically review of medical and non-medical literature to identify methods for specifying the target difference in a randomised controlled trial.\nMethods: Electronic searches of biomedical and non-medical databases were performed. Clinical Trial textbooks were also reviewed. Titles and abstracts were screened prior to full-text assessment. Studies which reported a method which could be used to specify the target difference were included. Clinical trial textbooks were also reviewed.\nResults: The search identified 11,485 potentially relevant studies; 1,434 were selected for full-text assessment. Seven methods were identified: anchor, distribution, health economic, opinion-seeking, pilot study, reviews of the evidence base and standardised effect size (SES). The anchor, distribution and SES methods were most commonly used.\nDiscussion: A variety of methods were identified though each had important variations in application. While no single method provides a perfect solution to a difficult question, methods are available to inform specification of the target difference and should be used whenever appropriate.\nA98\nSample Size and Screening Size Trade Off in the Presence of Subgroups with Different Expected Treatment Effects\n(1) Kyle Rudser, (2) Edward Bendert, (3) Joseph Koopmeiners\n(1) Division of Biostatistics, University of Minnesota (2) Statistics Collaborative (3) Division of Biostatistics, University of Minnesota\nStatistical study design considerations typically focus on sample size, power, and a single population treatment effect given a fixed significance level (generally 0.05). Eligibility criteria is formulated to select the patient population of interest to be studied for which the magnitude of the treatment effect is expected to hold. In some instances researchers may expect a larger treatment effect in one subgroup while others exhibit an attenuated effect. Identification of these subgroups can be based on a clinical decision rule, e.g., biomarker cutoff, but may not be precise, i.e., sensitivity and specificity are not simultaneously at 100%. In these settings there can be a trade-off between a smaller average treatment effect with broader enrollment criteria and a larger effect with restricted criteria but longer enrollment duration. We evaluate the impact of including attenuated subgroups on design operating characteristics and illustrate scenarios where overall trial enrollment duration, and hence time to complete the study, may be shorter by not being restrictive.\nA99\nImpact of Subject Attrition on Sample Size Determinations for Longitudinal Cluster Randomized Clinical Trials\nMoonseong Heo\nAlbert Einstein College of Medicine\nSubject attrition is a ubiquitous problem in any type of clinical trials and thus needs to be taken into consideration at the design stage particularly to secure adequate statistical power. Here, we focus on longitudinal cluster randomized clinical trials (cluster-RCT) that aim to test the hypothesis that an intervention has an effect on the rate of change in the outcome over time. In this setting, the cluster-RCT assumes a three level hierarchical data structure in which subjects are nested within a higher level unit such as clinics and are evaluated for outcome repeatedly over the study period. Furthermore, the subject-specific slopes can be modeled in terms of fixed or random coefficients in a mixed-effects linear model. Closed form sample size formulas for testing the hypothesis above have been developed under assumption of no attrition. In this paper, we propose closed form approximate samples size determinations with anticipated attrition rates by modifying those existing sample size formulas. Specifically, the two parameters in existing formulas that would be affected by subject attrition are the number and the variance of the assessment time points per subject. Our sample size determination strategy is to replace those two parameters by their corresponding expected number and variance under anticipated attrition rates. With extensive simulations, we examine performances of the modified approximate formulas considering the following factors: fixed and random slope models; different attrition rates; monotone attrition process; different types of distributions of attrition time points; and three attrition mechanisms: attrition completely at random, attrition at random and attrition not at random. In conclusion, the proposed modification is very effective under fixed slope models but yields biased, if not substantially, statistical power under random slope models.\nP01\nThe Evolution of the CRF Designer into a Data Manager\nChristina Tittsworth\nSGS Life Science Services\nThe role of the Case Report Form (CRF) designer has historically been independent; only needing to extract elements defined in the protocol to create CRFs. Now, the CRF designer is more data management and data standards oriented as clinical trials move to electronic data capture (EDC) systems. In the future, the “role” of the CRF designer will be pivotal in study and data management, applying standards for data collection, analysis, and sharing. Past: Relied solely on protocol for extracting CRF elements, Understood flow of data capture. Present: The CRF designer is mindful of the complexity of translating paper CRFs to eCRFs and understands: Intuitive and simple CRF design/508 compliance, Data standards and controlled terminology, Data analysis. Future: Maintains CRF library for faster study startup/quality control, Implements data standards with analysis and sharing in mind, Cost efficient, eliminates need to create paper spec to mimic eCRF As the electronic age pervades clinical trials, the role of CRF designer will transcend into clinical trials data manager. By recognizing important elements in the protocol, and using data management and data standards to streamline analysis elements, meaningful study results will be achieved.\nP02\nNon-Surgical Periodontal Therapy Reduces Coronary Heart Disease Risk Markers: A Randomized Controlled Trial\nSyed Akhtar Bokhari, Ayyaz A Khan, Arshad K Butt, Mohammad Azhar, Mohammad Hanif, Mateen Izhar, Dimitris N Tatakis\nThe University of Faisalabad, Dentistry\nAim: Periodontal disease elevates systemic inflammatory markers strongly associated with coronary heart disease (CHD) risk. The aim of this randomized controlled trial was to investigate the effect of non-surgical periodontal therapy on systemic C-reactive protein (CRP), fibrinogen and white blood cells in CHD patients.\nMaterials and Methods: Angiographically proven CHD patients with periodontitis (n = 317) were randomized to intervention (n = 212) or control group (n = 105). Primary outcome was reduction in serum CRP levels; secondary outcomes were reductions in fibrinogen and white blood cells. Periodontal treatment included scaling, root planing and oral hygiene instructions. Periodontal and systemic parameters were assessed at baseline and at 2-month follow-up. Intentto- treat (ITT) analysis was performed.\nResults: Study was completed by 246 subjects (intervention group = 161; control group = 85). Significant improvements in periodontal and systemic parameters were observed in intervention group. The number of subjects with CRP > 3mg/L in intervention group decreased by 38% and in control group increased by 4%. ITT analysis gave a significant (v2=4.381, p = 0.036) absolute risk reduction of 12.5%.\nConclusion: In CHD patients with periodontitis, non-surgical mechanical periodontal therapy significantly reduced systemic levels of C-reactive protein, fibrinogen and white blood cells.\nP03\nHeart Exercise and Remote Technologies (Heart): A Randomized Controlled Trial\n(1) Yannan Jiang, (2) Ralph Maddison, (3) Leila Pfaeffli, (4) Robyn Whittaker, (5) Ralph Stewart, (6) Andrew Kerr, (7) Geoffrey Kira, (8) Karen Carter\n(1)(2)(3)(4)(7)(8) National Institute for Health Innovation, The University of Auckland, Auckland, New Zealand (5) Auckland District Health Board, Auckland City Hospital, Auckland, New Zealand (6) Middlemore Hospital, Auckland, New Zealand\nCardiac rehabilitation (CR) can improve health behaviors to slow or reverse the progression of cardiovascular disease (CVD). Exercise is a central element of CR. Technologies such as mobile phones and the Internet (mHealth) offer potential to overcome the physical and geographical barriers associated with low levels of exercise participation. The HEART trial aimed to investigate the effects of a mHealth delivered exercise-based CR program to improve exercise capacity and functional outcomes at 24 weeks compared with usual CR care in New Zealand (NZ) adults with a diagnosis of CVD. A two-arm, single-blind, parallel randomized controlled trial was conducted in 2011-12 [Maddison 2011]. Participants were cardiac outpatients identified from Auckland and Middlemore hospitals, who were clinically stable, could perform exercise, owned a mobile phone, and had access to the Internet. The intervention group received a personalized automated package of text messages delivered via mobile phone, and were supported with a personalized interactive website [Pfaeffli 2012]. The control group received usual care which involves encouragement to exercise and an offer to join a local cardiac club. A total of 171 eligible patients were randomized (intervention N=85; control N=86) and followed for 24 weeks. Participants were on average 60 years old (39-79 years), and were mostly never or ex-smokers (94%), NZ Europeans (77%), males (81%), married (79%), with a self-reported medical history of high blood pressure (52%), high cholesterol (74%), heart attack (74%), angina (50%), and other forms of heart disease (28%). The primary outcome was change in maximal oxygen uptake (VO2max) (ml.kg-1.min-1) from baseline. Six minute walk test, health-related quality of life (SF-36v2TM), self-efficacy, and self-reported physical activity (IPAQ) were also assessed. Treatment evaluations were performed on the principle of intention-to-treat. Multiple imputation method was employed with missing data on the primary outcome. Full trial results will be presented and discussed.\nP04\nWhisperer: Integrated Team Approach to Declarative Study Specification\n(1) John Perkins, (2) Nicholas Best, (3) Kelly Cho\n(1) VA Cooperative Studies Program, Massachusetts Veterans Epidemiology Research and Information Center, (MAVERIC), VA Boston Healthcare System (for authors 1, 2, and 3) (2) Division of Aging, Department of Medicine, Brigham and Women’s Hospital; Harvard Medical School (for author 3)\nMassachusetts Veterans Epidemiology Research and Information Center (MAVERIC) supports data collection for clinical, observational, and biorepository studies. A declarative study specification includes all questions, element names, element types, repeating elements, and simple constraints plus specifications for calculations, complex constraints, question hiding, form layout, randomization, and patient calendar. The WHISpERer tool transforms a declarative data model into an operational data collection mechanism including data dictionaries, forms, databases, and sample data. Each integrated study team (program management, data management, and biostatisticians) collaborates to create and refine a platform independent declarative study specification where WHISpERer generated objects are exercised by the team on enterprise level collection, storage, reporting, and analysis mechanisms. MAVERIC has created extendable standardized forms relating to adverse events, contact information, demographics, drug accountability, medication history, military history, patient termination, protocol deviation, and vital signs. In addition, formalized study specifications exist for dozens of previous studies. The MeDItATE component of WHISpERer supports mining metadata from this study specification repository. WHISpERer encourages the integrated team to take an iterative stepwise approach to study specification. First begin study specification by reusing portions of existing specifications when practical. Second concentrate on specifying the data model that is shared by collection, storage, reporting, and analysis. Thirdly have one part of the team concentrate on specifying complex constraints and calculations while other team members concentrate on specifying form layout and the conditional hiding of questions and controls. This integrated team approach to declarative study specification 1) increases the quality of the data being collected, 2) insures the data model is the one desired by the entire study team prior to deployment to the production platform, 3) allows all team members to exercise the study specification based on technologies each member knows and uses, 4) and permits each study to benefit from lessons learned from previous studies.\nP05\nAccumulating Evidence and Research Organization (AERO) Model: A New Tool for Representing, Analyzing, and Planning a Translational Research Program\n(1) Spencer Hey, (2) Charles Heilig, (3) Charles Weijer\n(1) Biomedical Ethics Unit, McGill University (2) US Centers for Disease Control and Prevention (3) Rotman Institute of Philosophy, University of Western Ontario\nMaximizing efficiency in drug development is important for drug developers, policy-makers, and human subjects. Limited funds and the ethical imperative of risk minimization demand that researchers maximize the knowledge gained per patient enrolled in research. Yet, despite a common perception that the current system of drug development is beset by inefficiencies, there remain few approaches to systematically representing, analyzing, and communicating about the efficiency and coordination of the research enterprise. In this paper, we present the first steps toward developing such an approach: a graph-theoretic tool for representing the Accumulating Evidence and Research Organization (AERO Model) across a translational trajectory. We then apply this tool to a case study involving the antibacterial agent, moxifloxacin, for the treatment of tuberculosis disease.\nP06\nThe Path to a Global Clinical Trial for Acute Stroke in Japan\n(1) Haruko Yamamoto (2) Yoko Kai (3) Kanae Hirase (4) Shoichiro Sato (5) Shoji Arihiro (6) Masatoshi Koga (7) Kazunori Toyoda (8) Yuko Y. Palesch (9) Adnan I. Qureshi (10) Kazuo Minematsu\n(1)-(3) Dept. of Advanced Medical Technology Development, National Cerebral and Cardiovascular Center (4)-(7) Dept. of Cerebrovascular Medicine, National Cerebral and Cardiovascular Center (8) Dept. of Biostatistics, Medical University of South Carolina (9) Dept. of Neurology, University of Minnesota (10) Vice Director General, National Cerebral and Cardiovascular Center\nBackground: Stroke is a leading cause of morbidity and mortality and a burden on the healthcare system in Japan. Many domestic clinical trials in stroke have been conducted; however, international collaboration in investigator-initiated trials has been scarce (Asplund K, et al, 2012). In 2010, about 10 Japanese clinical sites decided to participate in the multi-national Antihypertensive Treatment of Cerebral Hemorrhage II (ATACH-II) Trial, which is funded by the National Institutes of Health.\nMethods: To overcome the language and regulation differences between the US and Japan, which are considered to be the main barriers against participation of Japanese institutions in global trials, a local coordinating center (JCC) was established at the National Cerebral and Cardiovascular Center in February 2011. The JCC acts as the liaison between the Japanese sites and the Clinical Coordinating Center (CCC) and the Statistical and Data Coordination Center (SDCC) in the US. The JCC’s work has included: 1) identifying and discussing with the CCC/SDCC the regulatory differences between US and Japan; 2) educating the site staff about the importance of adhering to the US regulations; 3) translating the main research documents and manuals; and 4) creating the ‘help-desk’ for the regulatory documents preparation. The JCC also reviews and appropriately approves the submitted regulatory documents in Japanese.\nResults: The first site in Japan was released to enroll in February 2012. The first subject was randomized within one month. As of the end of November 2012, 9 sites have been released to enroll, and 3 sites have randomized 16 subjects.\nConclusions: The Japanese sites have recruited over 15% of the total subjects in the ATACH-II to date. It attests to the successful integration of the Japanese clinical sites into an investigator-initiated global trial. Establishing the JCC has proven to be an effective way to enable and facilitate the process.\nP07\nMultiple Testing in Clinical Trials: Some New Applications\nYunzhi Lin, Kefei Zhou, Jitendra Ganju\nUniversity of Wisconsin Amgen Gilead Science\nThe traditional paradigm of determining whether or not the efficacy objective is met in Phase 2 and 3 trials is very limited: there is a single primary endpoint and a single primary method of analysis. The interpretation of results is highly inadequate when either of these ‘singles’ is deficient in capturing a non-null treatment effect. This is a serious problem given the complexity and the soaring cost of trials. This session will focus on a more reliable way to frame questions and test hypotheses so that the methods are more robust or more powerful. Applications will include group sequential trials, determination of cut-points for biomarkers and model selection for dose ranging trials. The goal of the session is to show that the proposed methods are a better way to plan Phase 2 and 3 trials than the current approach.\nP08\nA Smooth Estimate of the Hazard Ratio Over a Continuous Covariate\nTracy L Bergemann\nMedtronic Inc.\nSubgroup analysis of randomized clinical trial data is used to examine the relationship between a treatment, an independent covariate and a primary outcome of interest. The goal of such analysis is often to determine how a treatment effect is modified by another clinical covariate. Caution is warranted when subgroup analysis is performed and careful interpretation is required, particularly when the subgroup analysis is post-hoc. Usually, when the covariate is continuous, linearity is often assumed in the estimate of a treatment-by covariate interaction effect. Here we propose a varying-coefficient Cox model to obtain a non-linear estimate of the hazard ratio over the range of a continuous covariate. A penalized spline approach is used for estimation and a likelihood ratio test for inference. Confidence intervals for the hazard ratio over the continuum of the covariate are constructed with a bootstrap procedure. Estimation and inference are validated via a simulation exercise. A local fitting approach is also discussed as an alternative. The proposed method is illustrated using data from randomized clinical trials in heart failure. The analysis of data from heart failure studies shows that there are important instances where the linearity assumption is violated. Results suggest that further caution may be needed when presenting subgroup analyses that involve a continuous covariate. This is especially the case when the subgroup analysis may inform inclusion and exclusion criteria for subsequent clinical trials or restrict the guidelines by which therapy is applied.\nP09\nThe Coronis Trial: International Study of Caesarean Section Surgical Techniques – A Randomised Non-Regular Fractional, Factorial Trial\nBarbara Farrell on behalf of the CORONIS Trial Collaborative Group\nUniversity of Oxford, NPEU Clinical Trials Unit\nIntroduction: A variety of surgical techniques are used in the caesarean section operation. Many have not been rigorously evaluated in an RCT, and it is not known which techniques are associated with better outcomes for women and babies. Five specific aspects of the caesarean section procedure were examined to determine which methods lead to optimum outcomes. Women were eligible if they were undergoing their first or second caesarean section through a transverse abdominal incision. Practical challenges The CORONIS trial was a non-regular fractional, factorial RCT conducted in Argentina, Chile, Ghana, India, Kenya, Pakistan and Sudan. This is a rarely used trial design. Five comparisons were evaluated using a was via the web (with an automated back-up telephone system). The intervention pairs were: 1. Blunt versus sharp abdominal entry 2. Exteriorisation versus intra-abdominal repair of the uterus 3. Single versus double layer closure of the uterus 4. Closure versus non-closure of the peritoneum (pelvic and parietal) 5. Chromic catgut versus Polyglactin-910 for uterine repair Primary outcome: death or maternal infectious morbidity. Sample size: 15,000 women; minimum 9,000 women per intervention pair. Organisation & management Regional Trial Offices staffed by small teams managed the local data, dealt with queries, collected SAE data, distributed trial material and monitored protocol adherence. The intelligent data entry, management and monitoring systems used were developed by the International Co-ordinating Centre (ICC) in Oxford. This model allowed “real-time” data management; routine central data monitoring at the ICC was a key element of trial management. 15,935 women were randomised and 98% were followed-up at approximately six weeks post discharge. Central monitoring and regular on-site monitoring meant that recruitment targets were met, overall and by intervention pair. Adherence to the allocation by surgeons was exceptional.\nP10\nBlinding Strategies and their Effect in a Randomized Placebo-Controlled Trial (RCT) of Elastic Compression Stockings (ECS) for Prevention of Post-Thrombotic Syndrome (PTS)\nAdrielle H. Houweling, Stan Shapiro, Jacqueline M. Cohen and Susan R. Kahn\nMcGill University, Montreal, Quebec, Canada\nBackground: When successfully implemented in randomized controlled trials (RCTs), blinding can prevent ascertainment bias. While many RCTs include procedures to enhance blinding, few report on consequent results.\nMethods: The SOX Trial is a multicenter RCT of active (A) vs. placebo (P) elastic compression stockings (ECS) to prevent post-thrombotic syndrome (PTS) in 803 patients with deep vein thrombosis (DVT). Attempted blinding of patients, research coordinators (RC) and site investigators (SI) utilized several strategies: enrolling patients with a first episode of DVT (“ECS-naÔve”), shipping ECS directly to patients’ homes, and instructing patients not wear ECS on study visit days. Success of blinding was assessed at the end of follow-up by guess of treatment group. Responses were analyzed using two indices: James’ Blinding Index (BI), which assesses overall degree of disagreement between treatment allocation and guess, where BI0.2 represents unblinding.\nResults: James’ BI was 0.71 (95% CI 0.68-0.73) for patients, 0.81 (95% 0.79-0.83) for RCs, and 0.90 (95% CI 0.88-0.91) for SIs. Bang’s BI for the A-ECS and P-ECS interventions respectively were 0.38 (95% CI 0.33-0.43) and −0.15 (95% CI −0.21,−0.09) for patients, 0.22 (95% CI 0.18-0.26) and 0.00 (95% CI −0.04-0.05) for RCs, and 0.12 (95% CI 0.09-0.15) and 0.02 (95% CI −0.01-0.05) for SIs. There was no difference in the incidence of PTS between treatment groups (HRadj 1.17; 95% CI 0.75-1.81; p=0.49).\nConclusions: Results suggest that RCs and SIs were blinded to the allocated intervention. For patients, James’ BI suggests that blinding was achieved, while Bang’s BI suggests those in the A-ECS group were unblinded. As overall trial results were negative, this unblinding has minimal impact on trial conclusions. Supported by Canadian Institutes for Health Research and Sigvaris Corp.\nP11\nFactors Associated with Retention of African-American and White Participants in a Prostate Cancer Prevention Trial\n(1) John A. Hermos, (2) Kathryn B. Arnold, (3) Elise D. Cook, (4) Karen B. Anderson, (5) Jeffrey L. Probstfield\n(1) VA Cooperative Studies Program Coordinating Center, VA Boston Healthcare System (2) SWOG Statistical Center (3) MD Anderson Cancer Center, University of Texas (4) Cancer Research and Biostatistics (5) Division of Cardiology, University of Washington School of Medicine\nBackground: Adequate retention of African-American participants in long-term, randomized trials is important for achieving broadly applicable results.\nPurpose: To determine the incidence of retention failure and individual and study site factors associated with retention failure among White and African-American participants from the Selenium and Vitamin E Cancer Prevention Trial (SELECT), a phase III study of selenium and vitamin E for prevention of prostate cancer.\nMethods: SELECT randomized 35,533 participants from 427 study sites. Age eligibility was >55 years for Whites, >50 years for African-Americans; race was determined by self-report. Participants from Puerto Rico and non-African-American, non-White participants, as well as participants from sites with severe participant and data management issues, were excluded from analysis. The population analyzed was 32,440: 28,118 (87%) White and 4,322 (13%) African-American. Time to retention failure was defined as days to the earliest of either (1) the second consecutive missed in-person or phone visit or (2) refusal of future contact with study staff. Covariates included participant and study site characteristics. Cox regression was used to estimate hazard ratios.\nResults: From August 2001 to October 2008, 26.8% of African-American men and 12.9% of White men had retention failure, with rates of 62.5 and 25.7 per 1000 person-years, respectively. This difference was driven by younger African American participants, ages 50-59, comprising 60% of African-Americans with a high retention failure rate (75.0 per 1000 person years). In preliminary analyses, non-adherence to study supplements was strongly associated with retention failure in both racial groups; smoking, living alone, participation from Veterans Affairs-associated sites, and joining SELECT not to “help others in the future” were also important factors in both groups.\nConclusion: Results may be useful in identifying personal and study site characteristics to guide recruitment and enhance retention of both African-American and White men in long-term prevention trials.\nP12\nOverview Focus and Drilldown – Data Report Intelligence\nSherrie Whisler\nPennsylvania Academy of Family Physicians and Foundation\nThe Pennsylvania Academy of Family Physicians and Foundation developed a data reporting website with grant funds from the PA Department of Health (PADOH). Phase I development was fast-tracked, starting January 2012 and in production by June 2012. Practices using the site report data on Diabetes, Asthma, IVD, HTN, and Preventative quality measures and were trained and using the site by August 2012. The primary design of the project - called Data Diamond - was to develop a dynamic, highly efficient, and scalable website for data reporting. Using SQL Server 2008, Microsoft .net, and LogiXML, we quickly produce high quality data reports for funders (the PADOH) and practices. The new website houses multiple projects and limits user access based on project permissions. Data analysis through dynamic reporting functions is critical to all projects on the website. LogiXML is the reporting layer, starting at a high project-specific level and then drilling down to specifics as determined by the report requester. Reports show the data in various views, i.e. gauge charts for a quick visual comparing outcome and process measures to an overall project goal. We tweak the concept into a line chart for disease-level analysis. Next level down is practice level and deeper still is the individual measure level analysis. Views for data analysis include gauges, charts, spark lines, and target deltas. All of this data is produced in one interactive report so users can click and drill down where they choose based on their needs. Reports are both flexible and secure, designed based on the user, their roles and permissions. Users only see what is applicable to them and their projects even if they are associated with more than one organization or more than one project. The system manages all roles and provides appropriate views.\nP13\nImplementing a Central Monitoring Process\nGary R Gensler, Zorayr Manukyan, Noga Lewin, Gaurav Sharma, Anne S Lindblad\nAll affiliated with The EMMES Corporation except Zorayr Manukyan (Biogen Idec)\nIn August 2011, the FDA released a draft guidance encouraging sponsors to develop “risk-based approaches to monitoring clinical investigations of human drug and biological products, medical devices and combinations thereof.” Prior to this date, the FDA Office of Scientific Investigations (OSI) contracted the EMMES Corporation to develop risk-based approaches in an attempt to determine which of four de-identified NDAs were flagged by OSI for having sites with significant good clinical practice (GCP) violations. The methods noted below helped EMMES to successfully identify the 2 NDAs with GCP violations and to further identify nearly all flagged study sites within the two NDAs. These methods included reviewing a) enrollment patterns, b) randomization sequences and treatment balance, c) timing of scheduled study visits relative to anticipated date of occurrence, d) parallel coordinate plots that display within-subject means, variability, Euclidian distance and repetitiveness over time for a vector of common measurements, e) distribution plots, including a plot of each significant digit, f) measures of adverse experience severity and g) Chernoff faces that can combine multiple measurements into pictorial display of site variability. Figures in each domain showed all sites on the same plot for easy comparison. Following the work with the FDA, these methods were applied on a pilot basis to three EMMES projects by a central monitoring team. Each project selected variables of interest and corresponding reports were created. By reviewing the report created by the team, each project identified issues not previously detectable by typical quality-control methods. Efforts are now being made to automate this process so that any project may create on-demand comprehensive reports of side-by-side site-specific clinical data in the domains noted above.\nP14\nA Weibull Model for the Correlation Between Progression-Free Survival and Overall Survival\n(1) Yimei Li, (2) Qiang Zhang\n(1) Division of Oncology, The Children’s Hospital of Philadelphia & 2University of Pennsylvania Perelman School of Medicine, Philadelphia, Pennsylvania, USA (2) Radiation Therapy Oncology Group, American College of Radiology, Philadelphia, Pennsylvania, USA\nIn cancer clinical trials, overall survival (OS), time to progression (TTP) and progression free survival (PFS) are three commonly used endpoints. Empirical correlations between OS and PFS have been investigated and reported for different cancers, but statistical models describing the dependence structures are limited. Recently Fleischer et al. (Statistics in Medicine, 28:2669-2686, 2009) proposed a statistical correlation model based on Exponential distributions. This approach is mathematically tractable and shows some flexibility in describing the dependencies between PFS and OS. However, we usually observe hazard rates that deviate from the Exponential distribution assumption. In this research, we aim to extend their model to account for the non-constant hazard rates using Weibull distributions. We derived the correlations among different survival outcomes, as well as the distribution of overall survival as induced by the model. Model parameters were estimated using the maximum-likelihood method and the goodness of fit was assessed by comparing the predicted vs. observed overall survival curves. Results are also compared between the two correlation models. Simulations suggested that the Weibull correlation model provides better fit than the Exponential model, for data generated from a range of distributions. We also applied the proposed method to data from two cancer clinical trials. In the non-small-cell lung cancer trial, both the Exponential and Weibull correlation model provide a good fit to the data and the estimated correlations were very similar under each approach. In the prostate cancer trial, the Weibull correlation model showed advantage over the Exponential model with larger estimated correlations. Realistic description of the correlation between PFS and OS from phase II studies is very helpful for planning the phase III trial with OS as the primary endpoint.\nP15\nAn Assessment of Agreement Between Self-Reported and Clinic-Reported Data Collection in a Type 1 Diabetes Clinic Registry\n(1) Kellee M. Miller, (2) Dongyuan Xing, (3) Stephanie N. Dubose, (4) Roy W. Beck for the T1D Exchange Clinic Network\n(1, 2, 3, 4) Jaeb Center for Health Research\nThe T1D Exchange clinic registry consists of 25,833 adults and children with type 1 diabetes (T1D) enrolled at 67 endocrinology clinics through the US. Clinical data are obtained through (1) completion of a questionnaire by the participant/parent of participant (pt) and (2) retrieval of information collected from the medical record (MR). The objective of this analysis is to assess the agreement on the following data elements collected from both the pt and MR: occurrence of diabetic ketoacidosis (DKA) and severe hypoglycemia (SH), frequency of self-monitoring of blood glucose (SMBG), continuous glucose monitor use (CGM), insulin pump use, and total daily insulin (TDI) for insulin pump users. Kappa statistics were reported for categorical variables and Spearman correlation was used for continuous variables. The agreement between self-reported and clinic-reported data ranged from low to moderate for all measures but pump use (Table 1). MRs were much less likely to contain a record of SH and DKA events in the past 12 months. Among 452 pts reporting 3 SH events in the past year, MR review failed to identify any events for 369 pts (82%). Similar trends were observed across all ages. There are substantial discrepancies between clinic- and self-reported data. It is important to consider these discrepancies and possible reasons for poor agreement in data analysis.\nP16\nEvaluating Potential Impact of Modified Eligibility Criteria on Enrollment and Study Results in Cardiovascular Clinical Trials\n(1) Jae Eun Lee, (2) Junghye Sung, (3) Jiyoung Lee\n(1) RCMI Translational Research Network Data Coordinating Center @Jackson State University, Mississippi, USA (2) Biostatistics and Epidemiology @ Jackson State University, Jackson, Mississippi, USA (3) University of Pittsburgh, Pittsburgh, PA, USA\nPurpose: When recruitment delayed, modifying the eligible criteria may be one of the steps to be considered. However, little is known about guideline to evaluate the potential effect of modified eligibility criteria on enrollment and study goal. This study is to propose an empirical practice to proactively examine how much enrollment can be increased through revising eligibility criteria and how the revision impacts the trial results.\nMethods: Key eligibility criteria were obtained from 48 cardiovascular clinical trials registered in ClinicalTRials.gov website as of October 11, 2012: age, low serum vitamin D levels, overweight, well controlled blood pressure, and non-diabetes patients. Potential eligible enrollees were estimated by using the National Health and Nutrition Examination Survey data 2005-6. Its impact on study results was investigated to determine the difference of cardiovascular markers between samples meeting original criteria and those newly-identified by criteria modification by using multiple regression models (SURVEYREG for binary outcome and SURVEYREG for continuous outcome) after controlling age, gender and race.\nResults: Our analysis revealed that while decreasing BMI lower limit increased more younger samples, increasing hypertension upper limit increased more female, white, and older samples. Added samples by modifying BMI were better in HDL-Cholesterol, insulin, LDL-cholesterol, Triglyceride, Apolipoprotein, C-reactive protein, parathyroid Hormone, Folate, CVD, and Metabolic syndrome. Meanwhile, added samples by modifying hypertension criterion were more likely to have worse makers in insulin, fasting glucose, triglyceride, Apolipoprotein, Folate, and metabolic syndrome.\nConclusion: Our analyses suggested that modifying the eligible criteria increased potential enrollment, but differentially by select criteria and greater in a specific demographic subgroup, which may cause potential selection bias. This selection bias may misguide the results of the clinical trials. Therefore, change of the eligibility criteria should be carefully done after evaluating its impact on the study goal by using the available data (i.e., open source data).\nP17\nUse of Propensity Scores to Combine Data from Two Similar Clinical Trials in the Urinary Incontinence Treatment Network\nHeather Litman, Stephen Kraus, Gary Lemack, Toby Chai, Larry Sirls, Keith Lloyd, Peggy Norton, Elizabeth Mueller, Shawn Menefee, Jonathan Shepherd and John Kusek for the Urinary Incontinence Treatment Network\nNew England Research Institutes, University of Texas Health Science Center, University of Texas Southwestern, University of Maryland, William Beaumont Hospital, University of Alabama, University of Utah School of Medicine, Loyola Medical Center, University of California San Diego Medical Center/Kaiser Permanente, University of Pittsburgh, National Institute of Diabetes and Digestive and Kidney Diseases\nWhile propensity scores have long been used in observational studies, they are increasingly being utilized to aid analysis of clinical trials. In the Urinary Incontinence Treatment Network (UITN), two randomized clinical trials, the Stress Incontinence Surgical Treatment Efficacy Trial, SISTEr, and the Trial of Midurethral Slings, TOMUS, were conducted on similar yet distinct populations undergoing surgery for stress urinary incontinence. Pre and post-surgery urodynamic (UDS) changes were compared between those randomized to receive an autologous fascia pubovaginal sling in the SISTEr trial to patients who received a midurethral sling in the TOMUS trial. A propensity score analysis was used to help control for bias between the samples by using multiple logistic regression analysis to compute the probability of being enrolled in the SISTEr trial conditional on baseline covariates. The Hosmer-Lemeshow goodness of fit statistic indicated reasonable fit (p = 0.74). All propensity-adjusted associations of baseline covariates between studies became statistically non-significant indicating that the propensity score removed bias between the samples for the measures considered. To compare UDS changes between the trials, analysis of covariance was used for the mean difference in continuous parameters and chi-square analysis was used for parameters measured categorically with models including propensity stratum. These models confirmed that after accounting for differing factors between the two study populations using propensity scores, differences between pre and post-surgery UDS measures remained and were similar in both trials. This analysis provides an example of how and why propensity scores can be used in randomized clinical trials.\nP18\nAssessing the Actual Treatment Benefit with Non-Adherence to Study Drug in a Large Randomized Trial\n(1)Yumi Kubo, (2) Lulu Sterling\n(1)Amgen Inc., (2)Amgen Inc.\nIntro: The intention-to-treat analysis is the gold standard in assessing the effectiveness of a study drug in randomized clinical trials. However, when non-adherence to study drug is substantial, the actual treatment effect may be underestimated. The EVOLVE trial (Evaluation of Cinacalcet Therapy to Lower Cardiovascular Events), the largest randomized trial ever conducted in the dialysis population, was marked by a large proportion of patients withdrawing from randomized treatment. Various statistical methods were used to assess the impact of non-adherence on the estimated treatment effect.\nMethods and Results: The EVOLVE trial (N=3883) was an event-driven trial designed to assess the benefit of cinacalcet compared to placebo on a composite endpoint consisting of all-cause mortality and cardiovascular events. During the 5.5 years of the trial, a large proportion of patients withdrew from treatment but not to follow up in both groups (67% cinacalcet and 70% placebo). A proportion of patients also started taking commercially available cinacalcet (11% cinacalcet and 23% placebo). The median time on treatment (17.5 mo. cinacalcet and 21.2 mo. placebo) was approximately half of the total time patients were in follow-up for endpoints. The hazard ratio (95% CI) for the primary composite endpoint using the ITT analysis was 0.93 (0.85, 1.02). Using lag censoring, where data are censored 6 months after stopping study drug, the HR (95% CI) was 0.85 (0.76, 0.95). Using the inverse probability censoring weight (IPCW) method, the HR (95% CI) was 0.77 (0.66, 0.88). Accelerated failure time models including iterative parameter estimation were also used [HR (95% CI): 0.87 (0.75, 1.01)].\nConclusion: The best method to adjust for non-adherence remains debatable as they each have their limitations. The sensitivity analyses performed, while not providing definitive evidence that cinacalcet is more effective, suggest that the effect size is larger than that estimated by the ITT analysis.\nP19\nNo Collaboration, No Trial: Why Collaborator Opinion Matters\nClaire Cocharan on behalf of the EAGLE Study Group\nUniversity of Aberdeen\nEAGLE (Effectiveness, in Angle closure Glaucoma, of Lens Extraction) is an international multi-centre, pragmatic, publicly funded randomised controlled trial (RCT). EAGLE addresses whether removal of the lens of the eye for newly diagnosed Primary Angle Closure Glaucoma results in better clinical, economic and patient focussed outcomes compared with standard management. During recruitment 419 participants from 31 centres worldwide were randomised. Clinical and participant-reported outcomes are collected at the recruiting centres for three years following randomisation. Trial data from participant questionnaires and clinical case report forms are uploaded onto the bespoke EAGLE website by collaborators until the end of follow-up (December 2014). Communication and training are key components of managing any trial successfully. Within the UK training of the site collaborators is mainly undertaken via group study days. The aim of this study was to elicit opinions from EAGLE site collaborators to inform future pragmatic multi-centre trials: two focus groups involving eight and nine collaborators respectively were conducted during follow up training; Group training days, rather than individual initiation visits, were preferred as they provide an opportunity to network with peers, take part in practical tutorials and focus on the trial away from the distraction of the everyday environment. Collaborator newsletters remain popular for maintaining trial profile at a group level but at individual level distributing tasks by regular brief emails encourages engagement. Web based data entry gives collaborators a sense of ownership and completion over the data they collect, but thoughtful website design and (telephone) support is key to maintaining data quality. Collaborators like trials that mimic standard clinical care; they are perceived easier to recruit and retain patients within. Barriers included complexity of the inclusion and exclusion criteria, and issues with local financial support. Findings from this study will inform management practice of any future trial requiring collaborator cooperation.\nP20\nRecommendations for Future Patient Focussed Trials Addressing Foot Pain in Systemic Sclerosis\n(1*) Joint first authorship, Howard Collier, (1**) Lorraine Loughrey\n(1*) Clinical Trials Research Unit, University of Leeds (1**) Division of Rheumatology and Musculoskeletal Disease, University of Leeds\nIntroduction: A patient initiated Randomised Control Trial was undertaken to evaluate the effectiveness of a simple cushioning and thermal insole in reducing foot pain in patients with systemic sclerosis (SSc). A total of 560 patients were screened across four specialist sites and 141 patients were recruited to target. Three trial contacts were required over 12 weeks, with a total of 11 patient reported assessments completed per participant. Independent patient representatives (with SSc) participated in the trial meetings to inform trial management and design. Findings Compliance and completion of patient reported data was excellent; only 11 participants did not complete follow-up. Both the intervention and a sham device yielded a reduction in pain score over 12 weeks but did not meet the pre-specified clinically significant difference. Adjusting for seasonal effect showed only a minimal and insignificant difference in pain between warm and cold months, a factor suggested by the patient representatives. Patient diary comments highlighted severe effects of foot pain on the participant’s psychological status. Much gratitude and support was shown for the trial throughout. Recommendations Patient and public involvement (PPI) is integral for trial success especially in severe systemic diseases and should inform the design from the outset. To yield good recruitment rates, data collection and compliance, numbers of study visits should be minimised with flexibility and support. The nature and source of foot pain in SSc and the interaction with footwear choice requires further evaluation. Future intervention studies should be improved by reporting the nature and source of pain and should reflect the patient need throughout all seasons. An additional non-treatment arm should be incorporated in future trials to explore any potential positive properties of the sham device and any gratitude effect. Consideration of psychological support for participants is recommended when exploring new areas of chronic disease.\nP21\nBayesian Multiple Imputation for Missing Multivariate Longitudinal Data from a Parkinson’s Disease Clinical Trial\nSheng Luo, Andrew B. Lawson, Bo He, Jordan J. Elm, Barbara C. Tilley\nDivision of Biostatistics, University of Texas at Houston\nIn Parkinson’s Disease (PD) clinical trials, PD is studied using multiple outcomes of various types (e.g., binary, ordinal, continuous) collected repeatedly over time. The overall treatment effects across all outcomes can be evaluated based on a global test statistic. However, missing data occur in outcomes for many reasons, e.g., dropout, death, etc., and need to be imputed in order to conduct an intent-to-treat analysis. We propose a Bayesian method based on item response theory to perform multiple imputation while accounting for multiple sources of correlation. Sensitivity analysis is performed under various scenarios. Our simulation results indicate that the proposed method outperforms standard methods such as last observation carried forward and separate random effects model for each outcome. Our method is motivated by and applied to a PD clinical trial. The proposed method can be broadly applied to longitudinal studies with multiple outcomes subject to missingness.\nP22\nRandomization Metrics: Jointly Assessing Predictability and Efficiency Loss in Randomization Designs\nDennis Sweitzer\nMedidata Solutions Worldwide\nRandomization methods generally are designed to be both unpredictable and balanced between treatment allocations overall and within strata. However, when planning studies, little consideration is given to measuring these characteristics, nor are they examined jointly, and published comparisons between methods often use incompatible metrics and simulation assumptions. Furthermore, for purposes of real-world planning, such simulations often make unrealistic assumptions (e.g., equal sized strata), and summary statistics give limited information. In order to better reflect real-world study performance, we carried out a series of simulations with 2 treatment arms, and stratification factors that are unequally populated (e.g., 1:2, 1:2:3, or a power law distribution). To measure predictability, we modified the potential selection bias (Efron, Blackwell-Hodges) in which an observer guesses the next treatment to be the one that previously occurred least in the strata containing the subject (i.e., limiting the observer’s knowledge to individual strata, such as site). This reflects a game theory model of randomization pitting observers versus statistician, and is easy to calculate and interpret. To measure imbalances, we calculated efficiency loss using Atkinson’s method because: The main impact of imbalances on the outcomes of a study is a loss of statistical power; Even if treatments are balanced overall, imbalances within small strata can have a disproportionate impact on efficiency; And it is easy to interpret as lost sample size. We applied these methods to evaluate the performance of several popular and novel randomization methods for a variety of parameters, including methods based on permuted blocks, dynamic allocation/minimization, and urn designs. Simulation results were summarized with the median and confidence intervals to estimate best & worst-case scenarios as well as expected performance. Results showed consistent trade-offs between efficiency and unpredictability over methods and parameters, supporting no ‘best’ method to optimize both.\nP23\nMeta-Analysis of One Outcome from Group Sequential Trials with Composite Outcomes: Are Standard Methods Appropriate?\n(1) Abigail Shoben\n(1) The Ohio State University\nComposite outcomes, in which multiple events are combined into one outcome, are the primary outcome in many clinical trials. Composite outcomes are convenient because they provide a way to collapse over competing risks and provide simple interpretation to physicians and others evaluating effectiveness. However, composite outcomes may not provide clarity if treatment effects differ between the outcomes being combined and attempts to separate the effects post-hoc either in a single trial or in combined meta-analyses may be problematic. Of additional concern, the composite outcome, or one or more single outcomes may be sequentially monitored for safety or efficacy. Differential monitoring of one or more outcomes further complicates interpretations and post-hoc meta-analyses. This work is motivated by recent meta-analyses of data from randomized trials of magnesium sulfate on neurological deficits and death in preterm infants. This paper illustrates the potential problems resulting from separation of the composite outcome into single outcomes in meta-analyses and provides guidance for future studies.\nP24\nMultiple Pharmacokinetic Assessments of Five Unique Formulations of Ezogabine/Retigabine Compared with the Immediate Release Formulation\nChristopher Crean, Mauro Buraglio, Debra Tompson, James Storey, Thangam Arumugham\nValeant Pharmaceuticals\nThis study was designed to select an ezogabine/retigabine modified release (MR) formulation. The biopharmaceutical comparisons of interest were the bioavailability of the ezogabine MR formulation relative to the immediate release (IR) formulation, and the effect of a high-fat meal versus a standard meal. As previous studies have shown differences in relative bioavailability between single- and repeat-dose administration, this study was conducted at steady state which more closely reflects clinical practice. The study design also addressed the challenges of titration to, and maintenance of, steady state at clinically relevant doses, with minimal dropouts. Study RTG114552 (NCT01332513) was a 35-day study in 36 healthy volunteers (male or female) in which MR formulations were evaluated using steady state twice- (BID) and once-daily (QD) dosing. Subjects received 100 mg ezogabine IR three times daily (TID) on Days 1-3, increasing to 150 mg TID on Days 4-6 (Titration Phase). On Day 7, subjects were randomized into a 6-way crossover (Bioavailability Phase) to receive each of the five ezogabine MR prototypes (300 mg BID) and ezogabine IR (200 mg TID), each dosed for 4 days, with co-administration with a standard meal. On Day 31, subjects were re-randomized to the MR formulations (600 mg QD) for 4 days with co-administration of a high-fat meal on Day 4 (Food Effect Phase). The most common adverse events during the bioavailability phase were CNS-related (dizziness, somnolence, euphoric mood, headache, disturbances in attention). The relative bioavailability based on AUC0-24 for the MR formulations compared with the IR formulation ranged from 0.83 to 0.94. All formulations showed either a small or no impact of a high-fat meal on ezogabine pharmacokinetics. This study design informed MR formulation selection based on key biopharmaceutical criteria.\nP25\nDeveloping Approaches for Randomization within the Mini-Sentinel Distributed Database\n(1) Roberta Constantine, (2) Pamela Tenaerts, (3) Robert Califf, (4) Richard Platt\n(1) Harvard Pilgrim Health Care Institute, (2) Clinical Trials Transformation Initiative, (3) Duke University Medical Center and Duke Translational Medicine Institute, (4) Harvard Medical School and Harvard Pilgrim Health Care Institute\nMini-Sentinel (M-S) is a pilot project sponsored by the U.S. Food and Drug Administration (FDA) to create an active surveillance system to monitor the safety of FDA-regulated medical products using routinely collected electronic health data from over 100 million people. M-S uses pre-existing electronic healthcare data from multiple sources, employing a distributed data approach in which Data Partners maintain physical and operational control over electronic data in their existing environments. Presently, the M-S only supports observational studies. In concept, the M-S system could identify potential candidates for clinical trials and it could also identify selected clinical trial outcomes. We are assessing the feasibility and develop approaches to using the M-S distributed dataset for these purposes. This activity is a collaboration between M-S and the FDA Clinical Trials Transformation Initiative (CTTI). Investigators are examining the feasibility of using the M-S infrastructure to facilitate patient recruitment including working with the patients’ clinicians to enroll appropriate candidates and follow-up of participants in randomized trials. We are also exploring the ability to ascertain selected outcomes and through a combination of routinely collected electronic health data and medical record review. We are addressing policy and implementation issues concerning patient privacy and confidentiality, and approaches to obtaining informed consent. Investigators will complete a white paper summarizing 1) approaches to randomization, 2) approaches to obtaining informed consent, 3) the implications of use of M-S data from to conduct research (as opposed to public health surveillance), and 4) the appropriate oversight of such research.\nP26\nAlgorithms for Inference Following Bernoulli Random Walks with Desirable Absorbing Boundaries\n(1) Neal Oden, (2) Gaurav Sharma\n(1 & 2) The EMMES Corporation\nSequential probability ratio tests (SPRTs) and Simon designs are often used to implement phase II clinical trials. At trial’s end, researchers may wish to estimate the parameters underlying the trial design, but this cannot be correctly done without accounting for the design itself. Girshick et al (1946) discussed unbiased estimation of the parameter of a random Bernoulli walk with a simple boundary. Continuing this theme, we review ways to calculate uniformly minimum variance unbiased estimators (UMVUE’s), p-values, expected run lengths, and confidence intervals for Bernoulli walks with finite closed attainable simple boundaries, which we refer to as “desirable”. Single- and multiple-stage Simon trials and truncated binomial sequential probability ratio tests (SPRTs) are desirable walks, as are the k-stage trial of Jennison & Turnbull (1999), curtailed sampling, and a variety other un-named designs. We introduce a simple new method to tell whether a boundary is desirable, and develop a way to name, generate, and count all desirable designs of a given size. We provide several intuitive descriptions of these designs and give explicit computer algorithms for implementing them and determining their characteristics. For example, these algorithms allow truncation of a binomial SPRT at a considerably earlier time than Wald’s (1973) classical method without adversely affecting power and size. Finally, we compare by exact calculation the properties of 5 estimators of the Bernoulli parameter across a range of truncated SPRT’s. The maximum likelihood estimator (MLE), which Chang et al. (1989) found to often have absolute bias less than 0.025 in k-stage Simon designs, does not function as well with truncated SPRTs, where the UMVUE is clearly better. But if root mean squared error is more important than bias, a Bayes estimator, the MLE, and the median unbiased estimator may prove superior to UMVUE and Whitehead estimators.\nP27\nTricks of the Trade: Using Sasæ to Analyze Irregularly Scheduled Clinical Data\nStacey Slone, John Rinehart\nMarkey Cancer Center, University of Kentucky Dept. of Internal Medicine, University of Kentucky\nMany cancer chemotherapy treatments depend on laboratory values, such as creatinine, to determine dosing levels and schedule. In clinical trials, a patient may have multiple blood draws over a period of days to assess if he/she is capable of receiving chemotherapy and to establish the dosing level. These irregularly scheduled laboratory visits can be tricky to match to a particular chemotherapy cycle since the therapy may not be initiated on the exact day of the blood draw. Although the laboratory and chemotherapy dosing datasets both include the patient identifiers and the respective dates, the dates may not be exact or listed with same cycle code. Hence, merging the datasets becomes a challenge. SASÆ PROC SQL can be used to merge datasets in such situations when the matching variables in the datasets of interest do not have exact values but must be matched within a range. Using a real world example of dosing schedule and laboratory values from a lung cancer trial, I will demonstrate how various SAS functions in conjunction with PROC SQL can be utilized to create a clean analysis dataset to assess dose density. Issues faced include laboratory visits not completed, doses entered as free text and converting doses entered as milligrams back to the AUC dosing required by the protocol.\nP28\nEvolving Best Practices to Meet the Demands of Revolutionizing Research at Coordinating Centers\n(1) L. Suzanne Firrell, (2) Haema Nilakanta, (3) Mary A. Foulkes\n(1) The Biostatistics Center, The George Washington University (2) The Biostatistics Center, The George Washington University (3) The Biostatistics Center, The George Washington University\nThe protocol for a research study provides the clinical or scientific framework for recruitment, intervention/observation, assessment, data collection and analysis. A myriad of behind the scenes activities make it possible for a study to be implemented successfully. Most importantly, coordinating centers need to develop a framework to effectively manage these activities, and evaluate and efficiently implement new technology enabling them to deliver the required data and analyses. Technology is rapidly changing the tools for coordinating and managing project infrastructures. Coordinating centers have the experience to evaluate the operational value of these emerging tools, and need to have in place a flexible staff willing and able to develop new skills. These skills and tasks include: establishing effective communications and high-level collaborative relationships with each center; tracking multiple informed consents/renewals; writing scopes of work, requesting bids and selecting vendors and hotels; scheduling and supporting different types of virtual meetings; training a webmaster for a study website; and assuring completion of valid data. To accomplish the above, the recruiting, hiring and training of study staff to have a high level of quality communication skills, project planning and management proficiency is essential. Typically these tasks are not included in a protocol, nor considered a focus in the job description of the staff. In this presentation, we examine these developing demands, how research staff can acquire the skills needed, and look at current best practices and training methods in both assuring that these methods are addressed and ways to ensure continuity and ingenuity for the future.\nP29\nCommon Mistakes to Avoid in Clinical Trial Case Report form Design\nKaren, Briggs\nMedical University of South Carolina, Statistical and Data Coordination Center\nThe validity and reliability of the results and interpretations of clinical trials depends heavily on the quality of data collected by Case Report Forms (CRFs). The negative impact of common mistakes in CRF design upon study operation and clinical data quality, have been long recognized. Some problems include: discrepancies between the database work flow and study protocol; data collection schedules lacking coverage for all possible contingency scenarios; CRF items with improper data type specifications; multiple choices without mutually exclusive and exhaustive options; premature coding categorizations with uncontrollable subsequent code additions and edits; data items with arbitrary interpretations; data items difficult to verify with source documentation; data items based upon unrealistic expectations of site personnel; and real-time data validation, rule violation flags introducing the risk of biases. Other mistakes may reduce the efficiency of CRF data collection and procession, such as: CRFs with too many or too few items; CRFs associated with multiple data collection activities across a wide range of time; and an excess of CRF information not directly answering the study questions. These mistakes often go unrecognized until data and operational quality concerns arise in the middle of the trial. This often requires substantial database changes, retrospective data collection, CRF recalls, missing data items, and is likely to adversely affect the data quality and the credibility of trial results. This presentation will cover the lessons learned at the Statistical and Data Coordination Center while working on four multicenter trials in the Neurological Emergencies Treatment Trials (NETT) Network. Strategies and procedures aimed to avoid these mistakes will be shared.\nP30\nQuality Control (QC) of Field-Based Pulmonary Function Testing (PFT) in the Gulf Long-Term Follow-Up Study (Gulf Study)\n(1) Steven K. Ramsey,(1) Edward E. Gaunt, (1) Brian D. Blackmon,(1) Matthew D. Curry, (1) Christopher K. Treseder, (1) David A. Johndrow, (2) Richard K. Kwok, (2,3)Larry S. Engel, (2) Dale P. Sandler\n(1) SRA International, Inc. (2) National Institute of Environmental Health Sciences; (3) University of North Carolina, Chapel Hill, NC, USA\nPFT results are primary outcomes in most studies of pulmonary disease. The literature on PFT QC is largely focused on multi-center, clinic-based studies in which a PFT expert over-reads all tests and provides feedback directly to examiners working under the direction of local investigators. This presentation describes our experience implementing QC activities for field-based PFT measurements in the GuLF STUDY, a large-scale epidemiological study of the potential health effects of exposure to oil and dispersants among clean-up workers who responded to the 2010 Deepwater Horizon oil spill. More than 31,000 participants have enrolled to date, and a subgroup of 8,600 have completed an in-home examination that included PFT. Exams have been conducted by 57 home-based certified medical assistants (CMAs) across five Gulf states. All CMAs received web-based and in-person PFT training and completed practice and certification activities. During training, CMAs are provided with job aides that include a quick reference guide for calibrating and operating the spirometer, standardized scripts and a video to help introduce participants to the maneuver, and a handout demonstrating common PFT problems and solutions. Tests are conducted with standardized spirometers with built-in QC software, which provides real-time quality feedback and recommendations for improving poor quality maneuvers. Field managers periodically attend visits to observe coaching and testing techniques and provide feedback. Finally, CMAs receive weekly reports from field managers about the quality of their PFT scores. Individual and group re-trainings are provided as needed. In large-scale studies with multiple outcomes of interest that are carried out in home-based settings, an approach that relies more heavily on automated software may be an acceptable alternative to expert over-reading, though quality may not be as high as in tightly controlled clinical settings.\nP31\nA Versatile Test for Comparing Survival Curves Based on Weighted Log-Rank Statistics\nTheodore Karrison\nUniversity of Chicago\nThe log-rank (LR) test is perhaps the most commonly used, nonparametric procedure for comparing two survival curves. The LR test is known to yield maximum power under proportional hazards alternatives. Several authors have described more versatile tests using combinations of weighted log-rank statistics that are more sensitive to non-proportional hazards alternatives. Fleming and Harrington (Wiley, 1991) considered the family of G(?) statistics and their supremum versions, while Lee (Biometrics, 1996) and Lee (Computational Statistics and Data Analysis, 2007) proposed tests based on the more extended G(?, y) family. In this presentation we consider Zm=max(|Z1|, |Z2|, |Z3|), where Z1, Z2, and Z3 are z-statistics obtained from G(0,0), G(1,0), and G(0,1) tests, respectively. G(0,0) corresponds to the LR test while G(1,0) and G(0,1) are sensitive to early and late difference alternatives. Z=(Z1, Z2, Z3) has an asymptotic, trivariate normal distribution with covariance terms that can readily be estimated. The p-value for Zm can therefore be obtained by integrating under the trivariate normal density. An implementation in STATA (College Station, TX) is presented. A simulation study was conducted to compare the performance of the versatile method based on Zm to the LR test and to the “optimally” weighted test under the null hypothesis, proportional hazards, early difference, and late difference alternatives. Results indicate that the method maintains the type I error rate, provides increased power relative to the LR test under early difference and late difference alternatives, and is associated with no more than a 4%-5% power loss relative to the optimally chosen test. The procedure is applied to two real datasets: the Gastrointestinal Tumor Study Group data analyzed in Stablein, Carter, and Novak (Cont Clin Trials, 1981), and the Northern California Oncology Group head-and-neck cancer trial (Efron; JASA, 1988), both of which exhibit a certain degree of non-proportional hazard rates.\nP32\nEffectiveness of a Mid-Trial Training Session for Monitoring Serious Adverse Events: Experience in Action for Health in Diabetes (Look Ahead) Study\nSarah A. Gaussoin, Judy Bahnson, Alain Bertoni, Fred Brancati, Lawton Cooper, Jeff Curtis, Siran Ghazarian, Leslie Gregg, Steven Kahn, BJ Maschak-Carey, Anne Murillo, Anne Peters, F. Xavier Pi-Sunyer\nWake Forest Health Sciences\nMonitoring participant safety in clinical trials is often done via collecting serious adverse events (SAEs). While reporting guidelines exist for drug intervention studies through the FDA, there is limited literature related to behavioral interventions. Data regarding the training of clinical staff on SAEs reporting are also rare. Look AHEAD, a multi-center, randomized clinical trial of 5,145 overweight or obese participants with Type 2 diabetes evaluating the long-term health outcomes of an intensive lifestyle intervention program compared to diabetes support and education, attempted to bridge these gaps. At the start of the study, central training was provided in all aspects of the study protocol, include SAE reporting. During the trial, each SAE report was centrally reviewed, and it was noted that there appeared to be an increasing lack of consistency over time in interpretations of adverse events among sites. To address this, staff responsible for reporting adverse events were given a written test designed to detect areas regarding identifying and reporting SAEs where additional training may be needed. A one-time training was conducted five years into the study to reinforce common SAE terms and their definitions and review SAE reports and narratives that contained errors. After the training, the test was repeated on average 12 days after the first test. The percentage of significant errors decreased 16% after the training to a level of 0.5% and the scores on the test increased 10%. In longer studies, mid-trial SAE testing and training can be important to maintain high levels of quality control.\nP33\nThe Importance of Innovative Data Management Practices for Clinical Registries\n(1) Jason Stock, (2) Daniel Jeffers, (3) Jennifer Nobbe, (4) Eileen King\nCincinnati Children’s Hospital Medical Center\nThe importance of registries for clinical research has been increasing in recent years. Registries allow a look at real world situations that cannot be recreated in randomized clinical trials. Data generated from registries can be used for comparative effectiveness research, safety monitoring and/or quality improvement efforts. Many quality improvement registries lack the data management infrastructure common to clinical trials. Without a monitor on site to assist users with data entry training and data quality issues, the role of the data manager for these projects is greatly increased. With this realization, innovative data management techniques are necessary to successfully ensure the level of data quality for these registries meets the needs of its customers. Each registry presents different challenges to incorporating data quality procedures. Since these registries have a large scope with fewer resources than traditional RCTs, we have found it is not always feasible to institute a data querying process that requires sites to address all potential data entry errors. One registry attempted a full data querying process after 2 years of data collection had already been completed. When queries were issued, sites were overwhelmed and the response rate was much lower than expected. As a result, a key variable monitoring program is being developed instead. Other projects use front end logic checks on data collection forms as a means of ensuring quality. One project has instituted a process monitoring program for data quality similar to that used in the engineering industry to detect changes in data quality processes over time. With the help of innovative data management practices, we have been able to increase the level of data quality in registries without the level of resources typically employed for clinical trials.\nP34\nDevelopment of an Algorithm and a Web-Based Software Module for Single and Multi-Stage Patient Enrollment Processes for Web Randomization\nJohn E. Podoba\nDacima Software Inc.\nRandomizing patients through a web Electronic Data Capture (EDC) platform allows patients to be randomly allocated to treatment groups in real-time using a web browser. The web randomization process can include an enrollment web form which includes questions (data entry fields) to assess the eligibility or inclusion/exclusion criteria of the patients. Incorporation of validation syntax in the enrollment form allows the web randomization system to verify the eligibility criteria of the patients, classifying them as either eligible or ineligible and then randomize those patients who are eligible for the trial. Some trial designs require patients to be randomized at the time of patient enrollment, while in other trial designs the patient is not randomized at the time of initial enrollment but rather at a later time point, such as after completion of laboratory test or other patient assessments. In order to facilitate different trial enrollment designs, Dacima Software Inc. developed an enrollment/randomization algorithm and module for its web EDC software platform (Dacima Clinical) that allows the trial enrollment and randomization processes to be configured and implemented without the need for computer programming. Through an easy to use graphical user interface the system allows single-stage enrollment or multi-stage enrollment processes to be setup and configured. The multi-stage enrollment algorithm allows for the configuration of hierarchical enrollment form structures which verify eligibility criteria and track patient enrollment status at each stage of the enrollment process. Patients who meet the eligibility criteria at each stage in the sequential enrollment process are randomized after completion of the last enrollment form. The configuration of status levels at each stage in the enrollment process facilitates the management, tracking and monitoring of patients as they precede through the different enrollment stages. The algorithm and software provides a powerful and flexible tool for patient enrollment and randomization.\nP35\nKey Data Editing Features of the BSC Data Management System for Coordinating Centers in Multi-Center Clinical Trials\nSam Bergman, Zenobia Liendo, Pam Mangat, Susan Reamer, Alla Sapozhnikova\nThe George Washington University Biostatistics Center\nTimely cleaning of data is a critical process to ensure internal validity of a clinical trial. The George Washington University Biostatisics Center (BSC) has implemented a complex data management system that integrates validation during data entry with complex cross-form discrepancy checking and online resolution in real time. The application includes a robust scripting environment where Research Assistants without previous programming experience can independently add validation criteria. Furthermore, the scripting system is multi-purpose, and can be employed in participant eligibility confirmation, data validation, medical monitoring and notification, and adjudication. Simpler data management systems have limited editing functionality and edit evaluation is a time-consuming process. The BSC system automates much of the edit review process and provides online interactive resolution. Among the key features enhancing edit review efficiency are the ability to automatically update edit status to “fixed” when data are corrected by the keyer, and sorting and categorizing of edits to enable previously reviewed edits to be viewed separately from newly generated ones. Our reviewer investigation features include links for direct access to each form referenced in an edit, thus avoiding a multi-step process of accessing participant data. Discrepancy investigation is further improved by direct sharing of edits between central units and clinical centers for resolution. A valuable blog-type communication tool facilitates resolutions between the coordinating center and all related centers, providing context when an edit is redirected. A compatible test environment is a fundamental component that uncovers problems with edit logic and verifies the expected output. This presentation will discuss the complex data editing features of the BSC data management system that offer time savings, consistency, and ensure a high degree of data validity.\nP36\nRisk of Using Instruments in International Clinical Trials: The Scores may not be Comparable Across Different Countries\n(1) Weiquan Wei, (1) Chengwu Yang, Yang Xiao, Yuehua Peng, Dingyan Chen, Jincong Yu, Dongming Wang, Fang Ding, Zengzhen Wang 1These two authors contributed equally to this work\nPennsylvania State University, College of Medicine, Hershey, PA, USA (CY) Huazhong University of Technology and Science, Tongji Medical College, Wuhan, China (WW)\nPatient-Reported Outcomes (PRO) such as Quality of Life and their measurement tools (i.e., instruments) such as SF-36 have been increasingly used in clinical trials. On the other hand, international clinical trials are becoming more popular in recent years. In order to make the scores from these instruments comparable across different countries, the instruments’ factor structure must stay the same. However, due to many reasons including culture differences, this key assumption can be violated. As an illustration of this issue, in current study the widely used Obsessive Compulsive Drug Use Scale (OCDUS) was translated from English into Chinese, and then implemented onto 298 heroin addicts in China. Confirmatory factor analyses (CFA) failed to confirm the original 3-factor structure of the OCDUS, which implied that the 13 items cannot be summarized into the 3 sub-scale scores as specified in the original OCDUS to measure the 3 specific aspects of heroin addiction. Exploratory factor analyses (EFA) and another CFA explored and confirmed a new 3-factor structure, which is totally different from the original one. Therefore, if a drug abuse clinical trial needs to recruit participants from both of China and the western countries, and the OCDUS is chosen as the primary outcome measure given its high popularity, then, due to the totally different factor structures in populations from different countries, the sub-scale scores of the OCDUS cannot be used. In international clinical trials that used instruments, if factor structure and other psychometric properties of instruments were not investigated across patient populations in different countries, and a single factor structure was used to summarize the item scores into sub-scale scores and then comparisons across countries were made based on these sub-scale scores, then, the results can be misleading and the conclusions can be wrong.\nP37\nThe Effect of Compliance Cut-Points on Cace Treatment Effect Estimates\nJames Peugh\nBehavioral Medicine & Clinical Psychology, Cincinnati Children’s Hospital Medical Center\nIntent-to-treat (ITT) analyses of randomized control trial (RCT) data are commonplace because ITT assumes full compliance with the treatment regimen. Although unlikely, the ITT full treatment compliance assumption results in a conservative “lower-bound” estimate for the effect of treatment. Recent advances in finite longitudinal mixture modeling techniques have resulted in compliance average causal effect (CACE) statistical analyses of RCT data that allow treatment regimen compliance to be not only predicted by covariates, but also incorporated into treatment effect estimates to quantify the additive effects of both treatment condition assignment plus treatment regimen compliance upon the dependent variable of interest. However, CACE estimation techniques currently allow only a binary indicator of compliance to be used in statistical analyses. Forcing treatment compliance to be considered as a binary variable only has at least two undesirable results: patients with very different treatment regimen compliance rates could be assigned the same binary compliance value, and where the compliant/noncompliant decision cut-point is set can have a notable effect on all analysis results, including treatment effect estimates. To illustrate, longitudinal CACE analyses were conducted on data collected from juvenile fibromyalgia (N = 114) patients randomly assigned to a cognitive-behavioral therapeutic treatment condition designed to reduce functional disability, or a fibromyalgia educational control condition. Results showed treatment effect estimates can differ by as much as 350% across several (i.e., 30% - 95%) cut-points used to define a binary indication of treatment regimen compliance. Results also showed that the ability of covariates such as age, depression, and pain severity to significantly predict which patients would be more likely to comply with the treatment regimen can also differ substantially depending upon the cut-point location used in statistical analyses. Additional suggestions for the use of CACE statistical analysis techniques with RCT data, and future CACE analytic possibilities are also discussed.\nP38\nSucceeding as a Data Expert at an FDA Advisory Committee Meeting\nMark P. McIlduff\nBoston Biomedical Associates (BBA)\nUpon completion of a pivotal clinical trial, the final step in the FDA approval process often comes down to the recommendation made at an FDA advisory committee panel meeting. Preparing for panel well in advance is critical and will increase the chance of a successful presentation. Data experts may not be in the forefront on panel day, but their work in the lead up to the meeting is a key factor in how well the product is presented. Effective presentation and communication of data are one of the most important components of high-stakes presentations. If managed well, a data expert can support the message of the product and increase the chances of a favorable outcome. The data experts’ key responsibilities include creating succinct slides and assisting the panel team in training for the advisory committee question and answer period. The data expert should be familiar with and understand crucial questions at panel. They should be relaxed in calling up critical data for the presenters on the day of the meeting. A successful data expert is familiar with all areas of the product and presentation. Having a well prepared data expert on the panel team is critical to a company’s achievement at an FDA advisory committee meeting.\nP39\nDeveloping an Online Training Module for Multicentre Trials\nJohanna Sanchez, Dalah Mason, Elizabeth Asztalos\nThe Centre for Mother, Infant, and Child Research, Sunnybrook Research Institute\nThe Centre for Mother, Infant, and Child Research (CMICR) is the central coordinating centre for several large, multi-centre, international, academic randomized controlled trials that aim to improve clinical practice and the health outcomes of women and their children. In order to comply with Good Clinical Practice principles (GCP), it is essential that all participating sites receive protocol training, and that it is documented. To accomplish this, the individual trial teams organize pre-initiation investigator meetings for all site investigators, coordinators, and relevant study staff. In addition, a pre-initiation visit may be conducted to provide additional and individual training. However, it has become evident that since trials are conducted for several years, revisions to the study protocol or procedures must be reviewed with participating sites. In addition, staff changes at the participating sites result in additional people requiring training. To address this, trial teams are in constant communication with the sites by email and teleconference. Due to budgetary limitations, it is not possible to conduct interim investigator meetings, and it is not feasible to conduct site visits each time there are revisions to the study protocol or there is new personnel. To address this, CMICR has been developing an online training module for its research trials. The module will include the following trial-specific sections: study protocol, study procedures, and data entry. In addition, general sections on the module, which will be applicable to all trials, will include: ICH-GCP principles, the randomization process, and electronic data capture. After reading each section, site personnel will be prompted to complete a quiz. Upon successful completion of the quiz, the site and central coordinating trial team will receive an email of completion. The goal is to have an effective tool that will support training and also serve as a reference throughout the conduct of the trial.\nP40\nImproved Recruitment Rates in a Randomized Controlled Trial of Acute Myocardial Infarction Through Use of a Web-Based Interface for Central Randomization\nBrandi Meeks\nPopulation Health Research Institute, Hamilton Health Sciences/McMaster University\nRandomized controlled trials in the setting of acute myocardial infarction (AMI) pose many challenges in recruitment. AMI is an incident disease which can occur at any day or time and, most importantly, AMI is a medical emergency and the primary focus is on the patient’s care. It is imperative that any clinical trial-related procedures not cause undue delays since both safety and efficacy depend heavily on the time to treatment. Given the challenges in recruitment, selecting an appropriate randomization process is a fundamental decision in operationalizing a successful trial. Central randomization systems have been traditionally accessed by a telephone connection to an interactive voice response system (IVRS). Using IVRS for randomizations can be cumbersome for the user given the limitations of data entry using a telephone keypad. In a clinical trial of AMI, an investigator may be reluctant to use a tedious system themselves and it is not practical have a study nurse available 24-7. The internet now provides an alternative for accessing central randomization. Web-based interfaces involve the user accessing the central system using a web browser and entering information using a computer keypad onto an online form on a secure website. In an ongoing clinical trial in the setting of AMI patients undergoing primary PCI, feedback from participating investigators is that the web interface for central randomization is more convenient, easy to use and, most importantly in the emergency setting, quick. Observations from the trial suggest that the web-based interface increases off-hour recruitment and improves the rate of accrual. Achieving improved recruitment rates with a simple operational decision will ensure that study completion targets are met and that study results will be available sooner and while still relevant.\nP41\nNonparametric Covariate-Adjusted Hypothesis Tests Using R Estimation\n(1) John Kloke, (2) Tom Cook\nUniversity of Wisconsin – Madison\nStandard nonparametric tests are fairly common in clinical practice, for example the Wilcoxon rank-sum. These approaches, however, do not allow for the adjustment of covariates. Aligned rank tests as well as Wald tests and reduction in dispersion tests based on a rank-based fit allow the analyst to adjust for baseline demographics or other patient characteristics while maintaining the robustness and nonparametric properties of classical nonparametric tests. In this presentation we provide a brief overview of rank-based tests that allow for the adjustment of covariates. We then illustrated the use of rank tests on a large clinical dataset. We use this dataset as a basis for a Monte Carlo simulation which demonstrates the increase in power over standard parametric approaches when the assumption of normality is not met.\nP42\nFamily Forums: A Model for Participant Education and Engagement\nCharles Heisey, Dejan B. Budimirovic, Tanjala T. Gipson, Rebecca Hinton, Victor Talisa, Michael V. Johnston\nKennedy Krieger Institute\nDiscussions with families affected by Fragile X syndrome and their clinicians, led us to recognize a communication gap. Families have advocacy and LINKS groups. Clinicians have conventions and conferences. However, there were no local settings for small group discussions and expert presentations on topics of the families’ interest. We decided to target this great need by creating family forums. Using this approach, families have learned from clinicians, but our research team has learned a great deal from the families. Currently, the forums center around Fragile X Syndrome. We not only cover important subject matter, but also create an open atmosphere where individuals feel comfortable enough to discuss their real day to day issues. This enables them to receive guidance from professionals and reassurance from other families that they are not alone. Simultaneous webcasting is provided for those interested, but unable to physically attend. These webcasts are also stored on our website for later viewing. Since the response has been positive, and the audience has grown with each installment, the program will be expanded to include autism and tuberous sclerosis complex. Many clinical trials are hampered by poor recruitment. Our interactions with families led us to hypothesize that decreased opportunities for educational interactions with clinicians/investigators may contribute to poor recruitment. Family forums represent an attempt to target this barrier. This approach may become an important aspect of clinical trial operations that not only increases recruitment, but more importantly gives families an opportunity to be active participants in the process and receive thorough information to help them make an informed choice about clinical trial participation.\nP43\nBeware of too Many Cooks in the Kitchen: Concocting the Perfect Query Recipe\nChristina Tittsworth, Alexandra Stout\nSGS Life Science Services, KAI Research, Inc.\nDuring the course of a study, there are two individuals who act to ensure that the data being collected are valid and reliable: Clinical Research Associates (CRAs)/Monitors and Data Managers (DMs). The CRA conducts monitoring visits at the site, meets with the Investigative team and ensures that data are appropriately transcribed from source documentation. The DM remotely reviews the data entered in the database for consistency and accuracy by looking at trends in the data. Although they are working towards the same endpoint, they approach their roles from different perspectives, and their relationship during a study is not typically an extensive collaboration. This lack of communication can lead to the following issues: Sites issued duplicate queries, Monitor writes inappropriate queries as database specifications are not known, Monitor spends additional time writing and resolving basic queries These issues are not only burdensome to sites but also wastes resources. If the roles of the CRA and DM are clearly outlined and implemented at study start-up, efforts can be appropriately handled across the study management team. Instead, try the CoOK Approach: Coordinate query efforts between CRA and DM, Open the lines of communication by collaborating on the resolution of outstanding queries, Keep to the division of responsibility for generating data queries as outlined at study start-up Communication between the CRA and DM will ensure maximum efficiency of query writing and streamlined resolution by the site. Adhering to the agreed-upon division of responsibilities is a key factor in collecting high quality data, which in turn sets a good example to sites. A collaborative CRA/DM relationship will produce the least amount of queries, the highest quality data and likely a more successful study.\nP44\nA Practical Approach to Estimating Effect of Change in Recruitment Duration, Cluster Size, and Number of Cluster Units on Statistical Power of a Cluster Randomized Trial\nKarisse M. Torres and Bambi Arnold-Bush for the Diabetic Retinopathy Clinical Research Network ( DRCR.net )\nJaeb Center for Health Research, Diabetic Retinopathy Clinical Research Network ( DRCR.net )\nBackground: The DRCR.net is performing a cluster randomized trial to evaluate the effect of an educational intervention administered in the ophthalmologist’s office on blood glucose control in persons with diabetes. Initial sample size calculations were performed assuming an equal cluster size across 50 clusters. During the recruitment period, a number of clusters were not achieving the targeted cluster size. Therefore, it was necessary to assess the potential impact on statistical power if there was a modification to the number of clusters or cluster size.\nMethods: A conventional power computation was performed in lieu of computer simulations to obtain minimum estimates of power in order to assess how long recruitment should be extended for clusters not meeting the recruitment goal, whether additional clusters should be added, or whether the cluster size should be increased where possible. Letting m=number of clusters and n=number of participants per cluster, a power computation formula for a parallel cluster-randomized trial with a continuous outcome assuming equal cluster size was used with varying n and m and only including clusters where sample size was n or projected to be n after an additional 3, 6, or 9 months of recruitment.\nConclusions: The power estimates were used to choose the minimum number of months that recruitment needed to continue while maintaining acceptable statistical power. This is a conservative approach to power calculations since clusters with less than n participants are not included in the power calculations and only n participants are assumed for clusters with more than n participants, both of which will be included in the final analyses. The approach demonstrated may be useful to other cluster randomized trials experiencing similar recruitment difficulties.\nP45\nUsing a Global Outcome (Sum of Pre-Post Ratings) in Ascertaining Effect of an Intervention\nGeraldine E. Baggs\nAbbott Nutrition Research and Development\nIntolerance symptoms were collected at entry and 3 days post treatment. Subjects may have had different symptoms at the two timepoints. A global outcome combining the symptoms was defined to show effect of the treatment (i.e. infant formula) in resolving intolerance. A pre minus post score was obtained for each symptom and the scores added together. Positive (>0) total pre - post score indicates that more symptoms were present at entry than at 3 days post treatment. Let Xi = entry rating = 0 (absent) or 1 (present), i=1,2,…,w= total symptoms. Same for Yi = post treatment rating. Define Di = Xi - Yi. Then D = Sum(Di) + w has a Binomial (n=2w, p= 0.5) distribution, under H0. Suppose there are two groups under study, Control (C) and Experimental (E). Let mC, mE denote the number of subjects in C and E, respectively. Define DE = Sum(Di j) + mE w, the sum over all mE subjects in group E. Same for DC. In this clinical trial, p-values were calculated using the asymptotic distribution of DE – DC. We compared and contrasted results when using the exact and bootstrap distributions, and ascertained for this dataset how well asymptotic theory works, the basis of readily available computing methods.\nP46\nStandardized the Way to Management Queries without Relying Data Capturing or Cleaning Systems\nKeiko Ohta, Akiko Kada, Megumi Sakakibara, Yoko Kai, Kanae Takahashi, Haruko Yamamoto\nNational Cerebral and Cardiovascular Center\nBackground: Clinical researches need effective data capturing systems (DMS), which should have the function of consistency checks, range checks and logical checks, so that possible errors can be caught and corrected in real time. Because of their high costs and low research budgets, these systems are not always available, especially to young researchers. Some previous researches our data center supported used DMS with limited function or paper case report form without real time data checking, so we have set the query-generating program to run the defined checks automatically against the entire database, and to generate output lists of queries. The queries are then sent to the site. However, before sending, we should manually compare the new lists with previously sent queries each time, because a system cannot ignore queries which had been already responded.\nObjective: To standardize a way to manage queries without relying on DMS using a method we developed recently.\nMethods: Using Microsoft AccessÆ, we developed a query management system (QMS) which has four steps; 1) store queries with records of their date and type, 2) compare new queries to stored queries, 3) make status (necessary to send/ unnecessary to send), 4) create a reporting form. Our QMS was used in two different researches to test its applicability and usefulness.\nResults: In the two researches, our QMS ran without difficulties. The query management process became simple and smooth. [Conclusion] Our standardized way of managing queries alleviated the difficulties and simplified the process of data constancy checks, and thus can be considered a good alternative for DMS.\nP47\nThe Use of a Weighted Combination of the Life-Table Survival Estimates to Evaluate the Treatment Effect in a Randomized Clinical Trial When the Proportional Hazards Assumption is Violated\nHaijing Qin, Michele Melia\nJaeb Center for Health Research\nThe proportional hazard model is widely used for the analysis of treatment effect with censored survival data assuming constant hazard ratio. When this assumption is violated, different methods should be used to deal with non-proportionality of hazards such as using time-covariate interactions or stratification when the covariate that interacts with time is not of direct interest. In this presentation, an alternative approach is illustrated by an analysis of data in a Diabetic Retinopathy Clinical Research Network randomized clinical trial to evaluate the effect of intravitreal ranibizumab or triamcinolone acetonide on worsening of diabetic retinopathy for eyes without proliferative diabetic retinopathy at baseline that were followed for up to 3 years. The proportional hazards model could not be used to compute P-values for treatment effects due to significant violation of the proportional hazards assumption with the treatment effect changing with time. As treatment effect was of primary interest, the approach of using covariate-time interaction or stratification could not be applied. Instead, a weighted combination of the life-table survival estimates, stratifying by baseline covariates, was used to obtain an overall estimate of survival and standard error for each treatment group at each annual time point that was adjusted for the covariates. Weights were equal to the proportion of participants in each of the covariate strata with all treatment groups combined. The adjusted estimates were compared between treatment groups using the Z test.\nP48\nChallenges in Quantifying Physical Activity with Tri-Axial Accelerometers in a Randomized Controlled Clinical Trial of a Lifestyle Intervention in Sedentary Older Women\n(1) Shelly Y. Lensing, (2) Leanne L. Lefler, (3) Jean C. McSweeney, (4) Kimberly K. Garner\n(1) Department of Biostatistics, College of Medicine; University of Arkansas for Medical Sciences; (2 & 3) College of Nursing, University of Arkansas for Medical Sciences; (4) Geriatric Research, Education and Clinical Center, Central Arkansas Veterans Healthcare System\nPurpose: Little is published on quantifying physical activity in older women using tri-axial capable accelerometers. Processing of this objective data may be needed that recognizes age-related differences in motion and intensity.\nMethods: Twenty-two women > 60 years of age, able to perform activities of daily living, and who did not engage in regular physical activity were recruited to participate in a randomized study of an intervention to increase lifestyle physical activity. Metabolic Equivalent Tasks (METs), activity minutes, activity in bouts, and intensity levels were measured by the GT3X+ Tri-Axial accelerometer. As different methods for handling data can result in dramatically different values for the same outcome variables, we compared standard accelerometer settings to those that were modified to be sensitive to typical movement in older adults.\nResults: Average wear-time was significantly higher for modified 52% (SD 12) vs. 21% (SD 19) than for standard settings. Nine women (41%) did not meet criteria for usable data (>3 days of 10-hours) using standard settings, reducing the analytic sample to 13; whereas, all women were evaluable using our modified settings. Findings of our modified settings were confirmed with individual graph analyses of data. Comparing activity measures, we found significantly lower METS [Mean (Standard Deviation)] 1.09 (0.06) vs. 1.12 (0.10) and higher sedentary minutes per day, 674 (114) vs. 473 (66) for the modified vs. standard settings. There were trends for lower average kilocalories per day, higher number of bouts in moderate activity, and minutes/day in light activity for modified settings vs. standard.\nConclusions: Findings indicate standard accelerometers settings may not be sensitive enough to accurately detect activity of sedentary older women engaging in activity that is well below the standard intensity thresholds. Further research must be conducted to determine accurate measurements in older women.\nP49\nThe Potential of the Poincare Plot in the Search for Meaningful Measures of Heart Rate Variability\n(1) Steffanie M. Halberstadt, (2) Lei Gao, (3) Anne S. Lindblad, (4) Zorayr Manukyan, (5) Susan Mirow, (6) Lindell K. Weaver\n(1) The EMMES Corporation (2) Department of Statistics, George Mason University (3) The EMMES Corporation (4) Biogen Idec Inc. (5) Department of Psychiatry, University of Utah School of Medicine (6) Department of Hyperbaric Medicine, LDS Hospital\nIn many clinical trials, identifying outcomes with desirable statistical properties that can distinguish between healthy and unhealthy populations and detect treatment effects is challenging. An example is the search for an outcome that accurately describes heart rate variability (HRV), a measure of fluctuation around the mean heart rate used to evaluate cardiovascular activity. HRV is most commonly measured using time domain and frequency domain methods, such as the standard deviation of inter-beat (RR) intervals or the root of squared successive RR intervals. Another approach that has gained popularity is the geometric analysis of the Poincare plot, a dynamic visual display of each RR interval against the subsequent RR interval across time. Most analyses of Poincare plots focus qualitatively or quantitatively on the shape of the plot and how to interpret this shape in terms of short term or long term HRV. Recent attention focuses on the asymmetry of the Poincare plot and its potential for distinguishing between populations. We reviewed current analysis techniques for assessing the shape of the Poincare plot and propose a new approach to capture changes in the shape of the plot across time by quantifying the asymmetry in the plot. The new approach summarizes the asymmetry along the minor axis of the plot for each 5-minute window of the RR intervals to determine the differences over time between accelerations and decelerations of the heart rate for subjects in the same physical state (e.g. sleep or exercise). We will present data to show that this approach is a useful and robust means of describing HRV. We plan to implement this method in evaluating data from a mild traumatic brain injury population and to compare our approach to standard measures of HRV as well as other clinical outcome measures.\nP50\nDevelopment of a Parent Informational Video to Aid Recruitment in a Pediatric Surgical Randomized Trial\nDanielle L. Chandler, Michele Melia, David A. Leske, Jonathan M. Holmes, and Raymond T. Kraker, for the Pediatric Eye Disease Investigator Group\nJaeb Center for Health Research Mayo Clinic\nRecruiting children for randomized trials comparing different surgical procedures can be particularly challenging. After experiencing slower than expected recruitment in a randomized trial comparing two types of eye muscle surgery for treatment of intermittent exotropia (eye misalignment), we surveyed participating investigators and clinic coordinators to determine what specific recruitment barriers they were encountering. We identified the major recruitment barrier to be parental refusal to have their child’s surgical procedure assigned by randomization, a factor which was often related to the investigators’ difficulty in effectively communicating their clinical equipoise when discussing the study with parents. Although participating surgeons had genuine uncertainty about which surgical procedure is more effective, communicating this clinical equipoise to parents can be particularly difficult because in routine practice most surgeons tend to predominantly use one procedure or the other for treating this condition, a choice that is based more on where individual surgeons were trained than on medical evidence. To help overcome these barriers, we developed an informational video for parents who are considering enrolling their child into the randomized trial. The five-minute video describes the purpose of the study, illustrates the difference between the two surgical procedures, explains that each procedure is a very reasonable treatment option with a high reported success rate, and clarifies why randomization is necessary to determine whether one procedure is more effective over the long term. The video was posted on a public website so that it could be viewed by parents in the clinic or at home. IRB approval was required for each clinical site that elected to distribute the video to parents. Since introducing the video in February 2012, the average recruitment per month per site has increased from 0.101 patients to 0.127 patients (26% increase). We believe the video was one factor which contributed to the increased recruitment.\nP51\nEvaluation of Retention Rates by Demographic Characteristics in the Environmental Polymorphism Registry (EPR)\n(1) Rebecca Elmore, (2) Kathryn Rose, (3) Matthew Curry, (4) Dale Moffett, (5) Andrea Zombeck, (6) Beverly Warden, (7) Shepherd Schurman, (8) Stavros Garantziotis\n(1) SRA International, Inc. (2) SRA International, Inc. (3) SRA International, Inc. (4) SRA International, Inc. (5) SRA International, Inc. (6) SRA International, Inc. (7) National Institute of Environmental Health Sciences (NIEHS), National Institutes of Health (NIH) (8) National Institute of Environmental Health Sciences (NIEHS), National Institutes of Health (NIH)\nRegistries are potentially rich resources for clinical and epidemiologic studies. Retention of a registry cohort is critical to study designs that require additional data collection to address key aims. Periodic re-contacting and tracing strategies are often employed to minimize loss to follow-up and associated biases that could impact follow-up data collection, though retention of certain demographic subgroups remains a challenge for many studies. Thus, a comprehensive, multi-modal approach to retention is being implemented in the Environmental Polymorphism Registry (EPR), a large-scale registry established by the National Institute of Environmental Health Sciences (NIEHS) to facilitate genotype-driven translational research of complex disease. To date, the EPR has enrolled and collected blood samples from more than 15,000 participants, including sizeable demographic segments that are often difficult to retain (e.g., 38% male, 27% African American, and 36% less than 36 years old at the time of enrollment). In order to maximize retention, our annual re-contact plan includes efforts to reach participants by email, mail, and phone. Participants who cannot be reached by any of these methods are traced, followed by attempts to contact them by phone, if new information is obtained. This poster compares response rates for various contact methods and tracing outcomes by age, gender, race/ethnicity, and highlights challenges and potential solutions for retaining hard-to-reach registry subgroups for future study.\nP52\nA Comparison of the 3+3 Design, the Rolling Six Design, the CRM, and the Modified CRM in Phase I Clinical Trials Using Discrete Event Simulation\n(1) Tao Wang, (2) Susan Hilsenbeck, (3) Sarah Baraniuk, (4) Dejian Lai, (5) Xianglin Du, (6) Lem MoyÈ\n(1) Baylor College of Medicine (2) School of Public Health, University of Texas\nPhase I clinical trials are the “first in human” studies in medical research to examine the toxicities of new agents. Many phase I clinical trial designs proposed in the past 30 years aimed to determine the maximum tolerable dose (MTD) accurately with least trial duration. The purpose of the present research was to simulate phase I clinical trials and compare 4 commonly used designs. Patient’ population was created by the discrete event simulation (DES) method. The patients’ characteristics were generated by several distributions with the parameters derived from a historical phase I clinical trial data review. Patients were then selected and enrolled in clinical trials, each of which used the 3+3 design, the rolling six, the continual reassessment method (CRM), and the modified CRM. Five scenarios of dose-toxicity relationship were used to compare the performance of the designs. One thousand trials were simulated for each phase I clinical trial design under each dose-toxicity scenario. The results showed the rolling six design was not superior to the 3+3 design in terms of trial duration. The time to trial completion was comparable between the rolling six and the 3+3 design. However, they both shorten the duration as compared to the two CRM designs. Both CRMs were superior to the 3+3 design and the rolling six in accuracy of MTD estimation. The 3+3 design and rolling six tended to assign more patients to undesired lower dose levels. The toxicities were slightly greater in the CRMs.\nP53\nMeasuring Mental Health, Attachment, and Behavioural Development in 0-1 Year Old Infants and their Parents\nMaiken Pontoppidan\nSFI – The Danish National Center for Social Research\nIn recent years interest in early intervention for disadvantaged families has increased and preventive interventions have been implemented in Denmark, e.g. The Incredible Years Baby programme (IY Baby). Early intervention programmes aim to improve parent-infant relationship, parental self-worth, and infant development. It is however difficult to measure if improvements occur because both infants and parents go through rapid physical and psychological developments in the first year of a child’s life. This presentation will outline the criteria that have been developed to select endpoint assessment instruments for an RCT on the effects of IY Baby in Denmark. The challenges that have occurred in the process will be discussed, including issues such as: Should the primary endpoint be a parent or infant measure? Can attachment between mother and infant be measured without observation? Can a short version of the instrument be used instead of a long version? Will the test work in a Danish context (a country with a strong welfare state, high taxation, free access to school, hospitals etc., and less traditional family values than e.g. the US)? An additional challenge is that the majority of assessment instruments developed for this age group are not translated into Danish and have to be translated and validated as a part of the project.\nP54\nThe Case for Bayesian Predictive Probabilities for Interim Monitoring of Clinical Trials\n(1) Benjamin Saville, (2) Jason Connor, (3) Dan Ayers, (4) JoAnna Alvarez\n(1) Vanderbilt University (2) Berry Consultants (3) Vanderbilt University (4) Vanderbilt University\nWe explore the advantages of a fully Bayesian predictive probability approach versus a traditional Bayesian approach using posterior probabilities for the interim monitoring of clinical trials. Compared to posterior probabilities, predictive probabilities dramatically increase computing time required to design trials with known operating characteristics via simulations. However, some argue that the cost of the computational burden is overwhelmed by the benefits obtained from using predictive probabilities. In this manuscript, we provide evidence that the predictive probability approach is more closely aligned with the clinical decision process than the posterior probability approach. It is more straightforward to derive stopping rules based on predictive probabilities and we show examples of poorly chosen rules based on posterior probabilities. We explore the relationship between predictive probabilities and posterior probabilities as a function of the amount of remaining data to collect in the trial, and argue that the predictive probability approach is a superior strategy for clinical trial design.\nP55\nEnhanced Protocol Compliance in a Complex Clinical Trial Using Web-Based Response Adaptive Treatment Instructions\n(1) Martina Mueller, (2) Wenle Zhao, (3) Charles H. Kellner, (4) Sarah H. Lisanby, (5) Abeba Teklehaimanot, (6) Rebecca G. Knapp\n(1), (2), (5), (6) Medical University of South Carolina (3) Mount Sinai School of Medicine (4) Duke University\nMaintaining patients with severe depression symptom-free after remission due to an initial course of treatment remains challenging. Even mild residual symptoms contribute to functional impairment and increase the likelihood of relapse. Prolonging Remission In Depressed Elderly (PRIDE) is a multi-center randomized controlled clinical trial funded by NIMH. The study design entails a variable-length acute phase for treatment of severe depression (1 to 4 weeks depending on patient response) and a 24 week long randomized phase to maintain remission. The protocol requires that the schedule for study treatments and depression severity assessments must be determined based on the longitudinal trajectory of the clinical status as reflected in the subject’s current, previous, and baseline clinical depression rating score. To enhance protocol adherence and minimize the burden on study coordinators, a data-driven rule-based treatment instruction algorithm was developed and implemented as part of the web-based clinical trial management system (CTMS). The algorithm rules were developed based on the consensus understanding of the study protocol among clinicians, statisticians and computer programmers. Study coordinators obtain instructions from the study website related to the timing of the subject’s next visit and the number of electroconvulsive treatments that should be administered based on current, baseline and previous total scores on the patient’s depression rating. Use of this system results in three advantageous outcomes: 1) data entry is timely, because it is required to obtain treatment instructions, 2) decrease in study coordinator burden through removal of a substantial amount of data the study coordinator would otherwise be required to remember, therefore allowing the SC to focus on patient-centered activities, such as regular follow-up phone contact, and 3) removal of the possibility of human error in implementation of the complex algorithm, thereby improving the fidelity of the study intervention.\nP56\nCharacterization of Bias Between Validated Assays Using Regression Methods\n(1)Uma Kher, (2) Aditi Sapre, (3) Michael Stepanavage\n(1) Merck &Co (2) Merck &Co (3) Merck &Co\nThe efficacy of a new compound under development may be assessed based on a biomarker that is measured using several validated assays. Difficulties in the interpretation of the assay results may arise when treatment with the experimental compound impacts the biomarker results based on varying assays. Based on the assay utilized, biased biomarker assessments may result which can lead to an inaccurate treatment effect of the experimental compound. In recognition of this problem, the development team developed a plan to fully characterize the bias exhibited by varying assays across varying patient populations, subgroups, and biomarker ranges using statistical techniques commonly used in analysis of assay comparison studies. Inherent in the bias characterization is the classification of the bias components as constant or proportional. If the bias is similar up and down the range of expected values, the bias can be defined as constant. If the difference between assay methods is dependent on the true value of the assay, the bias can be characterized as proportional. We propose to compare the properties of two different regression methods in characterizing assay bias, specifically Deming regression analyses and Magari regression analyses (1998). Finally, we will utilize the methodology by Linnet (1999) to estimate the adequate sample size to detect a significant difference between assay methods in the presence of constant and proportional bias, using real life examples.\nP57\nHow Do Trial Steering Committees and Management Groups Contribute to Trial Conduct?\n(1) J. Athene Lane, (1) Rhiannon Macefield, (2) Michael Clarke, (3) Carol Gamble, (4) Shelia McCann, (5) Matthew Sydes, (1) Alison Haewood\n(1) School of Social and Community Medicine, University of Bristol, (2) All-Ireland HTMR, Queen’s University of Belfast (3) University of Liverpool (4) Aberdeen University 5MRC CTU\nAims and Introduction: Good clinical practice guidelines (GCP) emphasise trial oversight, usually through a Data Monitoring and Ethics Committee (DMEC) reporting to a Trial Steering Committee (TSC). The Trial Management Group (TMG) is responsible for the delivery of the trial. Empirical evidence underpins the role and functions of DMECs (DAMOCLES project). We aimed to investigate the roles of TSC and TMGs in decision making regarding trial conduct.\nMethods: Qualitative and semi-quantitative research to identify how TSCs and TMGs make decisions, who are the key members and how these committees interact. A purposeful sample of clinical trials from UK registered trials units selected across a range of trial designs, interventions and oversight structures. TSC and TMG meetings will be observed by a trained researcher using ethnographic methods. Follow-up interviews will also be held with key committee members (e.g. the Chair, the trial CI) and compared with the ethnographic data as well as to gain individual’s understandings of the committees roles and decision making. A parallel process evaluation will examine trial documents (e.g. protocol), databases, TSC and TMG minutes and reports to funders to establish the impact of these meetings on trial conduct.\nResults: Review of the existing published evidence base revealed no equivalent detailed guidance to that for DMECs for TSC or TMGs. A cohort of trials for ethnographic observation and analysis have been identified and are being contacted currently. Piloting the research process within one trial was successful and modifications have been made to accommodate teleconference meetings which substantially altered the meeting dynamics.\nConclusion: Mixed methods research is becoming more widely utilised for enhancing trial design and delivery and should also be valuable for identifying the role and functions of TSC and TMG in efficient trial conduct. This research should contribute to the evidence base and guidance available to trialists.\nP58\nOn the Analysis of Tuberculosis Studies with Missing Sputum Data\n(1) Daniel O. Scharfstein, (2) Andrea Rotnitzky, (3) Maria Abraham, (4) Richard Chaisson, (5) Jonghyeon Kim, (6) Lawrence Geiter\n(1) Department of Biostatistics, Johns Hopkins School of Public Health\nIn the design of randomized studies evaluating competing treatments for patients with tuberculosis, it is common to culture sputum for the presence of tuberculosis at regularly scheduled clinic visits over a specified time horizon. A primary goal in such studies is to estimate the treatment-specific distribution of the visit of first culture conversion. Culture conversion is said to have occurred for a patient at a given visit, if all culture sputums for that visit and all subsequent visits are negative. A key complication in the analysis is that culture sputum results are often missing intermittently at some visits and thus culture conversion status is interval censored in some study participants. As a result, the treatment-specific distribution of time of culture conversion is not identified without the imposition of untestable assumptions about the distribution of the culture conversion status within the censoring intervals. When positing assumptions, it is important to condition on as much of the relevant observed data as possible including observed sputum culture results both inside and outside of the censoring interval and auxiliary baseline and time-varying factors that are associated with the unknown sputum cultures inside the censoring interval. In this poster, we develop a methodology that estimates the treatment-specific distribution of time of culture conversion under a class of assumptions on the distribution of the censored culture conversion status that conditions on all of the available data. Each assumption in the class is indexed by a sensitivity analysis parameter which quantifies the magnitude of discrepancy from a specific benchmark assumption. We use a randomized tuberculosis study previously analyzed by Conde et al. (2009) as a case study.\nP59\nClinical Monitoring: A Collaborative Approach. Process and Outcomes of Implementing a Clinical Monitoring Program in the National Institute of Environmental Health Sciences Clinical Research Unit\n(1) Dawn Hunt, (2) Stavros Garantziotis, (3) Chloe Katz, (4) Lisa Murphy, (5) Silver Wevill\n(1), (3), (4), & (5) SRA International, Inc., Durham, North Carolina (2) National Institute of Environmental Health Sciences, National Institutes of Health, Research Triangle Park, NC\nIn 2011, SRA International, Inc. (SRA) developed a clinical monitoring program for studies implemented through the National Institute of Environmental Health Sciences (NIEHS) intramural Clinical Research Program. The NIEHS Clinical Research Unit (CRU), a new ambulatory care facility, began conducting research in 2009 and is currently implementing eleven active protocols, several of which were initiated prior to CRU management. Each active CRU study is monitored soon after the initial participant’s enrollment, and at least annually prior to study closure. SRA utilized a collaborative approach for monitoring these studies. The Study Monitor (SM) drafts a study-specific Monitoring Plan and obtains input from the CRU team prior to finalization. This identifies concerns, generates information on study operations, and facilitates relationship development and role identification among CRU staff. The SM subsequently schedules and conducts the monitoring visit and holds a de-briefing with the study team to discuss findings. This allows for recommendations from both the study team and SM, resulting in uniform practices, equal representation in study reports, and minimized ambiguities. Finally, the SM updates the Monitoring Plan using information from the monitoring visit to enhance the monitoring process at future visits. Early monitoring of new studies facilitates timely detection and resolution of issues that arise soon after study initiation and sets expectations from a regulatory and operational perspective. CRU study team members have reported that as a result of monitoring, the study teams have addressed consenting and quality assurance processes and refined research and documentation practices. Close collaboration between the SM and study team facilitated increased monitor understanding of the study, additional study team preparation, mutual consensus-building, expedited on-site monitoring, positive working relationships, and open communication.\nP60\nChallenges in Analyzing Safety Data in a Phase III Trial to Evaluate the Safety of Extended Regimen of Nevirapine (NVP) in Infants Born to HIV-Infected Mothers to Prevent Vertical HIV Transmission During Breastfeeding (HPTN 046)\n(1) Jenny Tseng, (1) Casey Herron, (1) Elizabeth Brown, (2) Philip Andrew, (1) Lynda Emel\n(1) Statistical Center for HIV/AIDS Research and Prevention (SCHARP), Fred Hutchinson Cancer Research Center, Seattle, WA (2) Science Facilitation, FHI 360, Durham, NC\nBackground: As a result of the systematic assessment of safety in large phase III clinical trials, many unexpected adverse events (AEs) are encountered that have no known association with the intervention. When comparing of these AEs, p-values are often reported unadjusted, which can result in a high false discovery rate (FDR). In the analysis of HPTN 046 phase III trial, using the Mehrotra-Heyse-Tukey “Double FDR” adjustment provides an objective method to address issues of multiplicity[1].\nMethods: Randomized infants who initiated study drug were included in the safety assessment. AEs, reported from 6 weeks through 8 months of life, were coded using the Medical Dictionary for Regulatory Activities (MedDRA) and grouped by Preferred Term (PT) and System Organ Class (SOC). Number and proportion of infants with specific PTs were tabulated by study arm (nevirapine vs. placebo) and compared using Fisher’s exact test, adjusting the resulting p-values using the Mehrotra-Heyse-Tukey “Double FDR” approach.\nResults: 1519 randomized infants initiated study drug (758 in nevirapine and 761 in placebo). In the first 8 months, 1250 (82%) infants reported a total of 3,696 AEs (1,876 in nevirapine and 1,820 in placebo). The non-rare AEs (those with overall incidence greater than 5) were grouped into 50 MedDRA PTs and 10 SOCs. Six PTs had unadjusted p-values less than 0.05; conjunctivitis, pneumonia, respiratory tract infection, Tinea faciei, upper respiratory tract infection and atopic dermatitis. After applying the “double FDR” adjustment only one PT achieved statistical significance (upper respiratory tract infection).\nConclusion: Safety data analysis using the “Double FDR” method to detect differences between study arms is a useful analytical tool when AE data are grouped into numerous categories. [1] Mehrotra DV and JF Heyse. Use of the false discovery rate for evaluating clinical safety data. Statistical Methods in Medical Research 2004; 13:227-238\nP61\nWhen Survival Curves Cross…Add a Landmark Analysis: An Example from Cardiology\n(1) Änne Glass, (2) Christoph Nienaber, (3) Guenther Kundt\n(1) (3) Institute for Biostatistics and Informatics in Medicine and Ageing Research, Rostock University Hospital, Germany (2) Medical Center I, Cardiology Department, Rostock University Hospital, Germany\nObjectives: When comparing survival probabilities of different treatments it may happen that Kaplan-Meier curves cross, and thereby indicate non-proportional hazards (HR). This violates assumptions of survival analysis and reduces the power of Log-Rank test. One alternative is dividing survival time into two parts by setting a landmark, and analyzing both parts separately. Results will give more background, but must be discussed cautiously, if concerning the second phase, due to limitations of Landmark analysis [1].\nMethods: 140 Patients were randomly assigned to elective standard (medical, n=68), or interventional therapy (standard and repair by stent-graft, n=72). The impact of intervention was assessed after 5ys as patients were followed for two mortality parameters and evidence of progression of disease. All analyses were based on intention to treat design: Cox regression (HR), survival curves (Log-Rank-test). As survival curves cross, Landmark analysis at 2ys after randomization was carried out, with HRs calculated separately for events 0-24 months and, 24-60 months. Additional landmarks at 1 month and 1y after randomization were investigated to diffuse the risk of dependence on the 2ys-landmark.\nResults: A benefit of endovascular repair at 5ys follow-up could be shown in the trial descriptively (decreased risks of mortality and progression), and graphically by Kaplan-Meier curves. HRs were significant for two parameters. More insight gave Landmark analysis: all parameters showed significant p-values after landmark, but not before. Additional set landmarks showed consistent results. Interactions between treatment and time were significant. The long-term benefit of repair versus medication starts beyond 2ys, if surgery-related risks are over.\nConclusions: Landmark analyses appear to be of special interest and need in cardiovascular research. Though this method is, in case of crossing survival curves, still better than a “naïve” approach, results can’t be generalized and have to be interpreted with regard to its conditional nature. [1] Dafni (2011) Circulation\nP62\nThe Sensitivity and Positive Predictive Value of Stroke Pattern Indicators in Traditional Korean Medicine: the Korean Standard Pattern Identification Questionnaire for Stroke\nTae-Yong Park, Tae-Woong Moon, Byoung-Kab Kang, Ju Ah Lee, Myeong Soo Lee, Mi Mi Ko\nMedical Research Division, Korea Institute of Oriental Medicine, Daejeon, South Korea\nBackground: Individual clinicians differ in the clinical importance they place on each clinical indicator used for pattern identification (PI). We aimed to identify the key indicators for stroke in traditional Korean medicine (TKM).\nMethods: The Korean Standard Pattern Identification for Stroke III (K-SPI-Stroke-III), a stroke questionnaire for PI that includes 44 clinical indicators associated with Dampness-Phlegm, Fire-Heat, Qi-deficiency, or Yin-deficiency, has been developed. TKM doctors from 11 hospitals evaluated 1,286 subjects, indicating the patients’ symptoms and signs on the K-SPI-Stroke-III. They were asked to leave a special mark beside the indicators that they considered important for PI, regardless of the number of indicators. Using a frequency analysis, the pattern sensitivity (PS) and the pattern positive predictive value (PPPV) for each pattern indicator were obtained.\nResults: Four indicators (reddened complexion, slippery pulse, looks weak and lethargic, and feels weak and lethargic) were chosen as the key indicators exceeding 50% for PS (differentiating between patterns) and PPPV (frequency of appearing within a pattern). In contrast, seven other indicators were determined to be the least useful indicators.\nConclusion: A new, more time-efficient version of the K-SPI-Stroke-III that maintains the current diagnostic accuracy should be published, reflecting the insignificant indicators and the key indicators.\nP63\nLiterature Review to Compare Interim and Final Results of Published Clinical Trials\n(1) Nicholas Counsell, (2) Despina Biri, (3) Allan Hackhaw\n(1) Cancer Research Uk & Ucl Cancer Trial Centre, University College London (2) Cancer Research UK & UCL Cancer Trial Centre, University College London (3) Cancer Research UK & UCL Cancer Trial Centre, University College London\nBackground: Interim analyses of randomised controlled trials (RCTs) are sometimes published before the final analysis. Because we occasionally noticed a marked difference in the reported treatment effects between the interim and final results, we aimed to determine how common this was, and the extent of the difference.\nMethods: We conducted a systematic review to compare the interim and final results of published RCTs. We used Medline (1990-2012) and some manual searches of several journals to identify pairs of articles reporting the interim and final results of the same RCT. 2712 articles were identified from the searches, in which there were 51 pairs of papers with relevant data.\nFindings: Our preliminary findings here are based on an initial 15 pairs, all of which refer to the first published article as “interim analyses”. Examination of these trials suggest that the final results were generally of a smaller magnitude than the interim ones, with smaller effect sizes in 11 (73%) trials and a marked reduction (the effect size reduced by 20% from interim to final report) in 6 (40%) of these. For example, in a trial examining trastuzumab after adjuvant chemotherapy in patients with HER2-positive early breast cancer, the reported interim hazard ratio was 0.54, but the subsequent reported hazard ratio was 0.76.\nConclusions: Our preliminary analysis suggests that published interim results may need to be considered with caution, because the final treatment effect could be noticeably smaller. Although our findings above are based on 15 of the 51 pairs, full analyses will be available for a presentation/poster. However, we can already say that at least 12% of such pairs (6/51) show a marked difference in effect sizes. It is essential to have reliable estimates of treatment effects, which are also important for regulatory authority reviews (marketing license) and health economic analyses.\nP64\nSignal Detection in Small Safety Databases\n(1) Katharine Poole, (2) Victoria Williams\n(1) Rho, Inc. (2) Rho, Inc.\nOn September 29, 2010, the FDA published a new final rule amending the IND safety reporting requirements under 21 CFR Part 312 to improve safety monitoring in clinical trials. Under this new regulation, effective March 28, 2011, all trials involving an investigational new drug (IND) application are held to more stringent analysis and reporting guidelines for adverse events. One specific requirement of the regulation is that sponsors should have a systematic approach in place for safety surveillance of their entire safety database. This signal detection process extends throughout the investigational lifetime of a drug and is designed to determine if the incidence any adverse events associated with a study drug is higher than their incidence associated with other drugs or placebos. It is intended to aid in detection of safety signals present at low frequencies that may escape detection by looking solely at individual trials. Several methods for analyzing large safety datasets aggregated across multiple studies have been published, but publications regarding signal detection in smaller safety databases are scarce. The NIAID, NIDDK, and JDRH -fund the Immune Tolerance Network (ITN) which includes a portfolio of several smaller clinical trials, many of which are under INDs held by DAIT, NIAID or by the investigators. To comply with the new regulations, attempts were made to extend the methods for analyzing large databases for use on much smaller scale analyses of single studies or groups of studies. This poster will focus on the feasibility of extrapolating a variety of methods used on large databases to smaller studies. We will also display graphical tools developed to enhance the evaluation of possible adverse event signals and discuss additional ways to group safety data for further analysis. Funded by NIAID #HHSN272200800029C.\nP65\nChallenges of Providing Medical Results to Participants in Field-Based Studies: Experiences in the Gulf Long-Term Follow-Up Study (Gulf Study)\n(1) Jessica M. Stucker, (1) Steven K. Ramsey, (1) Edward E. Gaunt, (1) Kathryn M. Rose, (1) Matthew D. Curry, (2) Aubrey Miller, (2) Richard K. Kwok, (2) Dale P. Sandler\n(1) SRA International, Inc; (2) National Institute of Environmental Health Sciences\nClinical and epidemiological studies often collect health data that are relevant to the medical care of study participants. Clinically relevant findings are typically shared with participants, many of whom view receipt of findings as a benefit of participation. This presentation focuses on our approach to sharing findings with participants in the GuLF STUDY, a prospective cohort study examining potential health effects of exposure to oil and dispersants among clean-up workers who responded to the 2010 Deepwater Horizon oil spill. Specifically, we will describe our approaches to collecting and sharing clinical measurements in the home setting, our resources and procedures for providing referrals to participants without health insurance or a primary care provider, and our procedures for responding to urgent physical and mental health problems. We will also discuss some of the challenges we have encountered in this large scale, field-based study. Some challenges are due to characteristics of the cohort, many of whom are economically disadvantaged, unhealthy, and medically underserved. Other challenges relate to the research setting where paraprofessionals interact with participants in home environments that are sometimes difficult to control. Our experiences sharing results in the home setting may be of interest to other field-based epidemiological studies.\nP66\nStrategies Implemented to Ensure Knowledge Transfer\nKathryn Mangoff, Dalah Mason, Johanna Sanchez, Elizabeth Asztalos, and Jon Barrett\nThe Centre for Mother, Infant, and Child Research, Sunnybrook Research Institute\nThe Twin Birth Study (TBS) is an international multicentre randomized controlled trial that recruited 2804 patients from 106 centres in 25 countries. TBS seeks to determine, in women expecting twins, whether a policy of planned Caesarean section decreases the likelihood of perinatal or neonatal mortality or serious neonatal morbidity, compared to a policy of planned vaginal birth. Recruitment for the study was completed in April 2011, and the data analysis was completed in early 2012. It was therefore important to develop a dissemination plan to share the results with the collaborating sites and the international perinatal community. In order to inform the TBS collaborators of the results, an international meeting was held in Toronto in May 2012. The Principal Investigator (PI) of the TBS study presented the results, which were followed by an extensive discussion with the collaborators, where everyone had the opportunity to ask questions. This meeting allowed everyone in attendance to gain a collective understanding of the results. Once the results are published or presented at a scientific meeting by the PI, a plan will then be devised on how best to disseminate the findings globally and into everyday practice. One strategy is to make a comprehensive PowerPoint slide presentation on the findings and distribute it to all of the investigators involved in TBS so they can share the information with their colleagues in their respective countries. Another strategy is to create a sub-committee made of collaborators representing different countries and regions to bring in an international perspective about the findings. Yet another strategy is to have the PI or members of the Steering Committee travel to different conferences to disseminate the findings reaching as many groups as possible. This presentation will share the strategies used to ensure successful dissemination of the TBS results worldwide.\nP67\nDelegation Logs in a Regulated Clinical Trial Involving Multidisciplinary Teams\nJessica Stortz, Melissa Brown, Dalah Mason, Johanna Sanchez, Elizabeth Asztalos\nThe Centre for Mother, Infant, and Child Research, Sunnybrook Research Institute\nThe Enhancing Breast Milk Production with Domperidone in Mothers of Preterm Neonates study (EMPOWER) is an international, multi-centre, randomized controlled trial investigating safety of domperidone for mothers of preterm infants as well as timing of initiation and duration of treatment. EMPOWER is a Health Canada regulated trial with approximately 20 participating sites. Study teams are comprised of individuals with various professional backgrounds including neonatologists, nurses, lactation consultants, pharmacists, research coordinators and assistants. The implementation and use of the Site Signature and Task Delegation Log is an ICH-GCP (International Conference on Harmonisation Good Clinical Practice) requirement and is in compliance with Health Canada regulations. This log assists in organizing delegated responsibilities at each site. The site investigator is required to authorize tasks delegated to individuals on the log, via signature, indicating each individual listed has been trained and is qualified to conduct the assigned task(s). EMPOWER sites initially faced challenges in correctly completing the Site Signature and Task Delegation Log. Some tasks on the original version of the log should only be performed by registered pharmacists or individuals with a medical license, but had been assigned to individuals without these qualifications. There were also inconsistencies with respect to the format of dates recorded in the log. Additionally, the date section was incorrectly interpreted as the start date as opposed to the date of training. Due to challenges faced at multiple sites, particularly with task assignment, the Site Signature and Task Delegation Log was revised to increase comprehensiveness of instructions for completion. This ensured compliance with both ICH-GCP and Health Canada regulations and resulted in significantly fewer errors in completion of the log. This presentation will discuss strategies utilized in the revision process and share the revised version of the EMPOWER Site Signature and Task Delegation Log.\nP68\nLocating Study Participants 5 Years After Closeout\n(1) Mae Gordon, (2) Patricia Morris, (3) Deborah Dunn, (4) Leonard Haertter, (5) The Ocular Hypertension Treatment Study\n(1) Washington University School of Medicine (2) Washington University School of Medicine (3) Washington University School of Medicine (4) Washington University School of Medicine (5) Washington University School of Medicine\nCoordinating centers and clinics typically do not have adequate tracing resources to locate participants 3-5 years after study close-out. We describe how The Ocular Hypertension Treatment Study (OHTS) triaged the task of tracing study participants from 30 clinics to a locator service. Our goal was to re-examine 80% of the surviving participants (median age of 67 years at study close-out, range 58- 94). Adjusting for deaths on the National Death Index, 1,025 study participants were classified as survivors. Clinics had PHI (birth date and place, SS, participant’s father’s name, last known address/telephone number, spouse’s name, and closest friend/relative not living with participant) collected at baseline and updated annually during the 15 year study. Clinics were able to have voice contact with 65% (669 of 1,025 survivors). The Coordinating Center referred these 356 non-contactable participants to a locator service. The locator service identified 41 additional deaths and retrieved updated contact information for the 315 survivors. Using updated contact information, Clinics were able to contact 82% (258 of the 315 surviving participants) of the “non-contactable” participants to date. Information from the locator service enabled us to increase the contact rate of surviving study participants from 65% (669 of 1,025 survivors) to 94% (927 of the 984 survivors). We will document how the locator service was useful in locating participants “lost to follow-up” and our cost benefit analysis.\nP69\nComparability of CTCAE Grading and Clinical Significance in Abnormal Clinical Laboratory Results\nAshley Pinckney\nRho, Inc.\nIn clinical trials, laboratory data is a key element of the safety profile for the treatment being studied. When local labs are used, there may be a large amount of variance in determining clinically significant abnormal lab values in the reporting of adverse events (AEs). Many clinical trials are moving towards the use of grading scales such as the National Cancer Institute common terminology criteria for adverse events (NCI-CTCAE) to provide standard ranges and implement suggested grading across clinical sites. Rho, Inc. has developed a macro to incorporate the CTCAE grading criteria into laboratory analysis datasets. The CTCAE grading is complex and encompasses a wide variety of laboratory tests making this macro extremely useful. The purpose of this poster is to explore the differences in AEs generated by CTCAE and those generated by clinical significance alone across local sites. Discussion will be presented on various reporting techniques and the distribution and comparability of abnormal clinical laboratory results across sites. This paper will also present the above macro that requires minimal user input to efficiently grade abnormal lab values based on the CTCAE.\nP70\nUse of Joint Modeling for Longitudinal and Survival Data in Long-Term Renal Clinical Trials\nAntonietta Chianca, Giovanni A Giuliano, Annalisa Perna\n‘Aldo e Cele Daccò’ Clinical Research Center, Mario Negri Institute for Pharmacological Research\nMany long-term clinical trials generate both longitudinal (repeated measurement) and survival (time to event) data, which are usually analyzed separately, using linear mixed effects models for repeated measures data, and semiparametric (Cox) proportional hazards models or parametric models for survival data. Their separate use however may be suboptimal when longitudinal variables are correlated with patient health status and survival endpoints. An unified, flexible approach by means of joint models has been proposed (Henderson 2000) in order to obtain less biased and more efficient inferences (Guo & Carlin, 2004). The ‘Aldo e Cele Daccò’ Clinical Research Center coordinates several multicenter, multi-national long-term randomized clinical trials in progressive non-diabetic chronic kidney diseases. In these studies the rate of change in measured glomerular filtration rate (mGFR) or time to end stage renal disease (ESRD) or both are selected as primary efficacy variables. We explored the performance of joint modeling through a real-data application using mGFR longitudinal data and time to ESRD measured in participants enrolled in such trials. The inclusion of time-varying covariates in survival analyses and the longitudinal response data affected by informative dropout are evaluated.\nP71\nThe Effect of the Transition from Paper-Based to Electronic Data Capture on Clinical Trial Metadata\n(1) Sunny Chan, (2) Michael Shi, (3) Cathy Yang, (4) David Lau, (5) Elizabeth Asztalos\n(1,2,3,4,5) The Centre for Mother, Infant, and Child Research, Sunnybrook Research Institute\nClinical trial metadata define the characteristics of data and documents related to trials. Common metadata are attributes of datasets from data forms and supporting documents. The Centre for Mother, Infant, and Child Research (CMICR), is the data and clinical coordinating centre for several multi-centre randomized controlled trials. In earlier trials, paper-based case report forms were found to be a reliable but costly method of data collection. Since 2011, CMICR coordinated trials use electronic data capture (EDC) as the primary method to collect data, with the exception of patient administrative questionnaires that are collected on paper-based case report forms. Clinical trial data are commonly stored electronically using a clinical database management system. With the adoption of an electronic data capture system, there exists a need to organize, standardize and manage clinical research data and metadata. A series of SAS programs will be developed to facilitate EDC data integration with existing systems, and ensure accurate and consistent data for analysis. This process will automate repeatable clinical data integration tasks and also allow central trial coordinators at CMICR to perform further data quality checks implemented in SAS logic programs. Efforts are underway to standardize the way the clinical data are organized and to ensure metadata transparency between paper-based and EDC data collection methods.\nP72\nWeb-Based Administration and Tracking of Cognitive Data in the Accordion-Mind Study\n(1) Patricia Davis, (2) John Hepler\n(1-2) Wake Forest University School of Medicine\nIn recent years more and more clinical trials have used web-based technology to aid in the management of portions of their studies. Whether used for aspects of recruitment, data collection, reporting or study analysis, the web is a useful means of communication between sites and a coordinating center within the clinical trial setting. In the ACCORDION Study, we use the web not only for data collection and analysis but also for many administrative tasks such as scheduling, sub-study management, outcomes tracking and event adjudication. By building administrative functionality into the website, data become more accessible by study staff, thus enabling the streamlining of tasks that were previously accomplished via much slower methods (phone, fax, email). In this poster, we will describe how the ACCORDION-Mind Study uses a web-based cognitive booklet tracking tool to monitor and track the mailing, completion, return and data entry of cognitive booklets for the purpose of determining status for payment. Prior to implementation of this system, booklets were managed manually with spreadsheets on multiple computers, resulting in difficulty accounting for all booklets. The implementation of a web-based process to administer and monitor this activity has greatly improved the process by which booklets are monitored and tracked, while providing a more cost- and time-efficient method for submitting requests for reimbursement to the coordinating center.\nP73\nPower Analysis and Sample Size Calculation in Designing a Smart Trial for Building Dynamic Treatment Regimens\n(1) Huitian Lei, (2) Kevin Lynch, (3) David Oslin, (4) Susan Murphy\n(1) Department of Statistics, University of Michigan (2) Treatment Research Center and Center for Studies of Addictions, Department of Psychiatry, University of Pennsylvania (3) Philadelphia Veterans Administration Medical Center, Philadelphia, Pennsylvania 19104, and Treatment Research Center and Center for Studies of Addictions, Department of Psychiatry, University of Pennsylvania (4) Department of Statistics, Institute for Social Research, and Department of Psychiatry, University of Michigan\nDynamic treatment regimen is a sequence of decision rules that specifies which treatment to provide first and how to adjust and adapt treatment according to patients’ ongoing performances and changing needs. The sequential multiple assignment randomized trial (SMART) is an experimental design used to inform the development of high-quality dynamic treatment regimens. In a two-stage SMART design, the first stage usually involves randomizing patients to two initial treatments/options. Patients’ responses to their initial treatments are measured, based on which they are classified as early responders or non-responders. The second stage of a SMART design usually involves randomizing (responders) non-responders to two (maintenance) rescue treatments/options. When designing a SMART study various research questions may be considered, some of which concern the comparison of two treatment components while others concern the comparison of embedded dynamic treatment regimens. This review focuses on the power analysis and sample size calculation in designing SMART trials. When the primary hypothesis of a SMART study is to compare two treatment components, standard sample size formulae for the two-arm randomized clinical trials are extended where a good guess of the early non-response rate may be needed. When the primary hypothesis concerns the comparison of dynamic treatment regimens as a whole, conservative sample size formulae/algorithm are developed. We conduct simulation studies to demonstrate the power performance of the formulae/algorithm and their robustness under the violation of (some) working assumptions. Throughout the review, we use the Extending Treatment Effectiveness of Naltrexone (ExTENd) SMART trial as an illustrative example.\nP74\nA Multi-System Setup for Developing Electronic Case Report Forms (E-CRFS)\nCathy Yang, Michael Shi, Sunny Chan, Elizabeth Asztalos\nThe Centre for Mother, Infant, and Child Research Sunnybrook Research Institute\nThe Centre for Mother, Infant, and Child Research (CMICR) is the data and clinical coordinating centre for several large, national, and international multi-centre randomized controlled trials. Since 2011, data collection for newly funded trials are implemented with a third party Electronic Data Capture (EDC) system, which provides cleaner data, a more efficient data entry and management process, and faster access to the data. To ensure that all patient data defined by the protocol are captured, data form design plays a key role in EDC implementation. EDC allows fast delivery and easy sharing of e-CRFs across trials, however, the challenges include: developing the e-CRFs in a short time frame, efficiently maintaining multi-version e-CRFs, and minimizing the downtime of the EDC system. Three EDC systems are set up to accommodate CMICR’s needs: a development site hosted on an internal virtual server; a production site hosted by a third party hosting provider; and a test site which is identical to the production site and hosted at the same location as the production site. This system setup allows for developing, testing, and managing live- running e-CRFs in parallel, which greatly shortens the development life cycle of e-CRFs and reduces the downtime of EDC when e-CRF updates are required at a minimal cost. A version control method was also developed to maintain the consistency of the version numbers across systems. This presentation will discuss the advantages and disadvantages of this three EDC system setup and the details of a version control method.\nP75\nComparing The Implementation of a Modified Continual Reassessment Method to a Traditional 3+3 Design in a Phase I Acute Myeloid Leukaemia Trial\n(1) Christina Yap, (2) Charlie Craddock, (3) John ‘O’ Quigley, (1,4) Lucinda Billingham\n(1) MRC Midland Hub for Trials Methodology Research, University of Birmingham (2) Centre for Clinical Haematology, Queen Elizabeth Hospital (3) UniversitÈ Pierre et Marie Curie (4) Cancer Research UK Clinical Trials Unit, University of Birmingham\nThe majority of Phase I dose-finding trials in oncology have been dominated by the traditional up-and-down designs, such as the 3+3 designs. However, in recent years, there is an emerging interest in implementing more innovative model-based dose finding methods such as the Continual Reassessment Method (CRM). Such approaches, despite being of higher statistical complexities, have demonstrated much more superior operating characteristics in correctly identifying the right dose to take forward to a Phase II trial, ability to expose fewer patients to potentially toxic doses and allocating more patients to the maximum tolerated dose. In this study, we provide further support for the use of a model based design in an Acute Myeloid Leukaemia trial. This trial studied the use of combined 5-Azacitidine and Lenalidomide as a salvage therapy in patients with acute myeloid leukaemia who relapsed after allogenic stem cell transplantation. The dose-limiting toxicity was defined as severe grade 3/4 Graft versus Host Disease and severe grade 3/4 non-haematological toxicities considered to be related to Lenalidomide or Azacitidine. We examine the operating characteristics of different variants of the continual reassessment method (CRM), with modifications to the CRM that are largely guided by the challenges in this trial. We then compare these operating characteristics with those from a traditional 3+3 design via a simulation study. Examining the operating characteristics of the 3+3 design is seldom done in practice when designing a Phase I trial. Using this trial as an example, we demonstrate how useful it is to guide the clinicians to the most appropriate design, when the operating characteristics of both the model based designs and 3+3 are presented and compared.\nP76\nThe Use of Logs and Forms for the Tracking of RT-CBM Devices in the Conceptt Trial\nAquila Farrell, Sonya Mergler, Dalah Mason, Johanna Sanchez, Denice S. Feig, Elizabeth Asztalos\n(1) Sunnybrook Health Sciences Centre, The Centre for Mother, Infant, and Child Research, Toronto, Ontario, Canada (2)Mount Sinai Hospital, Toronto, Canada\nThe Centre for Mother, Infant, and Child Research (CMICR) is the data and coordinating centre for Continuous Glucose Monitoring in Women with Type 1 Diabetes in Pregnancy Trial (CONCEPTT), a multi-centre, parallel randomized controlled trial ( RCT) with a sample size of 324 women. In CONCEPTT, pregnant women or women planning pregnancy will be randomized to either receive the Real-Time Continuous Glucose Monitor (RT-CGM) sensor added to standard therapy or standard therapy of home glucose monitoring (HGM) to determine whether continuous monitoring will improve glycemic control. Since half of the women will be randomized to receive the RT-CGM device, it is paramount to create an efficient tracking system for the management of the devices. To document the serial number of each device assigned to the participants, forms and logs were created to track devices assigned and in use. Three documents have been developed to track the RT-CGM devices: 1) Device Order Forms: used by the coordinating centre to document the devices that were sent to the site. 2) Device Inventory Form: used by the site for ongoing logging of the inventory of the devices. 3) Device Serial Number Log: used by sites to document the serial number, date the device was given and type of RT-CGM. In the event of device exchange, the reason for the switch is recorded. The coordinating centre and the sites, will be able to use these tracking documents to assign devices to a patient, order devices, and track the return of defective devices to the manufacturer. This presentation will illustrate the forms implemented to develop the monitoring systems.\nP77\nEnsuring Accurate Outcomes Review for Data Analysis\nAiny Zahid, Sunny Chan, Trinh Hoac, Johanna Sanchez, Dalah Mason, Elizabeth Asztalos\nThe Centre for Mother, Infant, and Child Research, Sunnybrook Research Institute\nThe Centre for Mother, Infant, and Child Research (CMICR) is the central coordinating centre for several multi-centre, international RCTs that aim to improve clinical practice and the health outcomes of women and their children. Outcomes are an integral part of a clinical study; therefore, it is necessary that reported outcomes are accurately reviewed to confirm they meet the definitions outlined in the study protocol. Each trial at CMICR has a process for the review of outcomes by an outcomes review committee. A tracking system is used to record cases with reported outcomes and the decisions of the outcomes review committee. For the purposes of the data analysis, a report generated from the tracking system was used as a cross referencing tool against data extracted from a database, to ensure that all cases with reported outcomes are accounted for and correctly designated as outcomes. In this process, two programs are used to ensure that true outcomes, as defined by the study protocol, are included in the data analysis. First, a SAS program is used to extract cases where outcomes have been reported on the case report forms. Then, the reported outcomes are compared against the outcomes review committee’s final decision as recorded in the tracking system. The outcomes review committee maintains the authority on the final decision regarding the designation of the reported outcomes; therefore, it is imperative that the adjudication tracking is used in conjunction with the SAS program to confirm all outcomes are accurately accounted for. The recent tracking system has proved to be useful in ensuring accurate outcomes data for data analysis, as discrepancies were revealed upon comparison of the two lists. This presentation will illustrate the process implemented to ensure accuracy for data analysis.\nP78\nElectronic Conversion and Archiving of Clinical Trial Paper Records\nDong Vo\nOttawa Hospital Research Institute Clinical Epidemiology Program, Methods Centre-DMS\nElectronic Conversion and Archiving of clinical trial paper records Moving to Electronic Archiving or Backfile digital conversion can provide remarkable value to any organization. However, it is a tremendous undertaking from an administrative perspective. Too often, completed trial paper records end up taking a lot of valuable office space and it is extremely challenging to re-access the old trial records once they are locked and stored in a secure, remote location. The OHRI-CEP, Methods Centre has developed an electronic archiving system that is comprehensive in scope, yet general enough that it can be adapted for use in different clinical centres. Here are some of the benefits to which the electronic archiving system offers: Save space by replacing money-eating space with revenue-producing office space, Save time with better accessibility: old study data are readily available, Save money with efficiency. This presentation will document the many challenges inherent to designing and implementing an effective electronic archiving system. Below are some key points in planning and developing an e-Archiving system: Roles and Responsibilities Human Resources Hardware and Software Requirements Document Assessment, Type of Documents, Retention Periods, Privacy and Confidentiality, Security, Scanning, Accessibility, Validation and Attestation Process, Document Standard Naming Convention, Document Electronic Record Format, Purging of duplicate and redundant records Document Tracking End of Life Purging Process\nP79\nA Bayesian Analysis of a Clinical Trial for Treatment of UVEITIS\n(1) Natalie Nardone, (2) S. R. Rathinam, (3) Manohar Babu, (4) Thomas Lietman, (5) Travis Porco, (6) Nisha Acharya\n(1) University of California, San Francsico (2) Aravind Eye Hospitals (3) Aravind Eye Hosptials (4) University of California, San Francsico (5) University of California, San Francsico (6) University of California, San Francsico\nPurpose: The purpose of this study was to perform a Bayesian analysis of a recent randomized clinical trial comparing the relative effectiveness of two antimetabolites (oral methotrexate and oral mycophenolate mofetil), using the pre-trial opinion of a panel of uveitis experts as a prior.\nMethods: A block-randomized observer-masked clinical trial comparing the effectiveness of the antimetabolites oral methotrexate to oral mycophenolate mofetil for non-infectious uveitis was previously conducted. The primary outcome was corticosteroid-sparing control of inflammation that was sustained for at least 28 days. Expert opinion on the most effective antimetabolite for treatment of non-infectious uveitis was queried through an online questionnaire sent to the Executive Board of the American Uveitis Society. A subjective Bayesian analysis was performed using expert opinion as the prior and the results of the clinical trial as the likelihood.\nResults: Twelve uveitis experts completed the online questionnaire out of 12 surveyed. The majority (67%) thought that mycophenolate mofetil was the more effective antimetabolite. A frequentist analysis of the trial found that 71% of methotrexate treated patients achieved treatment success, compared to 53% success with mycophenolate (OR 2.2, p=0.14). The Bayesian analysis posterior suggested that methotrexate offered 1.5-fold better odds for success (95% credible interval of 0.6 to 4.9-fold), with an estimated 71% belief that methotrexate was more effective.\nConclusions: While the frequentist analysis of the trial indicated that methotrexate was more effective, though not significantly so, the expert opinions indicated that mycophenolate mofetil would be most effective. The Bayesian analysis suggested that there was little evidence that one agent was better than the other. The clinical equipoise demonstrated in the Bayesian analysis allows justification for a larger clinical trial (NEI: U10 EY021125-01).\nP80\nDynamic Web-Based Management of a Lifestyle Intervention: The Study of Novel Approaches to Prevention Clinical Trial\n(1) Leah Griffin, (2) Karen Erickson, (3) Erica Ferguson, (4) Letitia Perdue, (5) Mark Espeland\nWake Forest Baptist Health\nThe Study of Novel Approaches to Prevention (SNAP) is a two-center randomized, controlled clinical trial of 599 young adults designed to compare two approaches to weight gain prevention. Participants were randomly assigned to one of three conditions: interventions targeting large or small behavioral changes plus self-regulation or a control group. Both intervention groups participated in an initial four month in-person program which included eight weekly group sessions followed by two monthly meetings. Following the initial intervention period, continued contact with study participants is driven by regular reporting on a group-specific intervention website. The main purpose of the website is for regular reporting of body weight. Participants also have access to treatment lessons for their group, quarterly newsletters, weekly tips, and a diary to report diet and exercise behaviors. Participants are also encouraged to use the website to submit their current weights at least weekly. They also have the ability to submit additional information such as exercise minutes, caloric intake, and number of diet changes made. In order to track overall and individual progress and monitor adherence, we developed an extensive series of interactive intervention reporting tools that take the information entered by the participant and report that data back to the clinics. The reports provide the clinics with up-to-date information including details such as website usage, weight tracking, diary submissions, and diary content. Both summaries and detailed information on each individual and their intervention history are provided. With these reporting tools, clinics monitor participants and continuously provide them with the feedback they need to maintain intervention participation such as personalized correspondence based on weight and website usage, tip sheets, and offers for individual counseling. Web-based monitoring provides an efficient means to track adherence to behavioral interventions in multi-center trials.\nP81\nThe Transforming of Multicenter Clinical Trials to Observational Follow-Up Studies\n(1) Lea Drye, (2) Anne Casper, (3) Janet Holbrook, (4) Gabrielle Jenkins, (5) Curtis Meinert\n(1) Johns Hopkins Bloomberg School of Public Health, Epidemiology Department and Center for Clinical Trials (2) Johns Hopkins Bloomberg School of Public Health, Epidemiology Department and Center for Clinical Trials (3) Johns Hopkins Bloomberg School of Public Health, Epidemiology Department and Center for Clinical Trials (4) University of Alabama at Birmingham School of Public Health, Epidemiology Department (5) Johns Hopkins Bloomberg School of Public Health, Epidemiology Department and Center for Clinical Trials\nInvestigators may elect to continue to follow participants in an observational study after the trial portion of a clinical trial comes to an end. The additional follow-up may be initiated because of a need for more data on adverse events, efficacy, or costs related to treatment, or for reasons unrelated to treatment such as to observe the natural history of the disease taking advantage of the established cohort from the trial. However, the transition from trial to follow-up study can be complicated by issues in funding, maintaining contact with and cooperation of participants and the relevance of the scientific question. At the Johns Hopkins Coordinating Centers, several trials have transitioned to observational follow-up. The Alzheimer’s Disease Anti-inflammatory Prevention Trial (ADAPT) follow-up study was carried out to examine whether a late emerging trend toward decreased risk of Alzheimer’s Disease in the naproxen treatment group was sustained in the long term and to compare mortality across treatment groups. The Childhood Asthma Management Program (CAMP) continuation studies were designed to assess long term treatment effects of budesonide and nedocromil on physical growth and development of children as well as to investigate the natural history of asthma and repercussions of persistent asthma. The objective of the Multicenter Uveitis Steroid Treatment (MUST) trial follow-up study is to extend the comparison of the effects of fluocinolone acetonide intraocular implant versus systemic steroid therapy for chronic uveitis as well as comparison of the treatments costs and to collect data on need for re-implantation. Issues encountered and lessons learned from these transitions and other trials will be reviewed. The authors will discuss issues such as obtaining funding and funding cuts, IRB and consent requirements, recruitment and enrollment and difficulty in combining trial and follow-up data.\nP82\nIntegrating Study Data from Multiple Sources Using SQL Server - The Accordion Study\nJason Griffin, John Hepler, Gregory Evans, Robert P. Byington\nWake Forest Health Sciences\nThe Action to Control Cardiovascular Risk in Diabetes (ACCORD) Trial was a randomized clinical trial in which 10,251 people with type 2 diabetes were treated and followed for an average of approximately 5 years from 2001 through mid-2009 to determine whether intensive risk factor management could reduce cardiovascular disease risk. A prospective observational follow-up study of ACCORD participants called ACCORDION is now underway and is designed to clarify the long-term effects of the ACCORD treatment strategies and provide additional data on the long-term relationships among various cardiovascular and diabetic risk factors. Data collection for ACCORDION includes clinical data from participant visits entered remotely via a web interface, as well as data from central agencies, including a central laboratory and fundus photograph reading center, which transfer data to the coordinating center on a periodic basis in csv, excel and xml formats. Merging data from multiple sources can be a time consuming and tedious process. Incoming data must be gathered, cleaned and organized in a way that will ensure an efficient structure for study management, reporting and analysis. Once data are received and stored in a secure location, automated processes are put in place to import and document the insertions of new data, along with corrections to existing data. Sql Server 2008 provides a set of robust tools and technologies that can simplify and improve the processes needed to this end. This presentation will describe the methods and tools within Sql Server 2008 which are used to import and integrate data within the ACCORDION study.\nP83\nMissing Data in Clinical Trials\nVicki Lu\nBoston Biomedical Associates\nMissing data is a prevailing problem in clinical studies and the credibility of clinical trial results could be substantially undermined by missing data. Missing data can arise during any stage of a clinical study due to a variety of reasons including patient dropout, incomplete data collection, and invalid or missed lab measurements. In practice, frequently used approaches to address missing data primarily focus on handing missing data during the data analysis phase of a study. However, the prevention and avoidance of missing data are more important in addressing the problem in clinical trials. Domestic and international guidelines and recommendations on missing data in clinical trials have been published. The current presentation will review strategies and steps that could and should be taken to reduce the amount of missing data and the impact of missing data in clinical trials. The focus is on three critical elements: (1) careful trial design; (2) good trial conduct; (3) appropriate data analysis in handling missing data.\nP84\nCreating a New Scoring Endpoint of Cardiac Events and Questionnaire-Based Heart Failure Symptom\n(1) Akiko Kada, (2) Masanori Asakura, (3) Hiroyuki Uesaka, (4) Kaori Doi, (5) Haruko Yamamoto, (6) Masafumi Kitakaze\n(1) National Cerebral and Cardiovascular Center (2) National Cerebral and Cardiovascular Center (3) Osaka University, National Cerebral and Cardiovascular Center (4) National Cerebral and Cardiovascular Center (5) National Cerebral and Cardiovascular Center (6) National Cerebral and Cardiovascular Center\nBackground: It is difficult to evaluate the efficacy of treatments for acute heart failure. According to “The guidelines for methods of clinical evaluation of anti-cardiac failure drugs”, essential efficacy variables should be clinical signs/symptoms, hemodynamics, prognosis, and improvement of quality of life (QOL). But there is no single endpoint to measure these variables simultaneously. So we create a new endpoint judging the effectiveness of treatment for acute heart failure in short-term outcome.\nMethods: We searched clinical trials of acute heart failure, and selected several measurements of cardiac events and QOL. We determined how to combine them, and the characteristics of the combinations. Sample size calculation and power analysis were also examined.\nResults: Composite scoring endpoints of all-cause mortality, hospitalization due to heart failure and questionnaire-based heart failure symptom (Kansas City Cardiomyopathy Questionnaire: KCCQ) at 3 months after the start of drug administration were evaluated. The summary score by KCCQ (scale from 0 to 100) at 3 months after the start of drug administration is divided into 10 points to give a score of −2.5, −2.0,−1.5,−1.0, −0.5, 0, 0.5, 1.0, 1.5, or 2.0. If hospitalization due to worsening of heart failure is observed during 3 months after randomization, one point is deducted from the above score. And if all-cause death within 3 months after randomization, the score is given with -4.\nConclusion: We created the new composite scoring endpoint of all-cause mortality, hospitalization due to heart failure and KCCQ. The endpoint will be confirmed in a current trial of aldosterone antagonist in acute heart failure exploratory.\nP85\nDigital Data Collection – The Life Experience\nScott Rushing, Delilah Cook, Kate Youngman, Erica McDavitt\nWake Forest University School of Medicine, Wake Forest University School of Medicine, Stanford University, Tufts University\nDigital data collection technologies afford clinical researchers the ability to capture data that would otherwise be impractical to obtain. Consider collection of physical activity levels of subjects in an intervention. Subjects could record their activity manually at set time-points during the day for estimated levels of activity, or they could wear an accelerometer and gather precise activity levels for the entire day. This new era of digital data collection allows us to more easily obtain detailed analytic measures, but comes with challenges of its own. In this presentation, we will explore the challenges faced during a current trial, and describe procedures and tools that were put in place to mediate those challenges. The Lifestyle Interventions and Independence for Elders (LIFE) Study is a phase 3, randomized controlled trial being conducted to compare a moderate-intensity physical activity program to a successful aging health education program in 1,635 sedentary older adults who are followed for an average of 2.7 years. LIFE Study subjects interact with computerized data capture tools including accelerometers for physical activity and two interactive programs for cognitive function and self-perception of mobility. Each of these proprietary systems collect and store data to a local computer which is systematically transferred to the Coordinating Center for analysis. While data transfers sound like a trivial task in today’s environment, in LIFE we experienced a number of challenges in transmitting and reconciling these data. In this presentation we will discuss systematic ways to ensure accurate and consistent electronic data transmission. As the medical research field continues to adopt new data collection technologies, our ability to maximize the quality of electronic datasets will become exceedingly important. Our discussion of the procedures we implemented in LIFE will guide a conversation about how we can fully benefit from the rich data sets these technologies afford us.\nP86\nUtilizing R for the Graphical Reporting of Adverse Event Clinical Trial Data to Facilitate DSMB Review\nMiguel Villarreal, James Rochon, Jeremy Wildfire, Agustin Calatroni, Tee Bahnson, Rebecca Zabel, Katy Jaffee\nRho, Inc.\nRho serves as the Statistical and Data Coordinating Center (SDCC) for the Immune Tolerance Network (ITN) and the Inner City Asthma Consortium (ICAC) and is responsible for generating materials for Data Safety Monitoring Board (DSMB) review. Depending on the size of the clinical trial, reviewers can sometimes be presented with an expansive amount of Adverse Event (AE) incidence summary data in tabular format. Tables and supporting listings can be overwhelming and ineffective in identifying safety signals and can distract the reviewer from absorbing meaningful patterns in the data. In order to facilitate and expedite safety data review, it is necessary to condense AE incidence into discernable chunks using graphical techniques. Utilizing R, an open-source statistical and computing language and environment, we expand on the graphical techniques prompted by Dr. Frank Harrell and summarize graphically the hierarchical classification of coded verbatim AE terms into System Organ Class (SOC) and Preferred Term (PT) using annotated dotplots. This technique combines the structural familiarity of a summary table with the visual economy of a graphic and conveys a considerable amount of information in a compact, intuitive, and readable manner. This makes it easier for the reviewer to quickly absorb meaningful patterns in the data and discern safety signals that could potentially be obscured by conventional tabular displays. Through the use and flexibility of R graphics, this method of reporting Adverse Events can be expanded and applied to a broad range of clinical trials and reproduced for other DSMB reports. Several examples of AE plots will be presented, and the pros and cons associated with using R will be discussed. This project is funded by the National Institute of Allergy and Infectious Diseases, National Institutes of Health, under contracts HHSN272200800029C and HHSN2722010000521.\nP87\nThe Evaluation of Medical Diagnostic Tests in Clinical Studies Using Local Likelihood Density Estimation and Roc Curves\n(1) Norberto Pantoja-Galicia, (2) Mary Thompson\n(1) Food and Drug Administration (2) University of Waterloo\nAn important component in the evaluation of diagnostic tests in clinical studies is the estimation of the distribution of diagnostic test results for the diseased and non-diseased subjects in a study population. The intuitive appeal of kernel density estimation makes it an attractive candidate to obtain nonparametric estimates of such probability density functions. However, in spite of the advantages presented by standard kernel density estimation techniques, a problem of increased bias at and near a known boundary is present in certain scenarios. We develop a local likelihood estimation method that is suitable for a density with a discontinuity at a boundary. We propose an estimator that corrects boundary bias. Applications in the context of diagnostic test evaluation in clinical studies using ROC curve analysis are discussed and displayed.\nP88\nEnsuring Compliance to Good Clinical Practice Requirements in Multi-Centre Studies\nAsma Qureshi, Melissa Brown, Johanna Sanchez, Dalah Mason, Elizabeth Asztalos\nThe Centre for Mother, Infant, and Child Research Sunnybrook Research Institute\nThe Centre for Mother, Infant, and Child Research (CMICR) is the central coordinating centre for several large, multi-centre, international randomized controlled trials that aim to improve clinical practice and the health outcomes of women and their children. As the sponsor for multiple regulated trials, it is important to ensure compliance to the International Conference of Harmonization-Good Clinical Practice (ICH-GCP) requirements and Health Canada Division 5 Food and Drug Regulations, at all sites. Efforts to maintain the collection of GCP essential documents from each person involved with the trial included constant communication with the sites via email, phone calls, site visits, and monthly newsletters. Attempts to retrieve essential documents and keep certificates and licenses up to date were intensified as protocol amendments occurred, along with the inclusion of international sites that have their own national regulatory requirements. As collection and management of these numerous documents became increasingly difficult, a new tracking system was required to increase efficiency and effectiveness of this process. A solution to this problem was to develop a process to automate site reminders of outstanding essential documents. The process involved creating a database to track and manage all of the required documents listed for each phase of the trial. Once created, the received documents are inputted into the database and the date is recorded. A program was then created to remind the sites to submit their renewals annually. This system would generate an email for each site, which would notify them of any outstanding documents. The introduction of automated tracking sheets has allowed for documents to be recorded and tracked in one place efficiently and reduced the potential for oversight.\nP89\nNo-Fault Compensation to Participants in Un-Notified Clinical Trials Including Stem Cell Study in Japan\nToshinori Murayama, Eriko Sumi, Manabu Minami, Toshiko Ihara, Masayuki Yokode\nClinical Innovative Medicine, Translational Research Center, Kyoto University Hospital\nCIOMS International Ethical Guidelines for Biomedical Research Involving Human Subjects (2002) state that investigators should ensure that research subjects who suffer injury are entitled to free medical treatment and financial assistance as compensates them equitably for any resultant impairment. While Helsinki Declaration of World Medical Association (2008) describes that the protocol should include information regarding provisions for treating and/or compensating subjects who are harmed as a consequence of participation in the research study. This no-fault compensation is a different notion from legal liability/indemnity/reparation due to malpractice or negligence in clinical trials. Un-notified clinical trials to the authorities are still allowed in Japan, other than IND/IDE trials. The Ethical Guidelines for Clinical Studies, the only regulation for those, were fundamentally revised and enacted in April 2009, which obligate researchers to take measures on compensation such as insurance in clinical trials to assess pharmaceuticals or medical devices. The Guidelines for Clinical Studies utilizing Human Stem Cells were also revised and enacted in November 2010, which require researchers to conduct compensation. Since casualty insurance companies have not accumulated know-how to estimate the risk of un-notified trials besides IND/IDE trials, compensation insurance remains inadequate in quality; 1) Medical expense or medical allowance cannot be paid. 2) There are a considerable range of exceptions for such clinical trials as employ anticancer agents, immunosuppressants, cells/tissue and implantable devices in this insurance. 3) Malpractice or Product Liablity is exempted from the insurance responsibility. In order to overcome the situation, we have investigated legal restriction for insurance, medical expense reduction system in the academic hospital, and an academic guideline for compensation in researcher-initiated un-notified clinical trials with a casualty company, so that we could contribute to improving participants protection.\nP90\nReproducible Research Methods with the SAS System and Microsoft Word\nPaul A. Thompson\nSanford Research/USD\nReproducible research methods are defined as performing the analysis and data management components of a project in a way which enables the process to be repeated strictly and easily. With the use of R, sweave, and LaTeX, this can be accomplished due to the entirely scripted nature of all components of the analysis and documentation systems. With SAS, Word .rtf file, and Adobe .pdf files, this has been previously less do-able. The system defined within Sanford Research/USD for DSMB reports is described. This system uses SAS to prepare both the report and perform the analysis. As SAS can be considered a scripting engine as well as an analysis engine, the entire process can be “canned-up” within SAS. While more attention has been paid to R, sweave, and LaTeX, these tools are often confined to the more sophisticated end of the analysis spectrum. Using the methods described here, all parts of the process can be done within SAS itself.\nP91\nComparison of a Linear Mixed Effects Model and Mixed Model Repeated Measures for Analyzing Incomplete Longitudinal Data in Clinical Trials\n(1) Amit K. Chowdhry, (2) Michael P. McDermott\n(1) (2) Department of Biostatistics and Computational Biology, University of Rochester School of Medicine and Dentistry\nConsider a randomized clinical trial with longitudinal measurements having a multivariate normal distribution obtained from participants at a number of fixed time points, where the measurement at the last time point is the primary outcome variable. A common analytic strategy to accommodate missing data is mixed model repeated measures (MMRM), a method based on direct likelihood that treats time as a categorical variable. An alternative strategy that might make more efficient use of information from the earlier time points is a linear mixed effects model (LMEM) that specifies a functional form for the relationship between response and time. We performed a simulation study to examine the consequences on power, variance, and bias of using the LMEM strategy for analysis. This strategy would be expected to be superior to MMRM in cases where the functional form of the relationship between response and time is correctly specified, but the sensitivity to model misspecification is important to investigate. The study considered a variety of assumptions concerning the number of time points, pattern of mean responses over time, correlation structure for the repeated measurements, and rates of subject withdrawal (under the missing at random assumption). Estimates of power, variance, and bias were based on 100,000 replications of the simulation. When the response evolved linearly over time, a correctly specified LMEM yielded noticeable gains in power and precision that depended on the correlation structure; a smaller benefit was seen for the setting with a quadratic relationship between time and response. When the LMEM was misspecified, substantial bias was possible depending on the degree of misspecification. LMEM can provide a substantial increase in power over MMRM when the functional form of the relationship between response and time is simple and correctly specified, but there is a risk of substantial bias if the model is incorrectly specified.\nP92\nTitles Versus Titles-and-Abstracts for Initial Screening of Articles for Systematic Reviews\n(1,2) Farrah J. Mateen, (1,2) Jiwon Oh, (1,3) Ana I. Tergas, (1,4) Neil H. Bhayani, (1,5) Biren B. Kamdar\n(1) Bloomberg School of Public Health, Johns Hopkins University (2) Department of Neurology, Johns Hopkins Hospital (3) Division of Gynecologic Oncology, Johns Hopkins Hospital (4) Department of Surgery, Howard University Hospital (5) Division of Pulmonary and Critical Care Medicine\nBackground: There is no consensus on whether screening titles or titles and abstracts simultaneously is the preferable screening strategy for inclusion of articles into a systematic review.\nMethods: Two methods of screening articles for inclusion in a systematic review were compared: titles-first versus titles-abstracts simultaneously. Each citation found in Medline or Embase was reviewed by two physician reviewers for pre-specified criteria: the citation included (1) primary data; (2) the exposure of interest; and (3) the outcome of interest.\nResults: There were 2,965 unique citations. The titles-first strategy resulted in an immediate rejection of 2,558 (86%) of the records after reading the title alone, requiring review of 239 titles and abstracts, and subsequently 176 full text articles. The simultaneous titles-and-abstracts review led to rejection of 2,782 citations (94%) and review of 183 full text articles. Inter-reviewer agreement to include an article for full text review using the titles-first screening strategy was 89-94% (kappa=0.54) and 96-97% (kappa=0.56) for titles-and-abstracts combined. The final systematic review included 13 articles, all of which were identified by both screening strategies.\nConclusions: Screening via a titles-first approach may be more efficient than screening titles and abstracts together.\nP93\nComparison of Benefits and Harms of Roflumilast in Patients with Moderate to Severe Chronic Obstructive Pulmonary Disease and a History of Exacerbations\n(1) Tsung Yu, (2) Kevin M. Fain, (3) Sonal Singh, (4) Carlos O. Weiss, (5) Tianjing Li, (6) Ravi Varadhan, (7) Cynthia M. Boyd, (8) Milo A. Puhan\n(1) Epidemiology, Johns Hopkins Bloomberg School of Public Health (2) Epidemiology, Johns Hopkins Bloomberg School of Public Health (3) Division of General Internal Medicine, Johns Hopkins School of Medicine (4) Geriatric Medicine and Gerontology, Michigan State University (5) Epidemiology, Johns Hopkins Bloomberg School of Public Health (6) Division of Geriatrics, Johns Hopkins School of Medicine (7) Division of Geriatrics, Johns Hopkins School of Medicine (8) Epidemiology, Johns Hopkins Bloomberg School of Public Health\nBackground: Roflumilast is a phosphodiesterase 4 inhibitor that reduces risk of exacerbations in patients with chronic obstructive pulmonary disease (COPD). Roflumilast was approved by the FDA and EMA but has a complicated regulatory history because of its gastrointestinal and psychiatric harms. The aim of this study was to use a multidimensional approach that considers multiple outcomes, patient characteristics and preferences to quantitatively compare the benefit and harm of roflumilast (500 mcg per day) as compared to placebo.\nMethods: The outcomes, evaluated for a time horizon of 1 year, were exacerbations prevented and gastrointestinal and psychiatric harms associated with roflumilast. We used publicly available documents from the FDA and calculated, based on an approach developed by the National Cancer Institute, the net benefit/harm index per 10,000 COPD patients treated over 1 year. Each outcome was weighted by their relative importance and we considered death as a competing risk. In sensitivity analyses we explored the impact of using different weights for the importance of outcomes.\nResults: The net benefit/harm indexes for each category of men and women with different age and baseline risk of exacerbations (20%, 40%, 60%, and 80% of patients having at least 1 exacerbation per year) suggest that roflumilast causes more harms than benefits overall. For example, in men age 65 and older with a baseline risk where 40% of patients having at least 1 exacerbation per year, the index is -346, which suggests more harms caused by roflumilast. Only in sensitivity analyses ignoring minor outcomes the index was positive for patients at higher risk of exacerbations.\nConclusions: Our results suggest that roflumilast causes more harms than benefits overall, even for patients at higher risk of exacerbations. The results of this multidimensional and transparent approach for benefit harm assessment of roflumilast challenge its approval in the US and Europe.\n""","0.048789218","""http://journals.sagepub.com/doi/10.1177/1740774513497438""","[-0.133363,51.525635]"
"""University_of_Aberdeen""","""The effects of mobile real-time information on rural passengers: Transportation Planning and Technology: Vol 39, No 1""","""Transportation Planning and Technology\nArticles\nThe effects of mobile real-time information on rural passengers\nGet access /doi/full/10.1080/03081060.2015.1108085?needAccess=true\nABSTRACT\nMobile real-time passenger information (RTPI) systems are becoming ubiquitous in public transport and a plethora of studies have explored the effects they have on passengers. However, these studies mostly focus on urban areas and largely ignore rural dwellers. In this paper, we present results of a study that looks into the effects that mobile RTPI has on passengers in rural areas. The results indicate that the participants primarily used the mobile RTPI system to gain situation and geospatial awareness and to adapt their travel behaviour in disrupted circumstances. Further, we have identified that mobile RTPI significantly affects the everyday public transport travel of individuals. The outcomes of this study provide an initial understanding of the effects of a mobile RTPI system on rural users.\n""","0.8743022","""http://www.tandfonline.com/doi/full/10.1080/03081060.2015.1108085""","[-2.099122,57.165019]"
"""University_of_Sheffield""","""Association Between Subject Functional Status, Seat Height, and Movement Strategy in Sit-to-Stand Performance - Mazzà - 2004 - Journal of the American Geriatrics Society - Wiley Online Library""","""Journal of the American Geriatrics Society\nPrevious article in issue: The Delirium Index, a Measure of the Severity of Delirium: New Findings on Reliability, Validity, and Responsiveness\nPrevious article in issue: The Delirium Index, a Measure of the Severity of Delirium: New Findings on Reliability, Validity, and Responsiveness\nAssociation Between Subject Functional Status, Seat Height, and Movement Strategy in Sit-to-Stand Performance\nAuthors\nClaudia Mazzà PhD,\nSearch for more papers by this author\nFrancesco Benvenuti MD,\nLaboratory of Physiopathology and Rehabilitation of Movement, Department of Geriatrics, “I Fraticini,” Istituto Nazionale Riposo e Cura Anziani, Florence, Italy\nSearch for more papers by this author\nCarlo Bimbi MS,\nLaboratory of Physiopathology and Rehabilitation of Movement, Department of Geriatrics, “I Fraticini,” Istituto Nazionale Riposo e Cura Anziani, Florence, Italy\nCited by (CrossRef): 12 articles Check for updates\nCitation tools\nCiting literature\nFunded by the Ministry of Education, Università e della Ricerca (Italy) and Ministry of Health (Italy). Related paper presentations: IV SIAMOC Congress, Catania, Italy, October 2003.\nAddress correspondence to Dr. Francesco Benvenuti, Dipartimento della Riabilitazione, AUSL11, Regione Toscana, Ospedale degli Infermi, Piazza XX Settembre 10, 56028 San Miniato, Pisa, Italy. E-mail: f.benvenuti@usl11.tos.it\nAbstract\nObjectives: To explore the association between an individual's functional status, movement task difficulty, and effectiveness of compensatory movement strategies within a sit-to-stand (STS) paradigm.\nDesign: Cross-sectional study.\nSettings: Rehabilitation unit of the Istituto Nazionale Riposo e Cura Anziani Geriatric Hospital of Florence, Italy.\nParticipants: A convenience sample (131 subjects) of the outpatient clinic and day-hospital population.\nMeasurements: A performance-based test (repeated chair standing) was used to divide the subjects into five functional groups. Subjects performed a series of single STS tasks across a range of five descending seat heights. They were instructed to stand without using arms or compensatory strategies. If unable, swinging the arms was allowed, and if the inability persisted, subjects could push with their arms during subsequent attempts. The strategy or inability to stand formed the dependent measures.\nResults: Subjects within the two highest functional groups could complete the single STS task at all seat heights, with a slight increased use of compensatory strategies at the lowest seat height. The effectiveness of the compensatory strategies decreased rapidly as a function of seat height and functional status. One-third (35.5%) of the subjects in the middle functional group swung their arms at the lower seat heights. Across the three least functional groups, 11.8%, 30.6%, and 83.3% of the subjects, respectively, were unable to stand at the lowest seat height.\nConclusion: The individual's functional status and difficulty of the task influenced the effectiveness of a compensatory strategy to maintain the ability to stand, supporting the idea that disability depends on the interplay between environmental demands and physical ability.\nArticles related to the one you are viewing\nCiting Literature\nNumber of times cited: 12\n1\nUrszula E. Dolecka, Tamara Ownsworth, Suzanne S. Kuys, Comparison of sit-to-stand strategies used by older adults and people living with dementia, Archives of Gerontology and Geriatrics, 2015, 60, 3, 528\nCrossRef\n2\nShamay S. M. Ng, Susanna Y. Cheung, Lauren S. W. Lai, Ann S. L. Liu, Selena H. I. Ieong, Shirley S. M. Fong, Association of Seat Height and Arm Position on the Five Times Sit-to-Stand Test Times of Stroke Survivors, BioMed Research International, 2013, 2013, 1\nCrossRef\n3\nN. Millor, P. Lecumberri, M. Gomez, A. Martinez-Ramirez, L. Rodriguez-Manas, F. J. Garcia-Garcia, M. Izquierdo, Automatic Evaluation of the 30-s Chair Stand Test Using Inertial/Magnetic-Based Technology in an Older Prefrail Population, IEEE Journal of Biomedical and Health Informatics, 2013, 17, 4, 820\nCrossRef\n4\nAdrian S Dobs, Ralph V Boccia, Christopher C Croot, Nashat Y Gabrail, James T Dalton, Michael L Hancock, Mary A Johnston, Mitchell S Steiner, Effects of enobosarm on muscle wasting and physical function in patients with cancer: a double-blind, randomised controlled phase 2 trial, The Lancet Oncology, 2013, 14, 4, 335\nCrossRef\n5\nYi-Liang Kuo, The influence of chair seat height on the performance of community-dwelling older adults’ 30-second chair stand test, Aging Clinical and Experimental Research, 2013, 25, 3, 305\nCrossRef\n6\nJanet A. Kneiss, Jeff R. Houck, Susan V. Bukata, J. Edward Puzas, Influence of Upper Extremity Assistance on Lower Extremity Force Application Symmetry in Individuals Post-Hip Fracture During the Sit-to-Stand Task, Journal of Orthopaedic & Sports Physical Therapy, 2012, 42, 5, 474\n7\nRichard W. Bohannon, Measurement of Sit-to-Stand Among Older Adults, Topics in Geriatric Rehabilitation, 2012, 28, 1, 11\nCrossRef\n8\nJason C. Gillette, Catherine A. Stevermer, The effects of symmetric and asymmetric foot placements on sit-to-stand joint moments, Gait & Posture, 2012, 35, 1, 78\nCrossRef\n9\nR C van Lummel, E Ainsworth, J M Hausdorff, U Lindemann, P J Beek, J H van Dieën, Validation of seat-off and seat-on in repeated sit-to-stand movements using a single-body-fixed sensor, Physiological Measurement, 2012, 33, 11, 1855\nCrossRef\n10\nR. Ganea, A. Paraschiv-Ionescu, C. Büla, S. Rochat, K. Aminian, Multi-parametric evaluation of sit-to-stand and stand-to-sit transitions in elderly people, Medical Engineering & Physics, 2011, 33, 9, 1086\nCrossRef\n11\nC. O. Weiss, L. P. Fried, K. Bandeen-Roche, Exploring the Hierarchy of Mobility Performance in High-Functioning Older Women, The Journals of Gerontology Series A: Biological Sciences and Medical Sciences, 2007, 62, 2, 167\nCrossRef\n12\nClaudia Mazzà, Steven J. Stanhope, Antonio Taviani, Aurelio Cappozzo, Biomechanic Modeling of Sit-to-Stand to Upright Posture for Mobility Assessment of Persons With Chronic Stroke, Archives of Physical Medicine and Rehabilitation, 2006, 87, 5, 635\n""","0.30910215","""http://onlinelibrary.wiley.com/doi/10.1111/j.1532-5415.2004.52472.x/abstract?systemMessage=Please+be+advised+that+we+experienced+an+unexpected+issue+that+occurred+on+Saturday+and+Sunday+January+20th+and+21st+that+caused+the+site+to+be+down+for+an+extended+period+of+time+and+affected+the+ability+of+users+to+access+content+on+Wiley+Online+Library.+This+issue+has+now+been+fully+resolved.++We+apologize+for+any+inconvenience+this+may+have+caused+and+are+working+to+ensure+that+we+can+alert+you+immediately+of+any+unplanned+periods+of+downtime+or+disruption+in+the+future.""","[-1.48871,53.381391]"
"""University_of_Glasgow""","""The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature: Urban Geography: Vol 0, No 0""","""KEYWORDS: Spatial segregation ,  suburbanisation ,  poverty ,  decentralisation ,  deconcentrataion\nIntroduction\nFor most cities in early-industrialising countries, suburbanisation initially occurred through the movement of more affluent groups to the suburbs, taking advantage of the expansion of public and private transport from the late nineteenth century onwards. As a result of this selective out-migration, low income groups tended to become over-represented in older inner urban locations with higher density housing close to the industrial core. Post-war reconstruction, in European welfare states in particular, led to some spatial redistribution of poverty to new social housing estates on the edge of the built-up areas. The general pattern, however, remained one of affluence further out where the environment was cleaner, and the neighbourhoods more socially “selective”. Poverty and urban deprivation became primarily “inner city” problems (Robson, 1988 Robson, B. (1988). Those inner cities: Reconciling the social and economic aims of urban policy. Oxford: Clarendon Press.  [Google Scholar] ).\nIn recent decades, there have been signs in a number of countries of a gradual shift away from this situation, described as the “suburbanisation of poverty”; for the US, Kneebone and Berube ( 2014 Kneebone, E., & Berube, A. (2014). Confronting suburban poverty in America. Washington: Brookings Institute.  [Google Scholar] ), Cooke ( 2010 Cooke, T. J. (2010). Residential mobility of the poor and the growth of poverty in inner-ring suburbs. Urban Geography, 31(2), 179–193. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) and Cooke and Denton ( 2015 Cooke, T. J., & Denton, C. (2015). The suburbanization of poverty? An alternative perspective. Urban Geography, 36(2), 300–313. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ); for Toronto in Canada, Hulchanski et al. ( 2007 Hulchanski, J. D., Bourne, L. S., Egan, R., Fair, M., Maaranen, R., Murdie, R., & Walks, A. (2007). The three cities within Toronto: Income polarization among Toronto’s neighbourhoods, 1970-2005. Toronto: Cities Centre.  [Google Scholar] ); for Australia, Pawson, Hulse, and Cheshire ( 2015 Pawson, H., Hulse, K., & Cheshire, L. (2015) Addressing concentrations of disadvantage in urban Australia. AHURI Final Report No. 247. Sydney: AHURI.  [Google Scholar] ) and Randolph and Tice ( 2017 Randolph, B, & Tice, A. (2017). Relocating disadvantage in five australian cities: socio-spatial polarisation under neo-liberalism. Urban Policy And Research, 35(2), 103-21. doi:10.1080/08111146.2016.1221337 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ); for England, Hunter ( 2014 Hunter, P. (2014). Poverty in suburbia: A Smith Institute study into the growth of poverty in the suburbs of England and wales. London: Smith Institute.  [Google Scholar] ); and for the Netherlands, Hochstenbach and Musterd ( 2017 Hochstenbach, C., & Musterd, S. (2017). Gentrification and the suburbanization of poverty: Changing urban geographies through boom and bust periods. Urban Geography. [Taylor & Francis Online]   [Google Scholar] ). The primary driver of change has been the fundamental shift in urban economies and labour markets under globalisation (Smith, 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ). More recently, the dismantling of social protection systems under neo-liberal regimes, including reductions in social housing, is further accelerating the change through the recommodification of housing stocks (Musterd, Marcinczak, Van Ham, & Tammaru, 2016 Musterd, S., Marcinczak, S., Van Ham, M., & Tammaru, T. (2016). Socioeconomic segregation in European capital cities: Increasing separation between poor and rich. Urban Geography, 1–22. doi:10.1080/02723638.2016.1228371. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Taylor-Gooby, 2013 Taylor-Gooby, P. (2013). The double crisis of the welfare state and what we can do about it. Basingstoke: Palgrave. [Crossref]   [Google Scholar] ). If these trends continue, the logical outcome will be, as Ehrenhalt ( 2012 Ehrenhalt, A. (2012). The great inversion and the future of the American city. New York: Knopf.  [Google Scholar] ) neatly expressed it, an “urban inversion” – cities with affluent and exclusive cores, where lower income groups have been driven out.\nThese shifts raise a number of issues. On the one hand, there are questions about whether we view the changes as processes of disruption and displacement for low income groups, during which valued communities are being destroyed, or whether they arise more through voluntary shifts which reflect positive (albeit constrained) choices (Smith, 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ; Van Gent, 2013 van Gent, W. P. C. (2013). Neoliberalization, housing institutions and variegated gentrification: How the ‘Third Wave’ broke in Amsterdam. International Journal of Urban and Regional Research, 37(2), 503–522. [Crossref] , [Web of Science ®]   [Google Scholar] ). With many former industrial cities marked by high levels of vacant and derelict land around the urban core, there is some scope for re-population without the displacement of existing communities but the extensive literature on gentrification suggests that that is the exception rather than the rule (Smith 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ; van Gent 2013 van Gent, W. P. C. (2013). Neoliberalization, housing institutions and variegated gentrification: How the ‘Third Wave’ broke in Amsterdam. International Journal of Urban and Regional Research, 37(2), 503–522. [Crossref] , [Web of Science ®]   [Google Scholar] ). Relatedly, there are questions about the implications of suburbanisation for the future welfare of low income groups: whether moves to lower density, less central locations bring more opportunities or costs, for example, in relation to access to employment, especially for those reliant on public transport (Hulchanski et al., 2007 Hulchanski, J. D., Bourne, L. S., Egan, R., Fair, M., Maaranen, R., Murdie, R., & Walks, A. (2007). The three cities within Toronto: Income polarization among Toronto’s neighbourhoods, 1970-2005. Toronto: Cities Centre.  [Google Scholar] ; Pawson et al., 2015 Pawson, H., Hulse, K., & Cheshire, L. (2015) Addressing concentrations of disadvantage in urban Australia. AHURI Final Report No. 247. Sydney: AHURI.  [Google Scholar] ); and whether these moves offer lower income households access to the better services of the suburbs such as higher quality schools, or a better environment with lower air pollution (reference removed for review), or strand them in locations where the provision of basic social services is lacking and civic organisations are weak (Cooke & Denton, 2015 Cooke, T. J., & Denton, C. (2015). The suburbanization of poverty? An alternative perspective. Urban Geography, 36(2), 300–313. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). In some countries, at least, there are challenges in re-orienting anti-poverty policies and infrastructures designed around problems of denser inner-urban locations to new settings with very different contexts (Kneebone & Berube, 2014 Kneebone, E., & Berube, A. (2014). Confronting suburban poverty in America. Washington: Brookings Institute.  [Google Scholar] ).\nMore immediately, the challenge for researchers is to capture the scale and nature of the changes, the processes which underpin them, and the new urban forms which are emerging. Some important questions include: is poverty suburbanising in all cities, and to the same extent? Where in the urban areas is poverty moving to? How do we best capture the changes, given that we lack standard definitions of core and suburb, and that cities take an increasing variety of spatial forms? Additionally, is poverty in the inner cities being diluted through the in-migration of non-poor groups but largely without displacement, or is the increase in the non-poor population occurring at the expense of those in poverty? Finally, do the processes lead to a more even distribution of poverty and a reduction in segregation overall, or to new forms of segregation and the re-concentration of poverty in some suburban locations?\nThe aim of the paper is to analyse the extent and nature of the suburbanisation of poverty in the major cities of England and Scotland. First, we look at overall change for each city using measures of both centralisation and concentration to cope with problems of polycentricity and heterogeneous urban forms. Second, we examine changes in the numbers of poor and non-poor in different locations to assess whether any suburbanisation of poverty is merely a relative one, occurring through dilution of poverty in inner locations as these areas are re-populated, or an absolute one, suggesting displacement of lower income by higher income groups. Third, we examine the relationship between changes in decentralisation, deconcentration, and spatial segregation or unevenness – the extent to which poor and non-poor tend to live in the same neighbourhoods regardless of where in the city these are located.\nBackground\nDrivers of poverty suburbanisation\nCities appear to be undergoing quite fundamental transformations in spatial organisation under the impact of a related set of economic, political and social processes: rising income and wealth inequalities, reflecting in part the impacts of globalisation and economic restructuring; parallel political shifts in welfare and housing policies under neo-liberal political regimes; and social changes of deferred family formation as well as cultural preferences for urban living (Atkinson, 2014 Atkinson, A. B. (2014). After piketty? British Journal of Sociology, 65(4), 619–638. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ; Musterd et al., 2016 Musterd, S., Marcinczak, S., Van Ham, M., & Tammaru, T. (2016). Socioeconomic segregation in European capital cities: Increasing separation between poor and rich. Urban Geography, 1–22. doi:10.1080/02723638.2016.1228371. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ; Peck, 2014 Peck, J. (2014). Pushing austerity: State failure, municipal bankruptcy and the crises of fiscal federalism in the USA. Cambridge Journal of Regions, Economy and Society, 7(1), 17–44. [Crossref] , [Web of Science ®]   [Google Scholar] ). Rising inequality feeds through into the urban system, both in an increased desire for spatial distance to reflect socio-economic distance (Musterd et al., 2016 Musterd, S., Marcinczak, S., Van Ham, M., & Tammaru, T. (2016). Socioeconomic segregation in European capital cities: Increasing separation between poor and rich. Urban Geography, 1–22. doi:10.1080/02723638.2016.1228371. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ), but also because of the increasing ability of higher income groups to outbid lower income ones in the housing market for desired neighbourhoods (Hulchanski et al., 2007 Hulchanski, J. D., Bourne, L. S., Egan, R., Fair, M., Maaranen, R., Murdie, R., & Walks, A. (2007). The three cities within Toronto: Income polarization among Toronto’s neighbourhoods, 1970-2005. Toronto: Cities Centre.  [Google Scholar] ; Randolph & Tice, 2017 Randolph, B, & Tice, A. (2017). Relocating disadvantage in five australian cities: socio-spatial polarisation under neo-liberalism. Urban Policy And Research, 35(2), 103-21. doi:10.1080/08111146.2016.1221337 [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ). Related to this is the continued restructuring of urban economies and labour markets under the dual influences of technological change and globalisation. Urban labour markets have seen a loss of employment opportunities for skilled manual workers, particularly in more central urban locations, alongside the growth in professional and other white collar occupations. Whether these changes are described as polarisation, or professionalisation (Hamnett, 1998 Hamnett, C. (1998). Social polarisation, economic restructuring and welfare state regimes. In S. Musterd & W. Ostendorf (eds), Urban segregation and the welfare state: Inequality and exclusion in Western cities (pp. 15–27). London: Routledge.  [Google Scholar] ; Sassen, 1991 Sassen, S. (1991). The global city: New York, London, Tokyo. Princeton: Princeton UP.  [Google Scholar] ), there is a relative decline in the market position of lower income groups.\nWhile structural economic shifts help explain the rising opportunity for higher income groups to move into central urban areas, other factors are needed to explain why they choose to take this up. Cultural explanations provide part of the answer, although they do not stand apart from economic factors. Urban living has been promoted by those interested in the recommodification and redevelopment of inner urban locations (Smith, 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ; Urban Task Force, 1999 Urban Task Force. (1999). Towards an urban rennaissance. London: E & FN Spon.  [Google Scholar] ). The latter includes both private interests but also the increasingly “entrepreneurial” urban governments seeking to maintain or re-build the economic base of their cities (Harvey 1989 Harvey, David. (1989). From managerialsim to entrepreneurialism: the transformation in urban governance in late capitalism. Geografiska Annaler, 71 B (1): 3–17.  [Google Scholar] ; Peck, 2014 Peck, J. (2014). Pushing austerity: State failure, municipal bankruptcy and the crises of fiscal federalism in the USA. Cambridge Journal of Regions, Economy and Society, 7(1), 17–44. [Crossref] , [Web of Science ®]   [Google Scholar] ). Other factors include the demographic shift of deferred fertility and hence the rising number of young adults in childless households (Castles, 2003 Castles, F. G. (2003). The world turned upside down: Below replacement fertility, changing preferences and family-friendly public policy in 21 OECD countries. Journal of European Social Policy, 13(3), 209–227. [Crossref] , [Web of Science ®]   [Google Scholar] ), for whom inner urban locations offer many advantages.\nPolitics and policy are implicated at a range of levels. Rising inequality reflects in part changes in labour market regulation and social protection measures (Atkinson, 2014 Atkinson, A. B. (2014). After piketty? British Journal of Sociology, 65(4), 619–638. [Crossref] , [PubMed] , [Web of Science ®]   [Google Scholar] ). More directly, urban policy has played a significant role in facilitating – and occasionally limiting – processes of gentrification (Bailey & Robertson, 1997 Bailey, N., & Robertson, D. (1997). Housing renewal, urban policy and gentrification. Urban Studies, 34(4), 561–578. [Crossref] , [Web of Science ®]   [Google Scholar] ; Smith, 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ). In the 1980s and 1990s, efforts to stem urban population loss and drive re-investment in declining urban areas were promoted by governments as interventions that would benefit existing (and especially poorer) residents by increasing economic opportunities and promoting more mixed communities (Schoon, 2001 Schoon, N. (2001). The chosen city. London: Spon.  [Google Scholar] ; Urban Task Force, 1999 Urban Task Force. (1999). Towards an urban rennaissance. London: E & FN Spon.  [Google Scholar] ). Others criticised these efforts as state-led gentrification or neo-liberal urbanism, designed to displace working class communities for the pursuit of capital and middle class interests (Smith, 2002 Smith, N. (2002). New globalism, new urbanism: Gentrification as global urban strategy. Antipode, 34(3), 427–450. [Crossref] , [Web of Science ®]   [Google Scholar] ; Hochstenbach and Musterd 2017 Hochstenbach, Cody, & Musterd, Sako. (2017). Gentrification and the suburbanization of poverty: changing urban geographies through boom and bust periods. doi: 10.1080/02723638.2016.1276718 [Taylor & Francis Online]   [Google Scholar] ). In part, judgements about the impact of these programmes hinge on the eventual outcome: whether it is the development of stable mixed communities, still providing access to housing for low-income groups; or transitory mixing, before the eventual displacement of low-income groups, leading eventually to exclusive upper- or middle-class occupation of the urban cores. Displacement here is understood in the broad sense of both direct replacement of poor housholds by non-poor, but also indirect effects where poor households lose the ability to access a neighbourhood due to rising housing costs (Marcuse, 1985 Marcuse, P. (1985). Gentrification, abandonment and displacement: Connections, causes and policy responses in New York City. Journal of Urban and Contemporary Law, 28, 195–240.  [Google Scholar] ).\nWider welfare and housing policies play a role in the pace and form of change (Musterd and Ostendorf 1998 Musterd, Sako, & Ostendorf, Wim. (1998). Urban segregation and the welfare state: inequality and exclusion in western cities. London: Routledge. [Crossref]   [Google Scholar] ). Many countries have seen the progressive erosion in welfare benefit levels, with an acceleration since the Global Financial Crisis of 2007/8, which has been used to legitimise the rolling back of the state (Taylor-Gooby, 2013 Taylor-Gooby, P. (2013). The double crisis of the welfare state and what we can do about it. Basingstoke: Palgrave. [Crossref]   [Google Scholar] ). Housing policy decisions are especially important, with investment in de-commodified social housing limitting the scope for market-led restructuring (Bailey & Robertson, 1997 Bailey, N., & Robertson, D. (1997). Housing renewal, urban policy and gentrification. Urban Studies, 34(4), 561–578. [Crossref] , [Web of Science ®]   [Google Scholar] ), whereas efforts to recommodify housing have the opposite effect (Forrest & Hirayama, 2015 Forrest, R., & Hirayama, Y. (2015). The financialisation of the social project: Embedded liberalism, neoliberalism and home ownership. Urban Studies, 52(2), 233–244. [Crossref] , [Web of Science ®]   [Google Scholar] ). In the UK, key policy changes from the 1980s and 1990s have been towards re-commodification, notably through the sale of social housing to tenants under the Right-to-Buy, and the encouragement of investment in private renting through the deregulation of tenancies and of lending institutions (Kemp, 2010 Kemp, P. A. (2010). The transformation of private renting. In P. Malpass & R. Rowlands (eds), Housing, markets and policy (pp. 122–142). London: Routledge.  [Google Scholar] ; Malpass, 2005 Malpass, P. (2005). Housing and the welfare state: The development of housing policy in Britain. Basingstoke: Palgrave Macmillan.  [Google Scholar] ).\nSince 2007/8, the UK has seen successive moves to reduce entitlements to welfare and housing subsidies for tenants, particularly in the newly-expanded private rented sector. There are tighter limits on the rent levels which can be covered by Housing Benefits, caps on the total amount of benefits which a household can receive in one year including housing subsidies, and reduced entitlements for younger adults to housing subsidies (Department for Work and Pensions (DWP), 2014 Department for Work and Pensions (DWP) (2014) The impact of recent reforms to Local Housing Allowances: summary of key findings. Research Report No. 874. London: DWP.  [Google Scholar] ). These changes are explicitly intended to make the recipient households more “cost-conscious” and hence to drive changes in consumption patterns, as well as increasing incentives to find paid work or extend working hours. Early evaluations of the impact of these changes have shown particular pressures in the highest cost locations such as central London driving outward moves of poorer households (Department for Work and Pensions (DWP), 2014 Department for Work and Pensions (DWP) (2014) The impact of recent reforms to Local Housing Allowances: summary of key findings. Research Report No. 874. London: DWP.  [Google Scholar] ).\nAt the other end of the suburbanisation process, there is an important literature which has examined the growing differentiation within suburban locations and the emergence of decline within a particular sub-group of these places. Terms may vary but the broad details are clear: these are older inner suburbs, often from the post-war years where housing is becoming obsolescent compared with newer suburban developments, but reinvestment is more sporadic and uncoordinated than in the urban core. Many of these locations suffer a combination of social problems (high unemployment and crime, for example) and poor infrastructure and public services (Randolph and Freestone 2012 Randolph, Bill, & Freestone, Robert. (2012). Housing differentiation and renewal in middle-ring suburbs: the experience of sydney, australia. Australia, Urban Studies, 49(12), 2557-75. doi:10.1177/0042098011435845 [Crossref] , [Web of Science ®]   [Google Scholar] ; Hanlon 2010 Hanlon, Bernadette. (2010). Once the american dream: inner ring suburbs of the metropolitan united states. Philadelphia, PA: Temple University Press.  [Google Scholar] ) which are often associated with inner urban locations.\nWhile the broad drivers of the suburbanisation of poverty are clear, we would of course expect to see some degree of variation between places, as processes are historically and institutionally contingent as the evolutionary economics literature highlights (Boschma & Frenken, 2006 Boschma, R. A., & Frenken, K. (2006). Why is economic geography not an evolutionary science? Towards an evolutionary economic geography. Journal of Economic Geography, 6(3), 273–302. [Crossref] , [Web of Science ®]   [Google Scholar] ). Cooke and Denton ( 2015 Cooke, T. J., & Denton, C. (2015). The suburbanization of poverty? An alternative perspective. Urban Geography, 36(2), 300–313. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) show variations in patterns of poverty decentralisation between the major US metropolitan areas. In a closely-related study of spatial segregation in European cities, Musterd et al. ( 2016 Musterd, S., Marcinczak, S., Van Ham, M., & Tammaru, T. (2016). Socioeconomic segregation in European capital cities: Increasing separation between poor and rich. Urban Geography, 1–22. doi:10.1080/02723638.2016.1228371. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] ) show variation in both pace and direction of change in recent years, and relates this to a number of structural characteristics including the strength of ties to the global economy and the nature of the local welfare regime.\nIn the UK context, we would expect various factors to influence the pace of change. First there is city size, positively related both to agglomeration benefits and disbenefits, but also to the physical separation and hence commuting costs that go with suburban residence. We would expect pressures to be greater in the larger conurbations, with London a clear outlier in the UK context. Second, there is the state of the local economy which translates into both wage levels and housing costs, and hence affordability for lower income households. Affordability indices for England show wide variations between cities with particular problems again in London which contains seven of the ten local authorities with the least affordable housing in 2016 (Office for National Statistics (ONS), 2017 Office for National Statistics (ONS). (2017). Housing affordability in England and Wales: 1997 to 2016. London: ONS.  [Google Scholar] ). It is in the higher cost locations that the cuts and caps in Housing Benefits will also have most bite (Department for Work and Pensions (DWP), 2014 Department for Work and Pensions (DWP) (2014) The impact of recent reforms to Local Housing Allowances: summary of key findings. Research Report No. 874. London: DWP.  [Google Scholar] ).\nThe relationship between suburbanisation and spatial segregation or unevenness may not be simple. While, in general, we might expect that cities will ultimately become more segregated as poverty suburbanises – Ehrenhalt’s “urban inversion” thesis – it is possible that the processes bring at least a temporary reduction in segregation as inner urban neighbourhoods become more mixed during a transitional stage. Bailey, Van Gent, and Musterd ( 2017 Bailey, N., van Gent, W., & Musterd, S. (2017). Re-making urban segregation: Processes of income sorting and neighbourhood change. Population, Space and Place, 23(3). [Crossref] , [Web of Science ®]   [Google Scholar] ) show exactly this kind of change at work in Amsterdam during the early 2000s, for example. It is also possible in theory for decentralisation or suburbanisation to be associated with a dispersal of poverty and hence a longer-term reduction in segregation rather than new concentrations emerging in suburban locations although, in practice, this seems highly unlikely given that spatial inequalities have been almost the defining feature of modern cities.\nConceptual and measurement challenges\nThe term “suburbanisation” implies movement to areas with a particular built form and density, but it also carries important connotations of class and lifestyle, most notably of safety and conformity, in contrast to the “dangerous” or “degenerate” inner cities (Kneebone & Berube, 2014 Kneebone, E., & Berube, A. (2014). Confronting suburban poverty in America. Washington: Brookings Institute.  [Google Scholar] ). Suburbanisation therefore carries suggestions not just of loss or displacement, but also new opportunity. In most locations, however, we lack clear definitions of the “suburb”. In the UK, Hunter ( 2014 Hunter, P. (2014). Poverty in suburbia: A Smith Institute study into the growth of poverty in the suburbs of England and wales. London: Smith Institute.  [Google Scholar] ) uses a combination of housing type (low proportions of flats or terraced housing, to try to exclude inner urban locations) and density (above a minimum threshold, to exclude rural areas) as the basis of his analysis. Such an approach is open to a number of challenges over the selection of cut-off points. Others have focussed on density alone (Cooke & Denton, 2015 Cooke, T. J., & Denton, C. (2015). The suburbanization of poverty? An alternative perspective. Urban Geography, 36(2), 300–313. [Taylor & Francis Online] , [Web of Science ®]   [Google Scholar] , for example), offering more transparency but reducing suburbanisation to deconcentration.\nAn alternative is to focus centrality or distance from the city centre. This places a greater emphasis on physical location and, in particular, distance from and accessibility to the urban core. It foregrounds to a greater extent concerns about loss of access to the centre and the opportunities which that might offer, most notably for employment. Identification of the central location from which to measure distance remains a minor issue although empirical testing in relation to measures of decentralisation suggests it has minimal impact (Kavanagh, Lee, & Pryce, 2016 Kavanagh, L., Lee, D., & Pryce, G. (2016) Is poverty decentralising? Quantifying uncertainty in the decentralisation of urban poverty, Annals of the American Association of Geographers : 1-12. 106 135-1298 doi:10.1080/24694452.2016.1213156 [Taylor & Francis Online]   [Google Scholar] ). In the traditional monocentric city, there is a strong relationship between centrality and density, so the choice between deconcentration and decentralisation measures may be relatively unimportant but the growing polycentricity of many urban areas means that movement away from the main centre does not always represent movement to more suburban settings. Capturing suburbanisation through a combination of decentralisation and deconcentration measures is therefore important, as is an understanding of the relationship between centrality and density for each city.\nA related measurement issue is how we define our cities. If the boundary is drawn too tightly, it will exclude many areas which function as suburbs of the central city. In the UK context at least, administrative boundaries are highly problematic in this regard due to the very variable basis on which these have emerged, with significant under-bounding in many cases; the contrast between the city authorities for Leeds and Manchester is particularly stark. Physical built-up areas are little better, particularly in contexts like the UK, where planning policies have sought to contain outward sprawl through Green Belts, pushing urban overspill into physically-separated but functionally-connected settlements. City-regions or travel-to-work areas (TTWAs) offer a better basis for analysis since they are constructed from commuting flow data that reflect functional urban/regional relationships. However, the inclusion of surrounding settlements again means that the relationships between centrality and density may be more complex. Indeed, some TTWAs in the UK are composed of two or more urban centres of similar size, forming polycentric urban regions. As before, the solution is to pay greater attention to deconcentration in more polycentric contexts.\nA last issue concerns how it is that any decentralisation or deconcentration of poverty comes about. If there is increasing housing supply in inner or denser areas which is taken up by non-poor groups, decentralisation and deconcentration of poverty can occur without any reduction in the absolute numbers in poverty in these locations. In such cases, suburbanisation is a relative process as poverty is “diluted” in inner or denser neighbourhoods without displacement. High levels of vacant and derelict land in many former industrial centres make this a real possibility. Alternatively, change can occur through absolute suburbanisation if inner or denser areas see reductions in the number of people in poverty able to live there. To capture these processes, we need to move beyond global measures of the distribution of one group relative to another, and look at the distribution of each group separately.\nData and methods\nPoverty by neighbourghood\nIn the UK, each of the four national governments publishes area deprivation measures for small geographic areas through their official Indices of Multiple Deprivation (IMD). The IMDs have been constructed on a consistent basis since 2004 by compiling data from a wide range of administrative sources (for details, see Noble, Wright, Smith, & Dibben, 2006 Noble, M., Wright, G., Smith, G., & Dibben, C. (2006). Measuring multiple deprivation at the small-area level. Environment and Planning A, 38(1), 169–185. [Crossref] , [Web of Science ®]   [Google Scholar] ). They are widely used by government in policy analyses and resource allocation decisions, as well as by many researchers. The English IMDs (EIMDs) were updated in 2004, 2007, 2010 and 2015, while the Scottish IMDs (SIMD) were updated in 2004, 2006, 2009, 2012 and 2016.\nThe spatial units for the EIMD are called Lower Super Output Areas (LSOAs), designed to have populations between 1000 and 2000. Relatively minor adjustments were made to LSOA boundaries for the 2015 index to reflect population changes, with a small minority of LSOAs split or merged (and just 0.5 per cent seeing more complex changes). We re-apportion data for the earlier years to the 2015 boundaries based on the number of postcodes found in each area. 1 1.  This was done on the basis of the distribution of unit postcodes, using a lookup file kindly provided by Dr Paul Norman, Leeds University. View all notes\nThe analogous units for the SIMD are termed Datazones (DZ) with considerably smaller populations, between 500 and 1000. Boundaries were comprehensively updated in 2016 to reflect population changes. Data for earlier years can be re-aggregated to the new boundaries on the same basis as in England although the more complex boundary changes mean this leads to a lower quality fit. For measures of centralisation or concentration, we would expect this to have minimal impact. The centrality or density of neighbouring areas will usually be very similar, certainly with the former. Reallocations between neighbouring areas through boundary changes will therefore have limited impacts on overall measures of centralisation or concentration so results for these measures are all presented using current boundaries. (When inspecting trends across all the years with these data, we see no signs of discontinuity between years using reaggregated data and the subsequent year.) For measures of spatial segregation, reaggregation can effectively move populations between neighbourhoods with quite contrasting concentrations of poverty or disadvantage; Glasgow in particular has been shown to have a much more fragmented social geography than comparably deprived English cities making the risks here particularly great (Livingston et al., 2011 Livingston, M., Bailey, N., Walsh, D., Whyte, B., Cox, C., & Jones, R. (2011). O1-1.4 The patterning of deprivation and its effects on health outcomes in three post industrial cities in Britain. Journal of Epidemiology and Community Health, 65(Suppl 1), A8–A9. [Crossref] , [PubMed]   [Google Scholar] ). As a result, reaggregation for the Scottish Datazones may lead to significant error, rendering comparisons between measures based on old and new boundaries problematic. (Inspecting trends across the years, we see a sharp discontinuity when moving from reaggregated data to the latest year.) When looking at spatial segregation for the two Scottish cities, we therefore base our analysis on the original (2001) Datazone boundaries, making 2012 the latest available time point.\nAs part of the overall measure of area deprivation, both IMDs construct a measure of Income Deprivation and it is this which forms the basis for our analysis. This is a low-income poverty measure, capturing the proportion of people living in households in receipt of a national government welfare benefit or tax credit by virtue of their low-income. Importantly for our work, this measure covers not just the unemployed or those unable to work through long-term sickness or disability, as well as the low-income retired, but also people in employment who are still on low income. The UK government’s analysis shows that more than half of people in low-income poverty in the UK live in households where there is at least one person in paid work (Department for Work and Pensions (DWP), 2017 Department for Work and Pensions (DWP) (2017) Households below average incomes (HBAI) quality and methodology report 2015/16. London: DWP.  [Google Scholar] ); that study uses a household survey measure, defining low income poverty as below 60 per cent of the median (equivalised) household income. There are some minor differences between EIMD and SIMD Income Deprivation measures although they are quite comparable (for details, see Noble et al., 2006 Noble, M., Wright, G., Smith, G., & Dibben, C. (2006). Measuring multiple deprivation at the small-area level. Environment and Planning A, 38(1), 169–185. [Crossref] , [Web of Science ®]   [Google Scholar] ; Payne & Abel, 2012 Payne, R. A., & Abel, G. A. (2012). UK indices of multiple deprivation: A way to make comparisons across constituent countries easier. Health Statistics Quarterly, 53(Spring), 1–16.  [Google Scholar] ). Both measures very largely exclude full-time students as they are not eligible for the majority of the benefits on which the indices are based.\nThe detailed construction of both measures has changed over time as the welfare benefits and tax credit systems have evolved. These measures should not therefore be used to draw conclusions about how the absolute level of poverty in a neighbourhood or city has changed over time. In our analyses, we avoid this problem by focussing on the relative distribution of poverty across the cities at each time point. In effect, the changes in definition mean that we use a slightly different threshold for our poverty measure at each time point but there is little reason to expect this small variation will be strongly related to the geography of poverty within each city. 2 2.  All code used to perform the analyses are freely available from the following web address: https://osf.io/wtsxu/ . View all notes\nCategorisation of urban regions\nIn this paper, cities or urban regions are defined on the basis of official TTWAs, as updated in 2016 to reflect commuting patterns in the 2011 Census (Office for National Statistics (ONS), 2015 Office for National Statistics (ONS). (2015). Methodology note on 2011 Travel-to-Work Areas. London: ONS.  [Google Scholar] ). In general, residential moves within TTWAs should represent moves without a change in labour market area. Our selection of centres for each TTWA involved visual inspection of the central point suggested by typing in the name of a TTWA’s central city or town into an on-line mapping tool. The LSOA/DZ which contained this central point and contiguous LSOAs/DZs, were considered possible candidates for the central zone of the TTWA. The judgement about which to define as the centre was influenced by factors such as the presence of city halls, main shopping streets or central railway stations within the zone. The central point from which centrality was calculated was the geographic centroid of this central LSOA/DZ. Earlier analyses have shown that using different plausible definitions of the central areal unit and point, or using alternative cut-offs for city limits had no substantive influence on measures of relative centralisation (Kavanagh et al., 2016 Kavanagh, L., Lee, D., & Pryce, G. (2016) Is poverty decentralising? Quantifying uncertainty in the decentralisation of urban poverty, Annals of the American Association of Geographers : 1-12. 106 135-1298 doi:10.1080/24694452.2016.1213156 [Taylor & Francis Online]   [Google Scholar] ). The code and data used to produce all analyses are publically available (see endnote 2) and so readers are free to explore the effect of using different choices of centre.\nWe have used the largest 25 TTWAs by population as our set of cities, avoiding any other selection criteria. 3 3.  We make one change to the list, merging the London TTWA with a new TTWA first identified in 2016, covering Slough & Heathrow. This area to the west of London centres on the major employment centre formed around the airport and its related industries. However, it is continuous with the western suburbs of London and, while commuting flows may be sufficient to reach the threshold for self-containment, it clearly still forms part of the London housing market. View all notes Table 1 shows their populations in 2009/10 and poverty rates (Income Deprivation) for the first and last year. The latter are not to indicate change in poverty levels over time given the limitations discussed above but to show that the Income Deprivation measures offer a relatively consistent threshold over time; the changes over time within each city are much smaller than the differences between cities. Income Deprivation ranges from less than 10 per cent in some of the smaller cities of the south of England to 20 per cent or more for the larger cities.\nThe suburbanisation of poverty in British cities, 2004-16: extent, processes and nature\nAll authors\n""","0.13003777","""http://www.tandfonline.com/doi/full/10.1080/02723638.2017.1405689""","[-4.28836,55.871751]"
"""Imperial_College_London""","""Variations in population health status: results from a United Kingdom national questionnaire survey | The BMJ""","""Variations in population health status: results from a United Kingdom national questionnaire survey\nPapers\nVariations in population health status: results from a United Kingdom national questionnaire survey\nBMJ 1998; 316 doi: https://doi.org/10.1136/bmj.316.7133.736 (Published 07 March 1998) Cite this as: BMJ 1998;316:736\nPaul Kind (pk1{at}york.ac.uk), senior research fellow,\nPaul Dolan, research fellow,\nAlan Williams, professor of economics\nCentre for Health Economics, University of York, York YO1 5DD\nCorrespondence to: Dr Kind\nAccepted 31 October 1997\nAbstract\nObjective: To measure the health of a representative sample of the population of the United Kingdom by using the EuroQoL EQ-5D questionnaire.\nDesign: Stratified random sample representative of the general population aged 18 and over and living in the community.\nSetting: United Kingdom.\nSubjects: 3395 people resident in the United Kingdom.\nMain outcome measures: Average values for mobility, self care, usual activities, pain or discomfort, and anxiety or depression.\nResults: One in three respondents reported problems with pain or discomfort. There were differences in the perception of health according to the respondent's age, social class, education, housing tenure, economic position, and smoking behaviour.\nConclusions: The EQ-5D questionnaire is a practical way of measuring the health of a population and of detecting differences in subgroups of the population.\nKey messages\nMeasurement of health outcome requires the observation of states of health\nPatients' involvement in recording and assessing their own state of health is a major element in the process of evaluating the impact of health care\nThe EuroQoL EQ-5D questionnaire highlights variations in states of health which are consistent with previously published results\nHigh degrees of pain are reported in the general population. A category for pain is absent and thus undetected in the survey of disability by the Office of Population Censuses and Surveys\nIntroduction\nThe measurement of health is central to the evaluation of health care. By observing the extent of changes in health the benefits and disbenefits of health care for both patients and groups of patients can be evaluated; over the past 25 years several generic measures of health have been developed for use in this way. 1 – 8 These instruments were designed for use as general purpose measures of health, independent of diagnostic categorisation or disease severity. Information based on such measures is useful for establishing the degrees of morbidity in the community, enabling different population subgroups to be compared, which would help in assessing health needs or in informing those responsible for allocating health resources. Periodic reassessment of health could provide important data on the extent of any changes in the health of a population—for example, the extent to which the population is achieving national targets for health. If such standardised information was also routinely collected on individual patients it would provide a simple means of evaluating the outcomes of their health care.\nWe report on a study in which the EuroQoL EQ-5D questionnaire 9 was fielded in a survey of the population of the United Kingdom, conducted as part of a wider study of practical ways of measuring health related quality of life. 10\nSubjects and methods\nEQ-5D questionnaire\nThe EQ-5D questionnaire is a generic measure of health status developed by the EuroQoL Group, an international research network established in 1987 by researchers from Finland, the Netherlands, Sweden, and the United Kingdom. The EQ-5D questionnaire defines health in terms of five dimensions: mobility, self care, usual activities (work, study, housework, family, or leisure), pain or discomfort, and anxiety or depression. Each dimension is subdivided into three categories, which indicate whether the respondent has no problem, a moderate problem, or an extreme problem (appendix). Combinations of these categories define a total of 243 health states. The EQ-5D questionnaire comprises two pages; on the first page respondents record the extent of their problem in each of the five dimensions and on the second page they record their perception of their overall health on a visual analogue scale (0 denoting the worst imaginable health state and 100 denoting the best imaginable health state). The validity and reliability of the EQ-5D questionnaire have been tested, 11 – 13 as has its application in a range of patient groups. 14 – 16 Since the original survey reported here, the EQ-5D questionnaire has been fielded in three national surveys, including the English national health survey—an interview-based survey of about 16 000 people. The EQ-5D questionnaire has also been used in population surveys in Spain, Germany, and Canada.\nSurvey design and methods\nMembers of the public aged 18 and over were interviewed as part of a national survey. No upper age limit was stipulated. The sample was based on addresses in England, Scotland, and Wales, selected by postcode. 17 Eighty postcode areas were chosen, proportionately to the number of addresses in each area, after these areas had been stratified by regional health authority, socioeconomic group, and population density. Seventy six addresses were selected from each postcode area, yielding a total of 6080 addresses. At each of these addresses one adult aged 18 or over was selected using a Kish grid. 18 Individuals in institutions, hostels, care homes, or bed and breakfast accommodation were excluded from the sample. Of the selected addresses, 12% were unproductive as they were non-residential, empty, or untraceable. The final sample comprising 3395 subjects was representative of the general population with respect to age, sex, and social class. During the interview, respondents completed the EQ-5D questionnaire and provided information on age, sex, marital state, education, employment, housing tenure, and smoking behaviour. The interviews took place during the last quarter of 1993.\nAnalysis mainly compared the differences between the population subgroups. It was hypothesised that more health problems would be reported with increasing age, with lower social class, for those registered sick or disabled, and for smokers. χ2 Tests were used for the analysis of the descriptive profile data, and Student's t test was used to test for subgroup differences in the visual analogue scale data.\nResults\nA moderate problem on at least one dimension was reported by 42% of respondents, whereas only 6% of respondents reported any extreme problem (table 1 ). Problems were most often recorded in the pain or discomfort dimension. In subsequent analyses, moderate and extreme categories of each dimension were combined.\nTable 1\nNumbers (percentages) of respondents reporting a problem in each EuroQoL dimension\nView this table:\nDownload powerpoint\nDiscussion\nThis survey provides an important insight into the health status of the population of the United Kingdom at any one time. Although extreme problems with mobility and self care were rarely reported in this survey, there was a high level of reported problems with pain or discomfort. Over 50% of respondents aged ≥70 and about 20% of the youngest respondents reported some problem in this dimension. This finding has important implications. Pain does not seem to be a dimension of interest in a national disability survey despite being widely experienced in the community. The omission of a pain category means that it is assigned a zero weight, despite good evidence that it has a powerful influence on society's valuations of states of health. 21 These factors combine to disadvantage a significant proportion of the general population.\nSignificant differences were found between population subgroups with respect to age, social class, marital status, employment, education, and smoking behaviour. These findings compare with findings reported elsewhere. 22 – 24 Disability rates based on the EuroQoL classification reflected similar trends to those seen in the general household survey and surveys of the Office of Population Censuses and Surveys, although rates in these surveys were somewhat lower as they were based on a narrower definition of disability.\nPopulation averages\nThe representativeness of the survey suggests that the results are indicative of the average health status in the general population of the United Kingdom, although it should be borne in mind that sampling was limited to individuals living in the community and tended to exclude people who had extreme problems with mobility or with self care and therefore likely to be dependent on others for their daily needs. Current investigation of specific patient groups—for example, people attending their general practice surgeries—reveals a wider distribution of reported problems. Thus, to the extent that this survey excluded people who were likely to yield responses indicating more severe problems, the results may well underestimate the health related quality of life of the general population.\nAppendix\nDownload powerpoint\nOur data can be treated as descriptive population “norms.” As such, they could provide baseline values for monitoring variations in health for specific population groups, particularly if this information was also linked to local epidemiological data. In aggregate form, such information could be used to complement national targets by providing a measure based on health status rather than mortality. The capacity of the EQ-5D questionnaire to generate quantifiable and usable information on the health status of a population led to its inclusion in the 1996 health survey for England. 25\nMeasuring outcomes\nHowever, it is the measurement of change in health status for which the need is greatest. There can be few circumstances in which healthcare workers are not concerned with the measurement of outcome, and the EQ-5D questionnaire provides the capacity to measure change in health status, and hence outcomes, in a simple standardised way. The information on self reported problems recorded on the first page of the EQ-5D questionnaire identifies a unique health status for which there is a corresponding index value based on the views of the general population. 21 Changes in health status and the value of that change can be used to quantify outcomes for clinical and economic evaluation; the latter role was recommended for the EQ-5D questionnaire in a report commissioned by the United States Department of Public Health. 26 There is “an increasing consensus regarding the centrality of the patient's point of view in monitoring medical care outcomes,” 6 and the EQ-5D questionnaire has the obvious potential to contribute to that process. The national survey data reported in this paper show what can be achieved by using an uncomplicated instrument for measuring health status. The further exploitation of its potential is open to us all.\nAcknowledgments\nSurvey work for the 1993 survey was conducted by Social and Community Planning Research, and we thank the trained fieldwork staff for their help in the collection of the data.\nContributors: All four authors shared equally in the design and execution of the research reported in this paper. Social and Community Planning Research provided significant additional expertise in the design and management of the national survey. PK will act as guarantor for the paper.\nFunding: The project was funded by the Department of Health. The views expressed are those of the authors and not necessarily of the Department of Health.\nConflict of interest: None.\n""","0.32738078","""http://www.bmj.com/content/316/7133/736""","[-0.178219,51.500505]"
"""UCL""","""Iris Publication""","""http://discovery.ucl.ac.uk/1422345/\nAbstract\nThis work investigates how data from public transport fare collection systems can be used to analyse travellers' behaviour, and transform travel information systems that urban residents use to navigate their city into personalised and dynamic systems that cater for each passenger's unique needs. In particular, we show how fare collection data can be used to identify behavioural differences between passengers: we thus advocate for a personalised approach to delivering transport related information to travellers. To demonstrate the potential for personalisation we compute trip time estimates that more accurately reflect the travel habits of each passenger. We propose a number of algorithms for personalised trip time estimations, and empirically demonstrate that these approaches outperform both a non-personalised baseline computed from the data, as well as published travel times as currently offered by the transport authority. Furthermore, we show how to easily scale the system by pre-clustering travellers. We close by outlining the wide variety of applications and services that may be fuelled by fare collection data. © 2012 Elsevier B.V. All rights reserved.\nPublication data is maintained in RPS. Visit https://rps.ucl.ac.uk\n› More search options\n""","0.76258487","""http://iris.ucl.ac.uk/iris/publication/900727/7""",
"""Brunel_University_London""","""A system for remote sighted guidance of visually impaired pedestriansBritish Journal of Visual Impairment - V. Garaj, R. Jirawimut, P. Ptasinski, F. Cecelja, W. Balachandran, 2003""","""PDF\nAbstract\nSighted guidance is arguably the most efficient method for aiding visually impaired pedestrians in mobility. A sighted guide's verbal instructions compensate comprehensively for the insufficiency of visual input in navigation. Moreover, the companionship entails sharing of responsibilities and thus increases the blind traveller's sense of security during a journey. The disadvantages of the sighted guidance are that a sighted guide may not always be available or their presence may not be desirable because it restricts personal independence. This paper presents a novel system for navigation of visually impaired pedestrians, whereby advanced technologies were combined to allow a visually impaired user to remotely access the sighted guidance service. The user can choose when and for how long to use the system. The remote sighted guidance system is enabled by the integration of a remote vision facility with the Global Positioning System, the Geographic Information System and the third generation telecommunication network. A user trial is also reported in which the contribution of the system to the mobility of a visually impaired pedestrian was assessed. The results obtained lead to the conclusion that the remote sighted guidance is potentially a highly usable mobility aid.\nEspinosa, M.A. & Ochaita, E. (1998) 'Using tactile maps to improve the practical spatial knowledge of adults who are blind'. Journal of Visual Impairment and Blindness, 92 (5), 338-45. Google Scholar\nFarmer, L.W. & Smith D.L. (1997) 'Adaptive Technology'. In B. B. Blasch, W. R. Wiener & W. R. Welsh (Eds) Foundations of Orientation and Mobility. 2nd Edition (Chapter 7, pp.231-70). New York: American Foundation for the Blind. Google Scholar\nGaraj, V. (2001) 'The Brunel Navigation System for the Blind: Determination of the appropriate position to mount the external GPS antenna on the user's body'. In Proceedings of the 2001-GPS Technical Meeting. Salt Lake City: Institute of Navigation. Google Scholar\nGuth, D.A. & Rieser, J.J. (1997) 'Perception and the control of locomotion by blind and visually impaired pedestrians'. In B. B. Blasch, W. R. Wiener & W. R. Welsh (Eds) Foundations of Orientation and Mobility. 2nd Edition (Chapter 1, pp.9-38). New York: American Foundation for the Blind. Google Scholar\nHeyes, A.D. (1983) 'The Sonic Pathfinder - a new travel aid for the blind'. In W. J. Perkins (Ed.) High Technology Aids for the Disabled (pp.231-270). London: Butterworth & Co. Google Scholar , Crossref\nHeyes, A.D., Dodds, A.G., Carter, D.D.C. & Howarth, C.I. (1983) 'Evaluation of the mobility of Blind Pedestrians' . In W. J. Perkins (Ed.) High Technology Aids for the Disabled (pp.165-171). London: Butterworth & Co. Google Scholar , Crossref\nJansson, G. (1995) 'Theoretical analysis and literature survey'. In G. Jansson (Ed.) Requirements for Effective Orientation and Mobility by Blind People (Chapter 1, pp. 5-16). London: Royal National Institute of the Blind. Google Scholar\nKang-Tsung, C. (2001) Introduction to Geographic Information Systems . USA: McGraw-Hill. Google Scholar\nKorhonen, J. (2001) Introduction to 3G Mobile Communication. USA: Artech House. Google Scholar\nLoomis, J.M., Golledge, R.G. & Klatzky, R.L. (1998) 'Navigations system for the blind: auditory display modes and guidance'. Presence, 7 (2), 193-203. Google Scholar , Crossref\nMay, M. ( 2002) 'Accessible GPS navigation and digital map information' . In Proceedings of the 10th International Mobility Conference (IMC-10), Warwick. Google Scholar\nParkinson, B. W. & Spilker Jr., J. J. (Eds) (1996) 'Overview of GPS Operation and Design'. In Parkinson, B. W., Spilker Jr., J. J. (Eds) Global Positioning System: Theory and Applications (Volume 1, Chapter 2, pp.29-55). Washington DC: American Institute of Aeronautics and Astronautics. Google Scholar\nPeake, P. & Leonard, J.A. (1971) 'The use of heart-rate as an index of stress in blind pedestrians'. Ergonomics, 14 (2), 189-204. Google Scholar , Crossref , Medline\nPetrie, H., Johnson, V., Strothotte, T., Raab, A., Michel, R., Reichert, L. & Schalt, A. (1997) 'MoBIC: An aid to increase the independent mobility of blind travellers'. The British Journal of Visual Impairment , 15 (2), 63-66. Google Scholar , Link\nShingledecker, C. (1983) 'Measuring the mental effort of blind mobility' . Journal of Visual Impairment and Blindness, 77 (7), 334-39. Google Scholar\nWycherley, R.J. & Nicklin, B.H. (1970) 'The heart rate of blind and sighted pedestrians on a town route'. Ergonomics, 13 (2), 181-92. Google Scholar , Crossref , Medline\n""","0.54150933","""http://journals.sagepub.com/doi/10.1177/026461960302100204""","[-0.472855,51.532848]"
